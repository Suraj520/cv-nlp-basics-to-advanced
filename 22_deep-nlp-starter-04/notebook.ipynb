{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "Implementing transformers architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mandatory imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "1. W = [X_transpose.X]\n",
    "2. Y = W_Transpose.X\n",
    "\n",
    "* It can be regarded as a set operation with zero parameters that can be tweaked durind training.\n",
    "\n",
    "* Scaled Self attention is used when the W matrix's size grows proportionally to the input size.\n",
    "\n",
    "where X can be numericalised text such as Hi There, Let's have a look --> [0, 5, 4, 7, 6]\n",
    "\n",
    "* Multi-head attention- Different words related to each other by different relation in an input vector\n",
    "\n",
    "For example - Hi relates to There and us directly whereas the intent is to look there. These subtle features are extracted better using multi head attentions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
