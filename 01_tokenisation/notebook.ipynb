{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About\n",
    "Tokenization is one of the first steps of data processing when it comes to working with data in the domain of NLP.\n",
    "\n",
    "* We will use spacy to tokenize input sentences and compare it's results with basic tokenization performed via Python.\n",
    "\n",
    "\n",
    "#### Requirements\n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0 - Hi,\n",
      "Token 1 - There\n",
      "Token 2 - !\n",
      "Token 3 - This\n",
      "Token 4 - is\n",
      "Token 5 - a\n",
      "Token 6 - notebook\n",
      "Token 7 - on\n",
      "Token 8 - Tokenization\n"
     ]
    }
   ],
   "source": [
    "# tokenization of a text using python\n",
    "doc = \"Hi, There ! This is a notebook on Tokenization\"\n",
    "for i,token in enumerate(doc.split(\" \")):\n",
    "    print(\"Token {} - {}\".format(i,token))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, This straightforward approach of tokenisation encounters a lot of loopholes as text contains tokens which are noisy. Like associated with hyphens or name of various nouns.\n",
    "\n",
    "* BERT uses the concept of sub-word tokens to permute over various combinations of characters which can form part of the vocabulary. It helps it in narrowing down to the OOV(Out of vocabulary) tokens.\n",
    "\n",
    "Thus, We use spacy as an efficient tokenizer for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/envs/dl/lib/python3.7/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2023-01-15 01:21:26.881653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-15 01:21:29.320432: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-15 01:21:29.322004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-15 01:21:29.325578: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-01-15 01:21:29.325623: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: suraj\n",
      "2023-01-15 01:21:29.325632: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: suraj\n",
      "2023-01-15 01:21:29.325811: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.226.0\n",
      "2023-01-15 01:21:29.325842: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.226.0\n",
      "2023-01-15 01:21:29.325852: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.226.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Hi\n",
      "Token: ,\n",
      "Token: There\n",
      "Token: !\n",
      "Token: This\n",
      "Token: is\n",
      "Token: a\n",
      "Token: notebook\n",
      "Token: on\n",
      "Token: Tokenization\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#tokenizigng\n",
    "doc = nlp(\"Hi, There ! This is a notebook on Tokenization\")\n",
    "for token in doc:\n",
    "    print(\"Token: {}\".format(token))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also add his own tokenizer rules. Visit  <a href=\"https://spacy.io/usage/linguistic-features#special-cases\"> Link </a>\n",
    "\n",
    "Besides this, Each model like BERT, BART and its variants come with their own tokenizers. Let's have a look at one such variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"Hi, This is our first Tokenizer Notebook\",\n",
    "    \"Glad to see you here.\",\n",
    "    \"What are you upto ?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'hi': 2, 'this': 3, 'is': 4, 'our': 5, 'first': 6, 'tokenizer': 7, 'notebook': 8, 'glad': 9, 'to': 10, 'see': 11, 'here': 12, 'what': 13, 'are': 14, 'upto': 15}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=20)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_idx = tokenizer.word_index\n",
    "print(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8]\n",
      "[9, 10, 11, 1, 12]\n",
      "[13, 14, 1, 15]\n"
     ]
    }
   ],
   "source": [
    "# converting each tokenized sentence into sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "for seq in sequences:\n",
    "    print(seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6 7 8]\n",
      "[ 9 10 11  1 12  0  0]\n",
      "[13 14  1 15  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# to ensure that each sequence contains same number of tokens which are a primary need for any NN. We'll pad\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "for seq in padded_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer also comes as various types, Few of which are listed below\n",
    "1. Word piece tokenizer of BERT, DistilBERT, Electra\n",
    "2. Byte pair encoding tokenizer for GPT2.\n",
    "3. Unigram.\n",
    "4. Sentence piece(https://github.com/google/sentencepiece)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 14:32:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50052c996937e9a0e161d422489677fdaadc23d756ac209b7397e80e5ea8cea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
