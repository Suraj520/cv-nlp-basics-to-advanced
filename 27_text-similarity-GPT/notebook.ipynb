{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L10DOR9L3GgY"
      },
      "source": [
        "> Text Generation using GPT\n",
        "Dataset - https://cs.rochester.edu/nlp/rocstories/\n",
        "\n",
        "\n",
        "#### About GPT\n",
        "1. GPT model is 12 layer 12 attention Transformer decoder model.\n",
        "2. It's good for Question Answering, Semantic Similarity of Texts etc.\n",
        "3. It takes advantage of Semi supervised Learning in NLP where labelled data is small whereas unlabelled data is huge.\n",
        "4. It is trained for Language Modelling task on BooksCorpus Dataset.\n",
        "5. During Pretraining phase, Context vectors are generated.\n",
        "6. During Fine Tuning Strategy, Text and position embeddings are passed into 12 transformers block, Output is softmax applied on the output of transformer block.\n",
        "7. These architectures do Task specific input transformations by introducing $ sign delimiter.\n",
        "8. It's just the decoder part of the transformer.\n",
        "\n",
        "> Architecture's key components.\n",
        "1. Input embedding - The input text is transformed into a high dimensional vector representation.\n",
        "2. Multi head self attention - This is a mechanism for the model to focus on different parts of the input sequence and make predictions based on context of the entire input.\n",
        "3. Feedforward network - A series of dense layers that process the output of the attention mechanism.\n",
        "4. Layer normalisation - A normalisation step that helps improve the stability and efficiency of training. Similar to Batch Normalisation.\n",
        "5. Output projection : The final output of the model is a prob. distribution over a predefined vocab of tokens which is used to generate next token in sequence.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmBXklvZ3Ggb",
        "outputId": "a5ee808a-ad53-44ef-d8f6-f178f2d05a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi, there! what are you upto today? \" \n",
            " \" i'm working on a case, my dear, and want to go down and have a bite to eat with them some time today, if that's ok? \" said kristus\n"
          ]
        }
      ],
      "source": [
        "# Basic implementation from Hugging Face\n",
        "import torch\n",
        "!pip install transformers --quiet\n",
        "from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n",
        "\n",
        "# Load the pre-trained GPT2 tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Input text\n",
        "input_text = \"Hi, There! What are you upto today ?\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate text\n",
        "output_ids = model.generate(input_ids, max_length=50, do_sample=True)\n",
        "\n",
        "# Decode the generated text\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZaOVEii3Gge"
      },
      "source": [
        "Reference - \n",
        "1. https://huggingface.co/docs/transformers/model_doc/openai-gpt\n",
        "2. https://github.com/openai/finetune-transformer-lm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNQ6oMbA3Gge"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch_dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3fc43966dd8a35b9bb4dacfb26d54ec70461d2f8773a70bf315d67d5e8c2bf14"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}