{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Knowledge Graph Construction\n",
    "\n",
    "Knowledge graph construction is the process of creating a graph that represents knowledge in a particular domain. It involves identifying entities and their relationships and representing them in a structured format. This graph can be used for various applications such as question answering, recommendation systems, and natural language processing.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a set of sentences that describe relationships between entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"John is married to Jane\", \"marriage\", \"John\", \"Jane\"),\n",
    "    (\"Jane is the mother of Tom\", \"parent-child\", \"Jane\", \"Tom\"),\n",
    "    (\"Tom is the son of Jane\", \"parent-child\", \"Jane\", \"Tom\"),\n",
    "    (\"John and Jane have two children\", \"parent-child\", \"John\", \"child1\"),\n",
    "    (\"John and Jane have two children\", \"parent-child\", \"Jane\", \"child2\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum length of a sentence\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Create a tokenizer to convert words to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([x[0] for x in training_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sentences to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences([x[0] for x in training_data])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Create one-hot vectors for the relationship types\n",
    "labels = np.zeros((len(training_data), 3))\n",
    "for i, (_, rel_type, _, _) in enumerate(training_data):\n",
    "    if rel_type == \"marriage\":\n",
    "        labels[i, 0] = 1\n",
    "    elif rel_type == \"parent-child\":\n",
    "        labels[i, 1] = 1\n",
    "    else:\n",
    "        labels[i, 2] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot vectors for the entities\n",
    "entities = np.zeros((len(training_data), 4))\n",
    "for i, (_, _, entity1, entity2) in enumerate(training_data):\n",
    "    if entity1 == \"John\":\n",
    "        entities[i, 0] = 1\n",
    "    elif entity1 == \"Jane\":\n",
    "        entities[i, 1] = 1\n",
    "    elif entity1 == \"Tom\":\n",
    "        entities[i, 2] = 1\n",
    "    else:\n",
    "        entities[i, 3] = 1\n",
    "    if entity2 == \"John\":\n",
    "        entities[i, 0] = 1\n",
    "    elif entity2 == \"Jane\":\n",
    "        entities[i, 1] = 1\n",
    "    elif entity2 == \"Tom\":\n",
    "        entities[i, 2] = 1\n",
    "    else:\n",
    "        entities[i, 3] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layers\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "entity_input = Input(shape=(4,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 19:17:12.176223: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-15 19:17:12.178261: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-15 19:17:12.185506: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding layer for the sentences\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1, 100, input_length=MAX_SEQUENCE_LENGTH)(sentence_input)\n",
    "\n",
    "# Define the LSTM layer for the sentences\n",
    "lstm_layer = LSTM(100)(embedding_layer)\n",
    "\n",
    "# Define the dense layer for the relationship types\n",
    "rel_type_layer = Dense(3, activation='softmax')(lstm_layer)\n",
    "\n",
    "# Define the dense layer for the entities\n",
    "entity_layer = Dense(4, activation='softmax')(entity_input)\n",
    "\n",
    "# Concatenate the output of the LSTM layer and the entity layer\n",
    "merged_layer = concatenate([lstm_layer, entity_layer])\n",
    "\n",
    "# Define the output layer for the knowledge graph\n",
    "kg_layer = Dense(1, activation='sigmoid')(merged_layer)\n",
    "\n",
    "# Define the Keras model\n",
    "model = Model(inputs=[sentence_input, entity_input], outputs=[rel_type_layer, kg_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 19:07:25.222502: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-15 19:07:25.225215: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-15 19:07:25.226997: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "/home/suraj/anaconda3/envs/dl/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n",
      "2023-05-15 19:07:27.091579: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-15 19:07:27.096959: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-15 19:07:27.101344: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step - loss: 1.1073 - dense_loss: 1.1073 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.2000 - dense_2_accuracy: 0.6000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.0995 - dense_loss: 1.0995 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.2000 - dense_2_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.0920 - dense_loss: 1.0920 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.4000 - dense_2_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.0840 - dense_loss: 1.0840 - dense_2_loss: 0.0000e+00 - dense_accuracy: 1.0000 - dense_2_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.0746 - dense_loss: 1.0746 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.8000 - dense_2_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.0621 - dense_loss: 1.0621 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.8000 - dense_2_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.0439 - dense_loss: 1.0439 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.8000 - dense_2_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0149 - dense_loss: 1.0149 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.8000 - dense_2_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9646 - dense_loss: 0.9646 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.8000 - dense_2_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8852 - dense_loss: 0.8852 - dense_2_loss: 0.0000e+00 - dense_accuracy: 0.8000 - dense_2_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9cd0775e80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padded_sequences, entities], [labels, np.zeros((len(training_data), 1))], epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
