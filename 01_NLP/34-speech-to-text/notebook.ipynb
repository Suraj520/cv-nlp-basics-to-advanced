{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-09T15:26:17.845363Z","iopub.execute_input":"2023-05-09T15:26:17.845778Z","iopub.status.idle":"2023-05-09T15:26:17.853152Z","shell.execute_reply.started":"2023-05-09T15:26:17.845745Z","shell.execute_reply":"2023-05-09T15:26:17.851630Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import os\n\n# # Set the path to the dataset directory\n# data_dir = '/kaggle/input/speech-to-text-russian/data/data/'\n\n# # Load the training labels into a pandas DataFrame\n# train_labels_path = os.path.join(data_dir, 'train_labels.csv')\n# train_labels_df = pd.read_csv(train_labels_path)\n\n# # Print the first few rows of the DataFrame to verify that it loaded correctly\n# print(train_labels_df.head())\n\n# # Set the path to the training audio files directory\n# train_audio_dir = os.path.join(data_dir, 'train_wavs')\n\n# # Loop over the audio files in the directory and print their filenames\n# for filename in os.listdir(train_audio_dir):\n#     print(filename)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:15:13.782220Z","iopub.execute_input":"2023-05-10T03:15:13.782679Z","iopub.status.idle":"2023-05-10T03:15:18.194702Z","shell.execute_reply.started":"2023-05-10T03:15:13.782649Z","shell.execute_reply":"2023-05-10T03:15:18.193637Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install torchaudio --quiet\n!pip install pandas --quiet\n!pip install torchaudio soundfile --quiet\n!pip install transformers --quiet\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:15:42.873985Z","iopub.execute_input":"2023-05-10T03:15:42.874353Z","iopub.status.idle":"2023-05-10T03:16:29.893046Z","shell.execute_reply.started":"2023-05-10T03:15:42.874323Z","shell.execute_reply":"2023-05-10T03:16:29.891819Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/speech-to-text-russian/data/data/train_labels.csv')\ndf = df[:5000]\naudio_files = ['/kaggle/input/speech-to-text-russian/data/data/train_wavs/' + str(i) + '.wav' for i in df['Id']]\ntranscripts = df['Expected']\n\n# Preprocess the data\nwaveform_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000)\ntokenizer = lambda x: x.split()\nvocab = set([word for transcript in transcripts for word in tokenizer(transcript)])\nword_to_idx = {word: i for i, word in enumerate(vocab)}\nidx_to_word = {i: word for i, word in enumerate(vocab)}\n\n\n\nclass SpeechToTextDataset(Dataset):\n    def __init__(self, audio_files, transcripts, waveform_transforms, tokenizer, word_to_idx):\n        self.audio_files = audio_files\n        self.transcripts = transcripts\n        self.waveform_transforms = waveform_transforms\n        self.tokenizer = tokenizer\n        self.word_to_idx = word_to_idx\n\n    def __len__(self):\n        return len(self.audio_files)\n\n    def __getitem__(self, idx):\n        waveform, sample_rate = torchaudio.load(self.audio_files[idx])\n        waveform = self.waveform_transforms(waveform)\n        transcript = self.tokenizer(self.transcripts[idx])\n        transcript = [self.word_to_idx[word] for word in transcript]\n        return waveform, transcript\n\ntrain_dataset = SpeechToTextDataset(audio_files[:800], transcripts[:800], waveform_transforms, tokenizer, word_to_idx)\ntest_dataset = SpeechToTextDataset(audio_files[800:1000], transcripts[800:1000], waveform_transforms, tokenizer, word_to_idx)\n\n\ndef collate_fn(batch):\n    # Sort the batch by sequence length\n    batch = sorted(batch, key=lambda x: x[0].shape[2], reverse=True)\n    # Pad the sequences to the same length\n    max_length = batch[0][0].shape[2]\n    padded_batch = []\n    for waveform, transcript in batch:\n        padded_waveform = nn.functional.pad(waveform, (0, 0, 0, max_length - waveform.shape[2]), mode='constant', value=0)\n        padded_transcript = nn.functional.pad(torch.tensor(transcript), (0, max_length - len(transcript)), mode='constant', value=0)\n        padded_batch.append((padded_waveform, padded_transcript))\n    # Stack the tensors into a batch\n    waveforms, transcripts = zip(*padded_batch)\n    waveforms = torch.stack(waveforms)\n    transcripts = torch.stack(transcripts)\n    return waveforms, transcripts\n\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in train_loader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:16:30.466922Z","iopub.execute_input":"2023-05-10T03:16:30.468819Z","iopub.status.idle":"2023-05-10T03:16:30.542824Z","shell.execute_reply.started":"2023-05-10T03:16:30.468783Z","shell.execute_reply":"2023-05-10T03:16:30.541846Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [5.1327e-06, 1.4528e-07, 5.0579e-07,  ..., 7.1468e-09,\n           5.0771e-09, 1.8826e-06],\n          [2.7636e-05, 7.8223e-07, 2.7233e-06,  ..., 3.8481e-08,\n           2.7336e-08, 1.0136e-05],\n          ...,\n          [6.7102e-04, 1.7031e-07, 9.9636e-08,  ..., 1.7734e-07,\n           1.4525e-07, 7.8576e-07],\n          [6.5696e-04, 2.3466e-07, 1.3278e-07,  ..., 1.4126e-07,\n           1.0601e-07, 4.2714e-07],\n          [6.3786e-04, 2.1794e-07, 1.7269e-07,  ..., 1.6624e-07,\n           1.0927e-07, 1.9881e-07]]]]), tensor([[11023, 13056,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0]]))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the model\nclass SpeechToTextModel(nn.Module):\n    def __init__(self, num_classes, input_size=128, hidden_size=256, num_layers=3, num_channels=32):\n        super(SpeechToTextModel, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, num_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n            nn.BatchNorm2d(num_channels),\n            nn.ReLU(),\n            nn.Conv2d(num_channels, num_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n            nn.BatchNorm2d(num_channels),\n            nn.ReLU(),\n            nn.Conv2d(num_channels, num_channels, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1)),\n            nn.BatchNorm2d(num_channels),\n            nn.ReLU(),\n            nn.Conv2d(num_channels, num_channels, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1)),\n            nn.BatchNorm2d(num_channels),\n            nn.ReLU(),\n        )\n        self.rnn = nn.LSTM(input_size=num_channels, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch_size, num_channels, seq_len, input_size = x.size()\n        x = x.permute(0, 3, 1, 2)  # Swap dimensions to (batch_size, input_size, num_channels, seq_len)\n        x = x.view(batch_size, input_size, num_channels*seq_len)  # Reshape to (batch_size, input_size, num_channels*seq_len)\n        x, _ = self.rnn(x)\n        x = self.fc(x)\n        return x\n\n\nmodel = SpeechToTextModel(128, 256, len(vocab))\n\n# Train the model\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(10):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs.view(-1, len(vocab)), labels.view(-1))\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if i % 100 == 99:\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n            running_loss = 0.0\n\n# Evaluate the model\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        inputs, labels = data\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 2)\n        total += labels.size(0) * labels.size(1)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the test data: %d %%' % (100 * correct / total))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:16:30.545072Z","iopub.execute_input":"2023-05-10T03:16:30.545748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}