{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "Implementing transformers architecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "1. W = [X_transpose.X]\n",
    "2. Y = W_Transpose.X\n",
    "\n",
    "* It can be regarded as a set operation with zero parameters that can be tweaked durind training.\n",
    "\n",
    "* Scaled Self attention is used when the W matrix's size grows proportionally to the input size.\n",
    "\n",
    "where X can be numericalised text such as Hi There, Let's have a look --> [0, 5, 4, 7, 6]\n",
    "\n",
    "* Multi-head attention- Different words related to each other by different relation in an input vector\n",
    "\n",
    "For example - Hi relates to There and us directly whereas the intent is to look there. These subtle features are extracted better using multi head attentions.\n",
    "\n",
    "- It is self attention applied parallely over each such relations.\n",
    "\n",
    "- Self attention is linear operation and thus doesn't suffer from vanishing or exploding gradients.\n",
    "\n",
    "- Self attention can be regarded as sequence to sequence layer used for machine translation for parallel computation, perfect long term memory.\n",
    "\n",
    "- Self attention can be stacked onto each other to build powerful models known as transformers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "1. Transformer model is Seq to Seq model that uses self attention to propagate information along the time dimension like when it's done across pixesl it becomes image transformer and when it's done across graph nodes then it's graph transformer.\n",
    "\n",
    "\n",
    "The basic transformer block modular class architecture is quoted below\n",
    "\n",
    "1. pass the input through layer normalisation which is similar to batch normalisation.\n",
    "2. The output is then passed through self attention.\n",
    "3. The output in 1. and 2. are concatenated over a residual connection.\n",
    "4. It is fed further to the patterns illustrated in 1. and 2.\n",
    "\n",
    "```\n",
    "class Block(nn.Module):\n",
    "    def forward(self,x):\n",
    "        y = self.layernorm(x)\n",
    "        y = self.self_attention(y)\n",
    "        x = x+y\n",
    "\n",
    "        y = self.layernorm(x)\n",
    "        y = self.linear(x)\n",
    "        return x+y\n",
    "\n",
    "```\n",
    "\n",
    "- Basic architecture for a sequence to label transformer shall comprise of input embeddings extracted from sequence of chars fed to a stack of transformer block with output sequence pooled to the output label.\n",
    "\n",
    "Limitations\n",
    "\n",
    "1. By this approach, The positional information between words is lost such as the difference between \n",
    "\n",
    "Yesterday, The car gave a bad riding experience than the bike.\n",
    "The bike doesn't not really give nice riding experience than the car.\n",
    "\n",
    "2. To fix this, We introduce \n",
    "- a. Position embeddings\n",
    "- b. Positing encodings\n",
    "- c. Relative positions\n",
    "\n",
    "\n",
    "- To take self attention layers and build transformers from it, We need the following ingredients.\n",
    "1. Define a transformer block.\n",
    "2. Mask the self attention block.\n",
    "3. Stack various transformer blocks\n",
    "4. Add positional information to the input vectors.\n",
    "\n",
    "\n",
    "- All of the transformer architectures have a pretraining phase and a Fine tuning phase.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT \n",
    "\n",
    "* BERT is trained on two tasks in Pretraining Phase on the corpus i.e Masking and Next Sentence prediction.\n",
    "\n",
    "1. In Masking - Few tokens are randomly corrupted intentionally and the BERT is asked to predict those. Similar to Fill in the blanks.\n",
    "\n",
    "2. Next Sentence Prediciton - CLS token is put at start of each sequence and SEP is used to concatenate sequence. In this task, BERT predicts whether two sequences are contiguous or from different parts of sentence.\n",
    "\n",
    "In Fine tuning phase, It is fine tuned for the desired task.\n",
    "\n",
    "For more information, Refer the blogs\n",
    "\n",
    "1. <a href=\"https://suraj52.medium.com/the-transformers-985bfe679001\" >Medium Blog1 - Suraj </a>\n",
    "2. <a href=\"https://suraj52.medium.com/types-of-transformers-i-bert-aa38e04f2458\" >Medium Blog2 - Suraj </a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Implemetation of Transformer\n",
    "<img src=\"/home/suraj/ClickUp/Jan-Feb/nlp-basics-to-advanced/22_deep-nlp-starter-04/1.png\" alt=\"Alt text\" title=\"BERT\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Input Embeddings.\n",
    "We need to convert each word in the input sequence to an embedding vector. These are substitute to one hot encodings with more semantic representations embedded in them.\n",
    "\n",
    "Dimension understanding\n",
    "\n",
    "> Suppose each embedding vector's dimension is 256 and our vocab size is 1000 then our embedding matrix shall be 1000*256. The matrix shall be learned during training or one can use pretrained embeddings like Word2Vec or Glove. \n",
    "\n",
    "> During inference in the sequence, Each word will be mapped to corresponding 256 dimension vector.\n",
    "\n",
    "> If batch size is 128, sequence length is 20 words then the output shall be 128*20*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size):\n",
    "        super(EmbeddingLayer,self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,embedding_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.embedding_layer(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0330,  0.9593,  0.1704,  ...,  0.1247,  0.0717,  0.5052],\n",
      "        [ 0.6873, -0.4152, -1.7856,  ...,  0.1738,  0.0799, -0.4244],\n",
      "        [ 0.8449, -0.0975,  0.5561,  ..., -2.1078, -1.1632,  1.7847],\n",
      "        [ 0.6218,  0.2450, -0.6984,  ...,  0.4097, -0.8713, -0.7066],\n",
      "        [ 1.3078,  0.3746, -2.0494,  ..., -0.2825, -0.5389,  3.4960],\n",
      "        [ 0.0629, -0.4317, -0.3321,  ..., -0.5504,  1.6873, -0.5489]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = [\"Hi, There ! This is your first demo\"]\n",
    "seq_dict = {\"Hi\":0, \"There\":4, \"This\":3, \"is\":2, \"your\":1, \"demo\":5}\n",
    "seq = [0,4,3,2,1,5]\n",
    "vocab_size=100\n",
    "embedding_size=256\n",
    "seq = torch.LongTensor(seq)\n",
    "embedding_layer = EmbeddingLayer(vocab_size,embedding_size)\n",
    "print(embedding_layer.forward(seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Positional Encoding\n",
    "\n",
    "> To understand the sentence better, The model needs to know two things about each word i.e. meaning of the word and the position of the word in sentence.\n",
    "\n",
    "> As per \"Attention is all you need\" paper, A cosine function was used in odd time steps whereas a sine for even time steps.\n",
    "\n",
    "PE(pos,2i) = sin(pos/10000^2i/d)\n",
    "PE(pos,2i+1) = cos(pos/10000^2i/d)\n",
    "where i refers to the position along embedding vector dimension, d is dimension of embedding.\n",
    "\n",
    "> Positional embedding generates a matrix similar to embedding matrix. The output dimension is dimension_seq_len * embedding dimension. For each token in sequence, We'll find the embedding vector which is of dim 1*256 to get 1*256 dimension output for each token.\n",
    "\n",
    "> So, If our batch size is 128 and seq_len is 20 then our output dimension of positional encoding shall be 128*20*256. At this stage positional embedding is concatenated with the previous embedding as per block diagram in paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc43966dd8a35b9bb4dacfb26d54ec70461d2f8773a70bf315d67d5e8c2bf14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
