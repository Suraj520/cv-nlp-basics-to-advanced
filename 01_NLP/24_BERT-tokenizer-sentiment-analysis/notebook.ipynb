{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "Sentiment Analysis using BERT Tokenizer and 1D CNNs\n",
    "\n",
    "Dataset - https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import random\n",
    "#!pip install bert-for-tf2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/suraj/ClickUp/Jan-Feb/data/all-data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment                                               Text\n",
       "0      neutral  According to Gran , the company has no plans t...\n",
       "1      neutral  Technopolis plans to develop in stages an area...\n",
       "2     negative  The international electronic industry company ...\n",
       "3     positive  With the new production plant the company woul...\n",
       "4     positive  According to the company 's updated strategy f...\n",
       "...        ...                                                ...\n",
       "4841  negative  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844  negative  Net sales of the Paper segment decreased to EU...\n",
       "4845  negative  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"Sentiment\", \"Text\"]\n",
    "data = pd.read_csv(dataset_path,header=None, names =cols, encoding='latin1')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4840 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment                                               Text\n",
       "0      neutral  According to Gran , the company has no plans t...\n",
       "1      neutral  Technopolis plans to develop in stages an area...\n",
       "2     negative  The international electronic industry company ...\n",
       "3     positive  With the new production plant the company woul...\n",
       "4     positive  According to the company 's updated strategy f...\n",
       "...        ...                                                ...\n",
       "4841  negative  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844  negative  Net sales of the Paper segment decreased to EU...\n",
       "4845  negative  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4840 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text,\"lxml\").get_text()\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\",\" \",text) # like replace for string\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\",' ',text) # replacing https and ? as s is not conformed\n",
    "    text = re.sub(r\"[^a-zA-Z.!?']\",\" \",text) # removing everything other than these\n",
    "    text = re.sub(r\" +\",\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/envs/dl/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Gran the company has no plans to move all production to Russia although that is where the company is growing .\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = [clean_text(text) for text in data.Text]\n",
    "print(cleaned_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "unique_sentiment = np.unique(data.Sentiment.values.tolist())\n",
    "print(unique_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "emotion_mapper= {}\n",
    "for i,sentiment in enumerate(unique_sentiment):\n",
    "    emotion_mapper[sentiment]=i \n",
    "print(emotion_mapper)\n",
    "\n",
    "reverse_mapper = {}\n",
    "for k,v in emotion_mapper.items():\n",
    "    reverse_mapper[v] = k\n",
    "print(reverse_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sentiments = [emotion_mapper[sentiment] for sentiment in data.Sentiment]\n",
    "print(sentiments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "bert_tokenizer = bert.bert_tokenization.FullTokenizer\n",
    "#bert layer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
    "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding sentence function\n",
    "def encode_sentences(sent):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = [encode_sentences(sentence) for sentence in cleaned_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding with collate equiv\n",
    "# preparing data\n",
    "#[list of token, label,seq_len]\n",
    "encoded_data_with_len = [[sent,sentiments[i],len(sent)] for i, sent in enumerate(tokenized_inputs)]\n",
    "random.shuffle(encoded_data_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4082,\n",
       "  3279,\n",
       "  23596,\n",
       "  7327,\n",
       "  2099,\n",
       "  1012,\n",
       "  24098,\n",
       "  4102,\n",
       "  2000,\n",
       "  1037,\n",
       "  5618,\n",
       "  1997,\n",
       "  7327,\n",
       "  2099,\n",
       "  1012,\n",
       "  24098,\n",
       "  1999,\n",
       "  1996,\n",
       "  7978,\n",
       "  2558,\n",
       "  1999,\n",
       "  1012],\n",
       " 0,\n",
       " 22]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_with_len[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_with_len.sort(key = lambda x:x[2]) # sorting based on seq_len\n",
    "#only using longer sentences with seq_len >5 for better understanding \n",
    "sorted_data=[(var[0],var[1])\n",
    "            for var in encoded_data_with_len if var[2] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3463, 2024, 3517, 2397, 1999, 1012], 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tensordataset\n",
    "train_data = tf.data.Dataset.from_generator(lambda:sorted_data, output_types=(tf.int32,tf.int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(6,), dtype=int32, numpy=array([3463, 2024, 3517, 2397, 1999, 1012], dtype=int32)>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=1>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking dataset\n",
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 64\n",
    "#padding the tensors -- collate\n",
    "batched_data = train_data.padded_batch(num_batches, padded_shapes=((None,),()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64, 8), dtype=int32, numpy=\n",
      "array([[ 3463,  2024,  3517,  2397,  1999,  1012,     0,     0],\n",
      "       [ 2053,  3361,  4751,  2020,  2800,  1012,     0,     0],\n",
      "       [ 4341,  1997,  5929,  2764,  2190,  1012,     0,     0],\n",
      "       [ 2053,  3361,  4751,  2020,  3024,  1012,     0,     0],\n",
      "       [ 2053,  3361,  6987,  2020,  2800,  1012,     0,     0],\n",
      "       [ 6636,  3872,  3445,  2011,  3155,  1012,     0,     0],\n",
      "       [ 3361,  4751,  2020,  2025, 21362,  1012,     0,     0],\n",
      "       [ 2053, 20874,  4751,  2020, 21362,  1012,     0,     0],\n",
      "       [ 2060,  4751,  2020,  2025,  3024,  1012,     0,     0],\n",
      "       [ 2035,  2060,  5571,  2020,  7219,  1012,     0,     0],\n",
      "       [ 7473,  2102,  2896,  2012,  1012,  1012,     0,     0],\n",
      "       [ 1996,  7909,  3058,  2003,  2233,  1012,     0,     0],\n",
      "       [ 2053,  3361,  4751,  2020,  2988,  1012,     0,     0],\n",
      "       [ 1996,  2986,  6140,  2003,  2182,  1012,     0,     0],\n",
      "       [ 9662,  7368,  2145,  4839,  2174,  1012,     0,     0],\n",
      "       [ 2017,  1005,  2128,  2025,  2894,  1012,     0,     0],\n",
      "       [ 2053,  3361,  4751,  2020, 21362,  1012,     0,     0],\n",
      "       [ 1996, 19939,  2005,  2003,  1012,  1012,     0,     0],\n",
      "       [ 3361,  3408,  2020,  2025, 21362,  1012,     0,     0],\n",
      "       [ 2053,  3361,  2592,  2001,  3024,  1012,     0,     0],\n",
      "       [ 2053,  3361,  4751,  2020,  3936,  1012,     0,     0],\n",
      "       [ 5427,  6043,  2323,  2022,  3722,  1012,     0,     0],\n",
      "       [ 2062,  2084,  6304,  2020,  2187, 25192,  1012,     0],\n",
      "       [ 2825,  5073, 25006,  5142,  3155,  2111,  1012,     0],\n",
      "       [ 2049,  3296,  3977,  2003,  2070, 12464,  1012,     0],\n",
      "       [ 1996,  5309,  3976,  2001,  2025, 21362,  1012,     0],\n",
      "       [ 1996,  7654,  3976,  2001,  2025, 21362,  1012,     0],\n",
      "       [ 1996,  2146,  2744,  3206,  2003,  3795,  1012,     0],\n",
      "       [ 3937,  8169,  3450,  2506,  2004,  3671,  1012,     0],\n",
      "       [ 3161,  7829,  2003,  6827,  2005,  6435,  1012,     0],\n",
      "       [ 2537,  2389,  3663,  2038,  2085,  5301,  1012,     0],\n",
      "       [14101,  1005,  1005,  8519,  7420,  9857,  1012,     0],\n",
      "       [ 2303,  1996,  4923,  4212,  2349,  2337,  1012,     0],\n",
      "       [ 1996,  7937,  7785,  2038,  2815,  2152,  1012,     0],\n",
      "       [ 1037,  5618,  2003,  2145,  2006,  4539,  1012,     0],\n",
      "       [ 2270,  2578,  2097,  2036,  2022,  2800,  1012,     0],\n",
      "       [ 1996,  3206,  2003,  2005,  2279,  2095,  1012,     0],\n",
      "       [ 1996,  5096,  3976,  2001,  2025, 21362,  1012,     0],\n",
      "       [ 2002,  4484,  2010,  3193,  2006,  2251,  1012,     0],\n",
      "       [ 2023,  2052,  2022,  1037,  4121,  2832,  1012,     0],\n",
      "       [ 2885,  3791,  2047,  2312,  3259,  6681,  1012,     0],\n",
      "       [ 2053,  3740,  5494,  3058,  2001,  3024,  1012,     0],\n",
      "       [ 6718,  2075,  2001,  4102,  2000,  1999,  1012,     0],\n",
      "       [ 1996,  4341,  3976,  2001,  2025, 21362,  1012,     0],\n",
      "       [ 4728,  1996,  3663,  2003,  2104,  2491,  1012,     0],\n",
      "       [ 1996,  7312,  5060,  4162,  2000,  2111,  1012,     0],\n",
      "       [ 5658,  4341,  2097,  2174,  3623,  2013,  1012,     0],\n",
      "       [ 6451,  8381,  7502,  2007,  3020, 13139,  1012,     0],\n",
      "       [ 1996,  4965,  1005, 12832,  2001, 28960,  1012,     0],\n",
      "       [12886,  2240,  2038, 13261,  2070,  2578,  1012,     0],\n",
      "       [16653,  6807,  4022, 13797,  1998, 20141,  1012,     0],\n",
      "       [ 2010,  8172,  2097,  2202,  3466,  3202,  1012,     0],\n",
      "       [ 3930,  2003,  3517,  2000,  3613,  1999,  1012,     0],\n",
      "       [ 1037,  3820, 13735,  3041,  2023,  3204,  1012,     0],\n",
      "       [ 2053,  2326, 20874,  4751,  2020, 21362,  1012,     0],\n",
      "       [ 1996,  9367,  1997,  1996,  3206,  2003,  2706,  1012],\n",
      "       [ 2035, 14627,  2005,  2582,  4353,  2003, 10890,  1012],\n",
      "       [ 4341,  3062,  6917,  2021,  3445,  1999,  6435,  1012],\n",
      "       [ 4341,  1997,  3054,  3997,  5404, 10548,  2011,  1012],\n",
      "       [ 2747,  1996,  3269,  5748,  2006,  2440,  3977,  1012],\n",
      "       [ 1996,  2434,  3206,  2001,  2772,  2197,  2621,  1012],\n",
      "       [ 8381,  2003, 19939,  2000,  4982,  2011,  2055,  1012],\n",
      "       [ 3361,  6143,  1998,  6515,  5876,  2024,  2641,  1012],\n",
      "       [ 2339,  2025,  4942, 29234,  2000,  1996,  2932,  1029]],\n",
      "      dtype=int32)>, <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1],\n",
      "      dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(batched_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train, val dataset\n",
    "num_batches_train = math.ceil(len(sorted_data)/num_batches)\n",
    "num_batches_val = num_batches_train//5\n",
    "\n",
    "batched_data.shuffle(num_batches_train)\n",
    "\n",
    "val_dataset = batched_data.take(num_batches_val)\n",
    "train_dataset = batched_data.skip(num_batches_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building like Pytorch modular class\n",
    "class SentimentModel(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim=256, num_filters=50,dense_dim=512, num_class=2, dropout_rate=0.2, training=False):\n",
    "        super(SentimentModel,self).__init__()\n",
    "        self.embedding_layer = layers.Embedding(vocab_size,embedding_dim)\n",
    "        #creating cnn layer\n",
    "        self.bigram = layers.Conv1D(filters=num_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\n",
    "        #creating cnn for trigram\n",
    "        self.trigram = layers.Conv1D(filters=num_filters,kernel_size=3, padding=\"valid\",activation=\"relu\")\n",
    "        #creating cnn for quadgram\n",
    "        self.quadgram = layers.Conv1D(filters=num_filters,kernel_size=4, padding=\"valid\",activation=\"relu\")\n",
    "\n",
    "        #creating a layer which takes max of all outputs\n",
    "        self.pool = layers.GlobalAveragePooling1D()\n",
    "\n",
    "        #creating  dense layer with\n",
    "        self.dense = layers.Dense(units=dense_dim,activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "        if num_class==2:\n",
    "            self.dense2 = layers.Dense(units=1, activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.dense2 = layers.Dense(units=num_class,activation=\"softmax\")\n",
    "\n",
    "    \n",
    "    def call(self,inputs,training):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x1 = self.bigram(x)\n",
    "        x1 = self.pool(x1)\n",
    "\n",
    "        x2 = self.trigram(x)\n",
    "        x2 = self.pool(x2)\n",
    "\n",
    "        x3 = self.quadgram(x)\n",
    "        x3 = self.pool(x3)\n",
    "\n",
    "        concat_features = tf.concat([x1,x2,x3],axis=1)\n",
    "\n",
    "        out =self.dense(concat_features)\n",
    "        out = self.dropout(out,training)\n",
    "        out = self.dense2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMBEDDING_DIM=256\n",
    "NUM_FILTERS=128\n",
    "DENSE_UNITS=512\n",
    "NUM_CLASSES=3\n",
    "DROPOUT_RATE = 0.2\n",
    "NUM_EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentModel(vocab_size=VOCAB_SIZE,embedding_dim=EMBEDDING_DIM,num_filters=NUM_FILTERS,num_class=NUM_CLASSES, dropout_rate=DROPOUT_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_CLASSES==2:\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "else:\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./bert_ckpt\"\n",
    "ckpt = tf.train.Checkpoint(SentimentModel=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint,max_to_keep=1)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback\n",
    "class custom_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs=\"None\"):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}\".format(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "     61/Unknown - 10s 94ms/step - loss: 0.8676 - sparse_categorical_accuracy: 0.6003Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 10s 102ms/step - loss: 0.8676 - sparse_categorical_accuracy: 0.6003\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5596 - sparse_categorical_accuracy: 0.7449Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.5596 - sparse_categorical_accuracy: 0.7449\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.2514 - sparse_categorical_accuracy: 0.9108Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.2514 - sparse_categorical_accuracy: 0.9108\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.1335 - sparse_categorical_accuracy: 0.9539Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.1335 - sparse_categorical_accuracy: 0.9539\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.1414 - sparse_categorical_accuracy: 0.9536Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.1414 - sparse_categorical_accuracy: 0.9536\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0875 - sparse_categorical_accuracy: 0.9699Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.0875 - sparse_categorical_accuracy: 0.9699\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0580 - sparse_categorical_accuracy: 0.9813Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.0580 - sparse_categorical_accuracy: 0.9813\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0540 - sparse_categorical_accuracy: 0.9816Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.0540 - sparse_categorical_accuracy: 0.9816\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0401 - sparse_categorical_accuracy: 0.9855Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.0401 - sparse_categorical_accuracy: 0.9855\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0332 - sparse_categorical_accuracy: 0.9904Checkpoint saved at ./bert_ckpt\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f437af27a90>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=NUM_EPOCHS,callbacks=[custom_callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 14ms/step - loss: 2.1012 - sparse_categorical_accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def predict(text):\n",
    "    tokens = encode_sentences(text)\n",
    "    #expanding dim for batch\n",
    "    inputs = tf.expand_dims(tokens,0)\n",
    "    output = model(inputs,training=False)\n",
    "    sentiment = np.argmax(output)\n",
    "    return(reverse_mapper[sentiment])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"The financial market is blooming, We can expect good outcomes. Yayy !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"The financial market is blooming !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50052c996937e9a0e161d422489677fdaadc23d756ac209b7397e80e5ea8cea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
