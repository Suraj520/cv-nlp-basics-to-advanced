{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Architecture Overview\n\n> UniLM(Universal Language Model)\n\nIt is a transformer based architecture designed for various nlp taks, including lang generation, summarization, machine translation and more. It introduces several building blocks that enable it to handle both bidirectional and unidirectional language modelling tasks, making it different from other transformer architectures like GPT.\n\n1, Bidirectional Language Model(BLM): The BLM component of UniLM is responsible for learning the bidirectional context of the input seq. It uses a transformer encoder that processes the input tokens in both forward and backward directions. By considering the context from both directions, the model captures dependencies and relationships between words effectively.\n\n2. Unidirectional Language Model(ULM): The ULM component is used for generating text and handling unidirectional language modelling tasks. It utilizes a transformer decoder, which takes the previous tokens as input and predicts the next token in the sequence. The ULM is trained to generate coherent and fluent text based on the context provided.\n\n3. Encoder-Decoder Framework : UniLM combines both BLM and ULM by employing an encoder-decoder framework. The encoder receives the input seq and produces a contextual representation of the entire seq, capturing the bidirectional context. The decoder takes the encoder's output and generates the output sequence, focusing on the unidirectional context. The encoder and decoder are trained jointly to enhance the model's performance.\n\n4. Masked Language Model(MLM): Similar to other transformer architectures, UniLM also utilizes the MLM objective during pre-training. In MLM, a portion of the input tokens is randomly masked, and the model is trained to predict the original tokens based on the surrounding context. This encourages the model to learn meaningful representations and improve its ability to understand & gen text.\n\nThe key diff between UniLM and other architectures like GPT lies in the combo of bidirectional and unidirectional modelling. GPT primarily focuses on unidirectional language modelling thereby generating text from left to right however UniLM employs bidirectional and unidirectional models both. It employs the bidirectional context during pretraining and leverages the unidirectional context during fine-tuning and generation, making it a dependable architecture in multiple tasks.","metadata":{}},{"cell_type":"code","source":"# Load the data\nimport pandas as pd\ndata = pd.read_csv('/kaggle/input/news-summarization/data.csv')[:100] # fine-tuning on first 100 samples for Proof of concept, remove for training on complete dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:17:14.607438Z","iopub.execute_input":"2023-05-30T23:17:14.607778Z","iopub.status.idle":"2023-05-30T23:18:27.229193Z","shell.execute_reply.started":"2023-05-30T23:17:14.607751Z","shell.execute_reply":"2023-05-30T23:18:27.228123Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertModel, BertConfig\n\nclass UniLMTokenizer(BertTokenizer):\n    def __init__(self, vocab_file, do_lower_case=True):\n        super().__init__(vocab_file=vocab_file, do_lower_case=do_lower_case)\n\n#UNILM model is an extension of BERT, Feel free to modify it as per original paper\nclass UniLMModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', config=config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n\n    def forward(self, input_ids, attention_mask=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state\n        logits = self.lm_head(sequence_output)\n        return logits\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:18:27.232234Z","iopub.execute_input":"2023-05-30T23:18:27.232532Z","iopub.status.idle":"2023-05-30T23:18:38.915853Z","shell.execute_reply.started":"2023-05-30T23:18:27.232506Z","shell.execute_reply":"2023-05-30T23:18:38.914864Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:18:38.917208Z","iopub.execute_input":"2023-05-30T23:18:38.917530Z","iopub.status.idle":"2023-05-30T23:18:40.252801Z","shell.execute_reply.started":"2023-05-30T23:18:38.917495Z","shell.execute_reply":"2023-05-30T23:18:40.251660Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2023-05-30 23:18:39--  https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\nResolving huggingface.co (huggingface.co)... 13.227.219.63, 13.227.219.125, 13.227.219.41, ...\nConnecting to huggingface.co (huggingface.co)|13.227.219.63|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 231508 (226K) [text/plain]\nSaving to: ‘vocab.txt’\n\nvocab.txt           100%[===================>] 226.08K  --.-KB/s    in 0.09s   \n\n2023-05-30 23:18:40 (2.53 MB/s) - ‘vocab.txt’ saved [231508/231508]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = UniLMTokenizer(vocab_file='/kaggle/working/vocab.txt', do_lower_case=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:18:40.257136Z","iopub.execute_input":"2023-05-30T23:18:40.257451Z","iopub.status.idle":"2023-05-30T23:18:40.299344Z","shell.execute_reply.started":"2023-05-30T23:18:40.257422Z","shell.execute_reply":"2023-05-30T23:18:40.298481Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Tokenize the content and summary columns\ntokenized_inputs = data['Content'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\ntokenized_summaries = data['Summary'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:18:40.300663Z","iopub.execute_input":"2023-05-30T23:18:40.301009Z","iopub.status.idle":"2023-05-30T23:18:43.411890Z","shell.execute_reply.started":"2023-05-30T23:18:40.300976Z","shell.execute_reply":"2023-05-30T23:18:43.410948Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define the maximum sequence length\nmax_seq_length = 512\n\n# Truncate and pad the tokenized inputs and summaries\ninput_ids = []\nsummary_ids = []\n\nfor tokens in tokenized_inputs:\n    if len(tokens) > max_seq_length:\n        tokens = tokens[:max_seq_length]\n    else:\n        tokens = tokens + [0] * (max_seq_length - len(tokens))\n    input_ids.append(tokens)\n\nfor tokens in tokenized_summaries:\n    if len(tokens) > max_seq_length:\n        tokens = tokens[:max_seq_length]\n    else:\n        tokens = tokens + [0] * (max_seq_length - len(tokens))\n    summary_ids.append(tokens)\n\n# Convert the tokenized inputs and summaries to tensors\ninput_ids = torch.tensor(input_ids, dtype=torch.long)\nsummary_ids = torch.tensor(summary_ids, dtype=torch.long)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:20:45.894141Z","iopub.execute_input":"2023-05-30T23:20:45.894530Z","iopub.status.idle":"2023-05-30T23:20:45.920347Z","shell.execute_reply.started":"2023-05-30T23:20:45.894496Z","shell.execute_reply":"2023-05-30T23:20:45.919393Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Create TensorDataset\ndataset = TensorDataset(input_ids, summary_ids)\n\n# Define batch size\nbatch_size = 1\n\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:21:53.895367Z","iopub.execute_input":"2023-05-30T23:21:53.895765Z","iopub.status.idle":"2023-05-30T23:21:53.903572Z","shell.execute_reply.started":"2023-05-30T23:21:53.895733Z","shell.execute_reply":"2023-05-30T23:21:53.902605Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define the configuration for UniLM\nconfig = BertConfig(\n    hidden_size=768,\n    num_hidden_layers=12,\n    num_attention_heads=12,\n    intermediate_size=3072,\n    hidden_act=\"gelu\",\n    hidden_dropout_prob=0.1,\n    attention_probs_dropout_prob=0.1,\n    max_position_embeddings=512,\n    vocab_size=tokenizer.vocab_size\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:21:54.982422Z","iopub.execute_input":"2023-05-30T23:21:54.982780Z","iopub.status.idle":"2023-05-30T23:21:54.988978Z","shell.execute_reply.started":"2023-05-30T23:21:54.982751Z","shell.execute_reply":"2023-05-30T23:21:54.987780Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = UniLMModel(config)  # Initialize the UniLM model with the desired configuration\nloss_fn = nn.CrossEntropyLoss()  # Define the loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Define the optimizer\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:21:55.379060Z","iopub.execute_input":"2023-05-30T23:21:55.379407Z","iopub.status.idle":"2023-05-30T23:21:56.745419Z","shell.execute_reply.started":"2023-05-30T23:21:55.379378Z","shell.execute_reply":"2023-05-30T23:21:56.744453Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n\nmodel.to(device)\n\n# Training loop\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for step,batch in enumerate(dataloader):\n        input_batch, summary_batch = batch\n        input_batch = input_batch.to(device)\n        summary_batch = summary_batch.to(device)\n\n        # Clear gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(input_batch, attention_mask=input_batch.ne(0))[0]\n\n        # Compute loss\n        loss = loss_fn(outputs.view(-1, outputs.shape[-1]), summary_batch.view(-1))\n        total_loss += loss.item()\n\n        # Backward pass\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n        if step%100==0:\n            print(\"Step-{}, Loss-{}\".format(step,loss.item()))\n\n    # Calculate average loss for the epoch\n    avg_loss = total_loss / len(dataloader)\n\n    # Print the average loss\n    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:22:44.619175Z","iopub.execute_input":"2023-05-30T23:22:44.619537Z","iopub.status.idle":"2023-05-30T23:22:54.057955Z","shell.execute_reply.started":"2023-05-30T23:22:44.619507Z","shell.execute_reply":"2023-05-30T23:22:54.056974Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Step-0, Loss-1.8001112937927246\nEpoch 1/1 - Average Loss: 1.7692\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Inference\n","metadata":{}},{"cell_type":"code","source":"# Sample input text\ninput_text = \"This is a sample input text for summarization. We are going to evaluate the trained model. Let's see how it performs ! Do you have any idea ?\"\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:30:57.584535Z","iopub.execute_input":"2023-05-30T23:30:57.585205Z","iopub.status.idle":"2023-05-30T23:30:57.589984Z","shell.execute_reply.started":"2023-05-30T23:30:57.585170Z","shell.execute_reply":"2023-05-30T23:30:57.588917Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Tokenize the input text\ntokenized_input = tokenizer.encode_plus(input_text, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='pt')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:30:58.347157Z","iopub.execute_input":"2023-05-30T23:30:58.347854Z","iopub.status.idle":"2023-05-30T23:30:58.354513Z","shell.execute_reply.started":"2023-05-30T23:30:58.347819Z","shell.execute_reply":"2023-05-30T23:30:58.353490Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"\n# Move the tokenized input to the device\ninput_ids = tokenized_input['input_ids'].to(device)\nattention_mask = tokenized_input['attention_mask'].to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:30:58.724244Z","iopub.execute_input":"2023-05-30T23:30:58.724926Z","iopub.status.idle":"2023-05-30T23:30:58.731032Z","shell.execute_reply.started":"2023-05-30T23:30:58.724885Z","shell.execute_reply":"2023-05-30T23:30:58.730034Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:30:59.047925Z","iopub.execute_input":"2023-05-30T23:30:59.048291Z","iopub.status.idle":"2023-05-30T23:30:59.058184Z","shell.execute_reply.started":"2023-05-30T23:30:59.048261Z","shell.execute_reply":"2023-05-30T23:30:59.057005Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"UniLMModel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=30522, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Perform forward pass\nwith torch.no_grad():\n    outputs = model(input_ids, attention_mask=attention_mask)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:30:59.406845Z","iopub.execute_input":"2023-05-30T23:30:59.407194Z","iopub.status.idle":"2023-05-30T23:30:59.425787Z","shell.execute_reply.started":"2023-05-30T23:30:59.407167Z","shell.execute_reply":"2023-05-30T23:30:59.424943Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Reshape the outputs tensor\nreshaped_outputs = outputs.permute(0, 2, 1)  # Reshape to (batch_size, vocab_size, sequence_length)\n\n# Get the predicted summary\npredicted_summary_ids = torch.argmax(reshaped_outputs, dim=1)\npredicted_summary = tokenizer.decode(predicted_summary_ids[0], skip_special_tokens=True)\n\nprint(\"Predicted Summary:\", predicted_summary)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T23:31:00.089475Z","iopub.execute_input":"2023-05-30T23:31:00.090151Z","iopub.status.idle":"2023-05-30T23:31:00.107045Z","shell.execute_reply.started":"2023-05-30T23:31:00.090117Z","shell.execute_reply":"2023-05-30T23:31:00.106018Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Predicted Summary: \n","output_type":"stream"}]},{"cell_type":"markdown","source":"Train model more to get better summary and it's also possible that our input text is very short.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}