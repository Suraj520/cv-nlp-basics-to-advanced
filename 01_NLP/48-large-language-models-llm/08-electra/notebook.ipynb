{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Architecture Overview\n\n## Electra\n\nIt is a neural network architecture that belongs to the hierarchy of transformers family.\n\nThe building blocks of Electra is explained below\n\n1. Transformer architecture - Similar to other models like BERT, GPT2, It is built upon the transformer architecture which utilize self attention mechanisms to capture the relationships b/w words in a sentence and enable parallel procesing.\n\n2. Pre-training and Fine tuning : Like BERT and GPT2, Electra also follows a two step process. It undergoes pre-training and fine-tuning. In the pre-training phase, the model is trained over a large corpus of unlabeled text to learn the language patterns and representations. In the fine tuning phase, the model is further trained on a smaller labeled dataset for specific downstream tasks like text classification or NER.\n\n3. Masked Language Modelling vs. Discriminative Training : In BERT, a technique known as MLM is used where random words are masked and the model is asked to predict them. In contrast, Electra employs Discriminative training. Instead of masking words, It replaces some of them with plausible alternatives and tasks the model with distinguishing the original word from the replacement. This way, Electra learns to discriminate b/w real and generated tokens resulting in more efficient training.\n\n4. Generator and Discriminator: Electra consists of two components, The generator and the discriminator. The generator takes in a sentence as input and tries to predict the replaced words whereas the discriminator aims to determine whether the replaced words are real or generated. These two components work in synchronicity during training with the Discriminator providing feedback to improve generator's performance.\n\n5. Model size and training efficiency- Electra is designed to be computationally more efficient compared to some other models. For instance, models like BERT and GPT2 are known for their large size, which makes training and deployment challenging. Electra whreas achieves similar performance with smaller memory footprints.\n\n> Key differences in the architecture of Electra and other models.\n\n- With respect to ALBERT : ALBERT uses parameter sharing to reduce model size and training time whreas Electra although smaller, focuses on the Discriminative Training approach for efficiency.\n\n- With respect to ROBERTA- ROBERTA uses large batch size and corpus to train but Electra achieves similar performance with smaller resources.\n\n- With respect to T5 - T5 is a versatile model for various NLP tasks while Electra's fine tuning also makes it capable for a wide number of NLP tasks.","metadata":{}},{"cell_type":"code","source":"!7z x /kaggle/input/mercari-price-suggestion-challenge/test.tsv.7z\n!7z x /kaggle/input/mercari-price-suggestion-challenge/train.tsv.7z","metadata":{"execution":{"iopub.status.busy":"2023-05-25T20:58:10.568106Z","iopub.execute_input":"2023-05-25T20:58:10.568710Z","iopub.status.idle":"2023-05-25T20:58:10.573183Z","shell.execute_reply.started":"2023-05-25T20:58:10.568679Z","shell.execute_reply":"2023-05-25T20:58:10.572175Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"Extracting the train and test sets using 7z command line tool","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ElectraTokenizer, ElectraForSequenceClassification\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:06:41.989893Z","iopub.execute_input":"2023-05-25T21:06:41.990273Z","iopub.status.idle":"2023-05-25T21:06:44.148721Z","shell.execute_reply.started":"2023-05-25T21:06:41.990242Z","shell.execute_reply":"2023-05-25T21:06:44.147621Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"This code snippet imports the necessary libraries and modules for deep learning model training using Electra.","metadata":{}},{"cell_type":"code","source":"# Set the random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:06:51.392480Z","iopub.execute_input":"2023-05-25T21:06:51.392851Z","iopub.status.idle":"2023-05-25T21:06:51.399932Z","shell.execute_reply.started":"2023-05-25T21:06:51.392822Z","shell.execute_reply":"2023-05-25T21:06:51.398757Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"\nThese lines of code set the random seed for reproducibility.","metadata":{}},{"cell_type":"code","source":"# Load the training and test data\ntrain_df = pd.read_csv('/kaggle/working/train.tsv', delimiter='\\t')\ntest_df = pd.read_csv('/kaggle/working/test.tsv', delimiter='\\t')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:07:11.397991Z","iopub.execute_input":"2023-05-25T21:07:11.398376Z","iopub.status.idle":"2023-05-25T21:07:18.745784Z","shell.execute_reply.started":"2023-05-25T21:07:11.398344Z","shell.execute_reply":"2023-05-25T21:07:18.744741Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"These lines of code load the train and test sets","metadata":{}},{"cell_type":"code","source":"# Remove rows with missing values\ntrain_df = train_df.dropna()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:07:18.747556Z","iopub.execute_input":"2023-05-25T21:07:18.747912Z","iopub.status.idle":"2023-05-25T21:07:20.930491Z","shell.execute_reply.started":"2023-05-25T21:07:18.747880Z","shell.execute_reply":"2023-05-25T21:07:20.929529Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and validation sets\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=seed)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:07:23.678706Z","iopub.execute_input":"2023-05-25T21:07:23.679093Z","iopub.status.idle":"2023-05-25T21:07:23.980396Z","shell.execute_reply.started":"2023-05-25T21:07:23.679061Z","shell.execute_reply":"2023-05-25T21:07:23.979420Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Define the tokenizer\ntokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:07:28.760651Z","iopub.execute_input":"2023-05-25T21:07:28.761022Z","iopub.status.idle":"2023-05-25T21:07:28.952127Z","shell.execute_reply.started":"2023-05-25T21:07:28.760992Z","shell.execute_reply":"2023-05-25T21:07:28.951203Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# Tokenize the input data\ndef tokenize_data(text):\n    return tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=256,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:07:34.662772Z","iopub.execute_input":"2023-05-25T21:07:34.663161Z","iopub.status.idle":"2023-05-25T21:07:34.668981Z","shell.execute_reply.started":"2023-05-25T21:07:34.663129Z","shell.execute_reply":"2023-05-25T21:07:34.667882Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# Create PyTorch DataLoader for training and validation sets\nclass MercariDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        text = row['name'] + ' ' + row['item_description']\n        inputs = tokenize_data(text)\n        label = row['price']\n        return {\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.float32)\n        }\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:07:45.519693Z","iopub.execute_input":"2023-05-25T21:07:45.520082Z","iopub.status.idle":"2023-05-25T21:07:45.527237Z","shell.execute_reply.started":"2023-05-25T21:07:45.520051Z","shell.execute_reply":"2023-05-25T21:07:45.526206Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"# Define batch size and create data loaders\nbatch_size = 32\ntrain_dataset = MercariDataset(train_data)\nval_dataset = MercariDataset(val_data)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:07:50.885428Z","iopub.execute_input":"2023-05-25T21:07:50.885790Z","iopub.status.idle":"2023-05-25T21:07:51.041902Z","shell.execute_reply.started":"2023-05-25T21:07:50.885757Z","shell.execute_reply":"2023-05-25T21:07:51.040840Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"for batch in train_loader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:08:03.584837Z","iopub.execute_input":"2023-05-25T21:08:03.585218Z","iopub.status.idle":"2023-05-25T21:08:03.700986Z","shell.execute_reply.started":"2023-05-25T21:08:03.585185Z","shell.execute_reply":"2023-05-25T21:08:03.699864Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  101,  2417,  2152,  ...,     0,     0,     0],\n        [  101,  6954,  2072,  ...,     0,     0,     0],\n        [  101, 21994, 19457,  ...,     0,     0,     0],\n        ...,\n        [  101,  2047,  2141,  ...,     0,     0,     0],\n        [  101,  1038,  2989,  ...,     0,     0,     0],\n        [  101,  9212,  2884,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'label': tensor([ 46.,   3.,  30.,  37.,  20.,  17.,  10.,  31.,  10., 895.,  19.,  61.,\n         14.,  96.,  90.,  23.,  13.,  74.,  19.,  26., 525.,  41.,   8.,  11.,\n         24.,  16.,  30.,  46.,  20.,  20.,  26.,  50.])}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the ELECTRA model\nmodel = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:08:09.180666Z","iopub.execute_input":"2023-05-25T21:08:09.181106Z","iopub.status.idle":"2023-05-25T21:08:11.498105Z","shell.execute_reply.started":"2023-05-25T21:08:09.181068Z","shell.execute_reply":"2023-05-25T21:08:11.497175Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:08:15.799863Z","iopub.execute_input":"2023-05-25T21:08:15.800317Z","iopub.status.idle":"2023-05-25T21:08:15.814821Z","shell.execute_reply.started":"2023-05-25T21:08:15.800283Z","shell.execute_reply":"2023-05-25T21:08:15.813800Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"len(train_loader), len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:10:29.674130Z","iopub.execute_input":"2023-05-25T21:10:29.674519Z","iopub.status.idle":"2023-05-25T21:10:29.681211Z","shell.execute_reply.started":"2023-05-25T21:10:29.674491Z","shell.execute_reply":"2023-05-25T21:10:29.680332Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"(21175, 5294)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Train the model\nnum_epochs = 1\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    \n    for step,batch in tqdm(enumerate(train_loader)):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        print(step)\n        if step%500==0:\n            print(\"Step-{}, Loss-{}\".format(step,loss.item()))\n            break # breaking the training at 500th step since 1 iteration may take around 5 hrs, Uncomment this during full training\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * input_ids.size(0)\n    \n    train_loss /= len(train_dataset)\n    \n    # Evaluate on the validation set\n    model.eval()\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for step,batch in tqdm(enumerate(val_loader)):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            \n            val_loss += loss.item() * input_ids.size(0)\n    \n    val_loss /= len(val_dataset)\n    \n    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:17:24.533884Z","iopub.execute_input":"2023-05-25T21:17:24.534443Z","iopub.status.idle":"2023-05-25T21:18:02.727724Z","shell.execute_reply.started":"2023-05-25T21:17:24.534411Z","shell.execute_reply":"2023-05-25T21:18:02.726351Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"0\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0, Loss-2053.11083984375\n","output_type":"stream"},{"name":"stderr","text":"123it [00:37,  3.27it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[114], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     43\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 45\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m val_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_dataset)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"This code block trains a model using a loop that iterates over a specified number of epochs. It uses the tqdm library to display a progress bar during training. The model is trained on a GPU if available; otherwise, it falls back to CPU.\n\nWithin each epoch, the code iterates over the train_loader to process batches of training data. It moves the input data and labels to the appropriate device (GPU or CPU) and performs forward and backward passes through the model. The optimizer is used to update the model's parameters based on the computed gradients. The training loss is accumulated and averaged over the entire training dataset.\n\nAfter each epoch's training, the code enters the evaluation phase on the validation set (val_loader). It iterates over the validation batches, performs forward passes through the model, and calculates the loss. The validation loss is accumulated and averaged over the entire validation dataset.\n\nAt the end of each epoch, the code prints the epoch number, the training loss, and the validation loss.","metadata":{}},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# Load the test data and create a DataLoader\ntest_dataset = MercariDataset(test_df)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128*8)\n\n# Generate predictions on the test set\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        \n        predictions.extend(logits.flatten().cpu().numpy())\n\n# Create the submission file\nsubmission_df = pd.DataFrame({'test_id': test_df['test_id'], 'price': predictions})\nsubmission_df.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T21:23:57.540791Z","iopub.execute_input":"2023-05-25T21:23:57.541156Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  7%|▋         | 49/678 [07:33<1:36:29,  9.20s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"The code provided is used to generate predictions on the test set using a pre-trained model and create a submission file for a competition. \n\n\nThe model is then put into evaluation mode using `model.eval()`, and predictions are generated for each batch in the test loader. The input tensors (`input_ids` and `attention_mask`) are moved to the appropriate device using `.to(device)`.\n\nThe model's forward pass is executed with the input tensors, and the output logits are obtained from `outputs.logits`. The logits are then flattened, converted to NumPy arrays on the CPU, and appended to the `predictions` list.\n\nOnce all the predictions are generated, a submission DataFrame is created with columns for `test_id` (assuming it's available in `test_df`) and `price`, using the `predictions` list. Finally, the submission DataFrame is saved as a CSV file called 'submission.csv' without including an index.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}