{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<div style=\"background-color:#035FCA; color:#19180F; font-size:40px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> T5 </div>\n<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> Architecture Overview.\n </div>\n<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nAt a high level, T5 is built using a transformer based architecture. It is different from other architectures like BERT, RoBERTa, GPT-2 and XLNET in several ways.<br>\n\n- The building blocks of T5 are similar to those of other transformer based models. It consists of an encoder and a decoder. The encoder takes in the input seq and generates a hidden representation of the input. The decoder takes in the hidden representation and generates the output sequence.<br>\n\n- The encoder and decoder are connected by an attention mechanism that allows the decoder to attend to different parts of the input sequence.<br>\n\n- One of the key differences between T5 and other architectures like BERT,RoBERTa, GPT2 and XLNet is that T5 is a text to text model. This means that it can be trained on a wide range of nlp taks by simply changing the input and output format.<br> \n\n- For example, It can be trained on machine translation by providing it with a source language sentence and a target language sentence as input-output pair. It can also be trained on summarization by providing it with a long document as input and short summary as output.\n<br>\n- T5 uses a pretraining and fine tuning approach. During pre-training, T5 is trained on a large corpus of text using a masked language modelling objective whereas during the fine tuning stage, The pre-trained model is fine tuned on a specific task by providing it with task-specific input and output pairs.<br>\n\n- T5 also uses a technique called task-specific prompts, which allows it to perform well on a wide range of tasks with minimal task specific training. The prompts are short text strings that are concatenated to the input sequence to provide task specific information to the model.<br>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> Architecture Diagram.\n </div>\n<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n    \n1. Input Layer: This layer represents the input text that is to be processed by the T5 model.<br>\n\n2. Tokenizer: The input text is passed through the tokenizer, which converts it into a sequence of tokens. The tokenizer breaks down the text into meaningful units, such as words or subwords, and assigns a unique token to each unit.<br>\n\n3. Tokens: The tokens obtained from the tokenizer are represented in this block. These tokenized representations of the input text form the input for the subsequent layers.<br>\n\n4. Encoder-Decoder Layers: This section includes the encoder and decoder components of the T5 model.<br>\n\n   a. Encoder: The encoder processes the tokenized input text. It consists of several layers, each containing a self-attention mechanism followed by a feed-forward network. The self-attention mechanism allows the model to weigh the importance of different tokens in the input sequence, capturing the relationships between them. The feed-forward network helps in transforming the representation of each token. Residual connections and layer normalization are applied within each encoder layer to improve the flow of information and stabilize the learning process.<br>\n\n   b. Decoder: The decoder takes as input the outputs from the encoder and is responsible for generating the output sequence. It also consists of multiple layers, each comprising a self-attention mechanism, an encoder-decoder attention mechanism to capture relevant information from the encoder outputs, and a feed-forward network. Similar to the encoder, residual connections and layer normalization are applied within each decoder layer.<br>\n\n5. Output Layer: This layer represents the generated output text. It is the result of the decoding process in the T5 model.<br>\n\nThe connections between the different components illustrate the flow of information through the model. The input text is passed through the tokenizer, and the resulting tokens are fed into the encoder. The encoder processes the tokens and produces representations, which are then used by the decoder. The decoder generates the output sequence, which is represented in the output layer.<br></div>","metadata":{}},{"cell_type":"code","source":"from IPython.display import SVG, display\n\n# Load the SVG file and display it\nsvg_file = '/kaggle/input/notebook-images/t5.svg'\ndisplay(SVG(filename=svg_file))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:09:58.040184Z","iopub.execute_input":"2023-06-09T02:09:58.040559Z","iopub.status.idle":"2023-06-09T02:09:58.066403Z","shell.execute_reply.started":"2023-06-09T02:09:58.040531Z","shell.execute_reply":"2023-06-09T02:09:58.065201Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"711pt\" height=\"689pt\" viewBox=\"0.00 0.00 711.24 688.80\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 684.8)\">\n<title>t5</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-684.8 707.2369,-684.8 707.2369,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_input</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"321,-596 321,-672.8 411,-672.8 411,-596 321,-596\"/>\n<text text-anchor=\"middle\" x=\"366\" y=\"-656.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Layer</text>\n</g>\n<g id=\"clust2\" class=\"cluster\">\n<title>cluster_tokenizer</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"306,-439.2 306,-588 426,-588 426,-439.2 306,-439.2\"/>\n<text text-anchor=\"middle\" x=\"366\" y=\"-571.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Tokenizer</text>\n</g>\n<g id=\"clust3\" class=\"cluster\">\n<title>cluster_enc_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-8 8,-431.2 549,-431.2 549,-8 8,-8\"/>\n<text text-anchor=\"middle\" x=\"278.5\" y=\"-414.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder-Decoder Layers</text>\n</g>\n<g id=\"clust4\" class=\"cluster\">\n<title>cluster_encoder</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"16,-160 16,-398.4 180,-398.4 180,-160 16,-160\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-381.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder</text>\n</g>\n<g id=\"clust5\" class=\"cluster\">\n<title>cluster_decoder</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"188,-16 188,-236.8 541,-236.8 541,-16 188,-16\"/>\n<text text-anchor=\"middle\" x=\"364.5\" y=\"-220.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder</text>\n</g>\n<g id=\"clust6\" class=\"cluster\">\n<title>cluster_output</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"580,-244.8 580,-321.6 680,-321.6 680,-244.8 580,-244.8\"/>\n<text text-anchor=\"middle\" x=\"630\" y=\"-305\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Output Layer</text>\n</g>\n<!-- input -->\n<g id=\"node1\" class=\"node\">\n<title>input</title>\n<text text-anchor=\"middle\" x=\"366\" y=\"-617.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Text</text>\n</g>\n<!-- tokenizer -->\n<g id=\"node2\" class=\"node\">\n<title>tokenizer</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"416.6383,-555.2 315.3617,-555.2 315.3617,-519.2 416.6383,-519.2 416.6383,-555.2\"/>\n<text text-anchor=\"middle\" x=\"366\" y=\"-533\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Tokenize Input</text>\n</g>\n<!-- input&#45;&gt;tokenizer -->\n<g id=\"edge11\" class=\"edge\">\n<title>input-&gt;tokenizer</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M366,-603.5997C366,-592.4112 366,-577.8983 366,-565.2954\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"369.5001,-565.2731 366,-555.2731 362.5001,-565.2731 369.5001,-565.2731\"/>\n</g>\n<!-- tokens -->\n<g id=\"node3\" class=\"node\">\n<title>tokens</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"418.2451,-483.2 313.7549,-483.2 313.7549,-447.2 418.2451,-447.2 418.2451,-483.2\"/>\n<text text-anchor=\"middle\" x=\"366\" y=\"-461\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Tokenized Text</text>\n</g>\n<!-- tokenizer&#45;&gt;tokens -->\n<g id=\"edge12\" class=\"edge\">\n<title>tokenizer-&gt;tokens</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M366,-519.0314C366,-511.331 366,-502.1743 366,-493.6166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"369.5001,-493.6132 366,-483.6133 362.5001,-493.6133 369.5001,-493.6132\"/>\n</g>\n<!-- encoder_self_attention -->\n<g id=\"node4\" class=\"node\">\n<title>encoder_self_attention</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"161.0926,-365.6 64.9074,-365.6 64.9074,-329.6 161.0926,-329.6 161.0926,-365.6\"/>\n<text text-anchor=\"middle\" x=\"113\" y=\"-343.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- tokens&#45;&gt;encoder_self_attention -->\n<g id=\"edge1\" class=\"edge\">\n<title>tokens-&gt;encoder_self_attention</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M327.0232,-447.0827C282.4513,-426.3647 209.4451,-392.4298 161.3023,-370.052\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"162.5646,-366.7792 152.0211,-365.7379 159.614,-373.1269 162.5646,-366.7792\"/>\n</g>\n<!-- cluster_enc_dec -->\n<g id=\"node12\" class=\"node\">\n<title>cluster_enc_dec</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"630\" cy=\"-347.6\" rx=\"73.4745\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"630\" y=\"-343.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cluster_enc_dec</text>\n</g>\n<!-- tokens&#45;&gt;cluster_enc_dec -->\n<g id=\"edge13\" class=\"edge\">\n<title>tokens-&gt;cluster_enc_dec</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M406.6714,-447.0827C454.9087,-425.5952 535.0615,-389.8908 585.0849,-367.6076\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"586.6542,-370.7402 594.3647,-363.4739 583.8058,-364.3459 586.6542,-370.7402\"/>\n</g>\n<!-- encoder_add_norm -->\n<g id=\"node6\" class=\"node\">\n<title>encoder_add_norm</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"166.2976,-288.8 39.7024,-288.8 39.7024,-252.8 166.2976,-252.8 166.2976,-288.8\"/>\n<text text-anchor=\"middle\" x=\"103\" y=\"-266.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- encoder_self_attention&#45;&gt;encoder_add_norm -->\n<g id=\"edge2\" class=\"edge\">\n<title>encoder_self_attention-&gt;encoder_add_norm</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M110.6301,-329.3995C109.447,-320.3132 107.9893,-309.1176 106.666,-298.9549\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.1148,-298.3342 105.3528,-288.8699 103.1734,-299.2381 110.1148,-298.3342\"/>\n</g>\n<!-- encoder_feed_forward -->\n<g id=\"node5\" class=\"node\">\n<title>encoder_feed_forward</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"171.7766,-204 24.2234,-204 24.2234,-168 171.7766,-168 171.7766,-204\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-181.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Feed-Forward Network</text>\n</g>\n<!-- encoder_feed_forward&#45;&gt;encoder_add_norm -->\n<g id=\"edge4\" class=\"edge\">\n<title>encoder_feed_forward-&gt;encoder_add_norm</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M104.7108,-204.0731C106.5088,-215.196 107.6202,-229.6946 107.7366,-242.3344\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.2361,-242.3509 107.5935,-252.3997 111.2354,-242.4505 104.2361,-242.3509\"/>\n</g>\n<!-- encoder_add_norm&#45;&gt;encoder_feed_forward -->\n<g id=\"edge3\" class=\"edge\">\n<title>encoder_add_norm-&gt;encoder_feed_forward</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M96.2367,-252.3997C94.4521,-241.2112 93.3571,-226.6983 93.2603,-214.0954\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.7601,-214.1278 93.4205,-204.0731 89.761,-214.0159 96.7601,-214.1278\"/>\n</g>\n<!-- decoder_self_attention -->\n<g id=\"node7\" class=\"node\">\n<title>decoder_self_attention</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"311.0926,-204 214.9074,-204 214.9074,-168 311.0926,-168 311.0926,-204\"/>\n<text text-anchor=\"middle\" x=\"263\" y=\"-181.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- encoder_add_norm&#45;&gt;decoder_self_attention -->\n<g id=\"edge5\" class=\"edge\">\n<title>encoder_add_norm-&gt;decoder_self_attention</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M149.9036,-252.7428C161.244,-247.9332 173.203,-242.4821 184,-236.8 199.1514,-228.8263 215.1639,-218.8475 228.7823,-209.8499\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.9918,-212.5826 237.3527,-204.1097 227.0964,-206.7666 230.9918,-212.5826\"/>\n</g>\n<!-- decoder_add_norm -->\n<g id=\"node10\" class=\"node\">\n<title>decoder_add_norm</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"345.2976,-132 218.7024,-132 218.7024,-96 345.2976,-96 345.2976,-132\"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- decoder_self_attention&#45;&gt;decoder_add_norm -->\n<g id=\"edge6\" class=\"edge\">\n<title>decoder_self_attention-&gt;decoder_add_norm</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M267.7945,-167.8314C269.8489,-160.0463 272.296,-150.7729 274.5756,-142.1347\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"277.9735,-142.9753 277.1409,-132.4133 271.2052,-141.1892 277.9735,-142.9753\"/>\n</g>\n<!-- decoder_encoder_attention -->\n<g id=\"node8\" class=\"node\">\n<title>decoder_encoder_attention</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"367.5976,-60 196.4024,-60 196.4024,-24 367.5976,-24 367.5976,-60\"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder-Decoder Attention</text>\n</g>\n<!-- decoder_encoder_attention&#45;&gt;decoder_add_norm -->\n<g id=\"edge8\" class=\"edge\">\n<title>decoder_encoder_attention-&gt;decoder_add_norm</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M287.976,-60.4133C288.7071,-68.0593 288.9203,-77.1084 288.6155,-85.5726\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"285.1048,-85.6264 287.9524,-95.8314 292.0902,-86.078 285.1048,-85.6264\"/>\n</g>\n<!-- decoder_feed_forward -->\n<g id=\"node9\" class=\"node\">\n<title>decoder_feed_forward</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"532.7766,-60 385.2234,-60 385.2234,-24 532.7766,-24 532.7766,-60\"/>\n<text text-anchor=\"middle\" x=\"459\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Feed-Forward Network</text>\n</g>\n<!-- decoder_feed_forward&#45;&gt;decoder_add_norm -->\n<g id=\"edge10\" class=\"edge\">\n<title>decoder_feed_forward-&gt;decoder_add_norm</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M420.5962,-60.038C397.618,-69.775 367.995,-81.869 342.0077,-92.1383\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"340.6422,-88.9143 332.617,-95.8314 343.2041,-95.4286 340.6422,-88.9143\"/>\n</g>\n<!-- decoder_add_norm&#45;&gt;decoder_encoder_attention -->\n<g id=\"edge7\" class=\"edge\">\n<title>decoder_add_norm-&gt;decoder_encoder_attention</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M276.0476,-95.8314C275.2972,-88.131 275.0763,-78.9743 275.3849,-70.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"278.8792,-70.6161 276.024,-60.4133 271.8935,-70.1697 278.8792,-70.6161\"/>\n</g>\n<!-- decoder_add_norm&#45;&gt;decoder_feed_forward -->\n<g id=\"edge9\" class=\"edge\">\n<title>decoder_add_norm-&gt;decoder_feed_forward</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M320.7122,-95.8314C343.7496,-86.0734 373.3858,-73.977 399.3404,-63.7242\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"400.6909,-66.9541 408.717,-60.038 398.1298,-60.4394 400.6909,-66.9541\"/>\n</g>\n<!-- output -->\n<g id=\"node11\" class=\"node\">\n<title>output</title>\n<text text-anchor=\"middle\" x=\"630\" y=\"-266.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Output Text</text>\n</g>\n<!-- cluster_enc_dec&#45;&gt;output -->\n<g id=\"edge14\" class=\"edge\">\n<title>cluster_enc_dec-&gt;output</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M630,-329.3995C630,-320.3132 630,-309.1176 630,-298.9549\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"633.5001,-298.8698 630,-288.8699 626.5001,-298.8699 633.5001,-298.8698\"/>\n</g>\n</g>\n</svg>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">Importing modules\n</div>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom transformers import T5Tokenizer, TFT5Model\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:38:17.613250Z","iopub.execute_input":"2023-06-09T01:38:17.615649Z","iopub.status.idle":"2023-06-09T01:38:23.522184Z","shell.execute_reply.started":"2023-06-09T01:38:17.615611Z","shell.execute_reply":"2023-06-09T01:38:23.521237Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nDefining the num of gpus to be used </div>","metadata":{}},{"cell_type":"code","source":"num_gpus = 2\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:38:23.535442Z","iopub.execute_input":"2023-06-09T01:38:23.535793Z","iopub.status.idle":"2023-06-09T01:38:23.542796Z","shell.execute_reply.started":"2023-06-09T01:38:23.535762Z","shell.execute_reply":"2023-06-09T01:38:23.540000Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe following code snippets loads the training and test data from CSV files using pandas and splits the training data into input (question + answer) and target variables.\n</div>    \n","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest_data = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:38:23.544188Z","iopub.execute_input":"2023-06-09T01:38:23.545024Z","iopub.status.idle":"2023-06-09T01:38:23.770000Z","shell.execute_reply.started":"2023-06-09T01:38:23.544981Z","shell.execute_reply":"2023-06-09T01:38:23.769023Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X = train_data['question_title'] + ' ' + train_data['question_body'] + ' ' + train_data['answer']\ny = train_data.iloc[:, 11:]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:38:23.773543Z","iopub.execute_input":"2023-06-09T01:38:23.774021Z","iopub.status.idle":"2023-06-09T01:38:23.798730Z","shell.execute_reply.started":"2023-06-09T01:38:23.773974Z","shell.execute_reply":"2023-06-09T01:38:23.797759Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n\nThe input data is tokenized using the T5 tokenizer from the Transformers library. The tokenizer converts the text into a numerical representation suitable for input to the T5 model.</div>","metadata":{}},{"cell_type":"code","source":"# Tokenize the input data\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\nX_encoded = tokenizer.batch_encode_plus(\n    X.tolist(),\n    padding='longest',\n    truncation=True,\n    return_tensors='tf'\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:38:23.800131Z","iopub.execute_input":"2023-06-09T01:38:23.800910Z","iopub.status.idle":"2023-06-09T01:38:46.909638Z","shell.execute_reply.started":"2023-06-09T01:38:23.800874Z","shell.execute_reply":"2023-06-09T01:38:46.908652Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe tokenized input data and target variables are split into training and validation sets using the train_test_split function from scikit-learn.\n</div>    \n","metadata":{}},{"cell_type":"code","source":"# Convert the tensor array to a numpy array of integers\nX_encoded_ids = np.array(X_encoded['input_ids'])\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_encoded_ids, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:38:46.911851Z","iopub.execute_input":"2023-06-09T01:38:46.912301Z","iopub.status.idle":"2023-06-09T01:38:46.927474Z","shell.execute_reply.started":"2023-06-09T01:38:46.912266Z","shell.execute_reply":"2023-06-09T01:38:46.926602Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThis snippet defines the architecture of the T5 model using the TFT5Model class from the Transformers library. The model takes the tokenized input IDs as input and produces a sequence of hidden states. The final hidden state corresponding to the first token is extracted and passed through a dense layer with sigmoid activation to obtain the model's output.\n</div>    \n","metadata":{}},{"cell_type":"code","source":"# Define the T5 model architecture\ninput_ids = Input(shape=(X_encoded['input_ids'].shape[1],), dtype='int32')\ndecoder_input_ids = Input(shape=(X_encoded['input_ids'].shape[1],), dtype='int32')\nt5_model = TFT5Model.from_pretrained('t5-base')\noutput = t5_model(input_ids=input_ids, decoder_input_ids=decoder_input_ids).last_hidden_state[:, 0, :]\noutput = Dense(y_train.shape[1], activation='sigmoid')(output)\nmodel = Model(inputs=[input_ids, decoder_input_ids], outputs=output)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:38:46.929040Z","iopub.execute_input":"2023-06-09T01:38:46.929676Z","iopub.status.idle":"2023-06-09T01:39:02.317438Z","shell.execute_reply.started":"2023-06-09T01:38:46.929619Z","shell.execute_reply":"2023-06-09T01:39:02.316425Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFT5Model.\n\nAll the weights of TFT5Model were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe model is compiled with the Adam optimizer and binary cross-entropy loss. It is then trained on the training data using the fit method, with the validation data used for monitoring the model's performance during training.</div>\n\n","metadata":{}},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:39:02.318862Z","iopub.execute_input":"2023-06-09T01:39:02.319219Z","iopub.status.idle":"2023-06-09T01:39:02.350088Z","shell.execute_reply.started":"2023-06-09T01:39:02.319188Z","shell.execute_reply":"2023-06-09T01:39:02.349123Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nA distribution strategy is defined to enable multi-GPU training. The MirroredStrategy from TensorFlow is used, which replicates the model across multiple GPUs and synchronizes their updates.\n</div>    \n","metadata":{}},{"cell_type":"code","source":"# Define the distribution strategy for multi-GPU training\nstrategy = tf.distribute.MirroredStrategy()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:39:02.351287Z","iopub.execute_input":"2023-06-09T01:39:02.351653Z","iopub.status.idle":"2023-06-09T01:39:02.665058Z","shell.execute_reply.started":"2023-06-09T01:39:02.351621Z","shell.execute_reply":"2023-06-09T01:39:02.664035Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe model is wrapped with the distribution strategy using the strategy.scope() context manager, which creates the distributed model and compiles it. This allows the model to be trained on multiple GPUs.\n</div>\n\n   ","metadata":{}},{"cell_type":"code","source":"# Create and compile the distributed model\nwith strategy.scope():\n    distributed_model = model\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:39:02.666410Z","iopub.execute_input":"2023-06-09T01:39:02.666845Z","iopub.status.idle":"2023-06-09T01:39:02.672871Z","shell.execute_reply.started":"2023-06-09T01:39:02.666811Z","shell.execute_reply":"2023-06-09T01:39:02.671361Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Compile and train the model\ndistributed_model.compile(optimizer='adam', loss='binary_crossentropy')\ndistributed_model.fit(\n    [X_train, X_train],\n    y_train,\n    validation_data=([X_val, X_val], y_val),\n    batch_size=1 * strategy.num_replicas_in_sync,\n    epochs=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T01:39:02.674345Z","iopub.execute_input":"2023-06-09T01:39:02.675464Z","iopub.status.idle":"2023-06-09T02:07:09.255797Z","shell.execute_reply.started":"2023-06-09T01:39:02.675431Z","shell.execute_reply":"2023-06-09T02:07:09.254758Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"2432/2432 [==============================] - 1635s 642ms/step - loss: 0.4144 - val_loss: 0.4052\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7b9b82266470>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe test data is tokenized using the same tokenizer used for the training data. The tokenized input IDs are then passed to the trained distributed model to make predictions on the test data.</div>","metadata":{}},{"cell_type":"code","source":"# Tokenize the test data\nX_test = test_data['question_title'] + ' ' + test_data['question_body'] + ' ' + test_data['answer']\nX_test_encoded = tokenizer.batch_encode_plus(\n    X_test.tolist(),\n    padding='longest',\n    truncation=True,\n    return_tensors='tf'\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:07:09.257491Z","iopub.execute_input":"2023-06-09T02:07:09.257867Z","iopub.status.idle":"2023-06-09T02:07:10.817907Z","shell.execute_reply.started":"2023-06-09T02:07:09.257833Z","shell.execute_reply":"2023-06-09T02:07:10.816950Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Convert the tensor array to a numpy array of integers\nX_test_encoded_ids = np.array(X_test_encoded['input_ids'])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:07:10.822028Z","iopub.execute_input":"2023-06-09T02:07:10.822340Z","iopub.status.idle":"2023-06-09T02:07:10.829075Z","shell.execute_reply.started":"2023-06-09T02:07:10.822297Z","shell.execute_reply":"2023-06-09T02:07:10.828069Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test data\npredictions = distributed_model.predict([X_test_encoded_ids, X_test_encoded_ids])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:07:10.830457Z","iopub.execute_input":"2023-06-09T02:07:10.830920Z","iopub.status.idle":"2023-06-09T02:07:57.918309Z","shell.execute_reply.started":"2023-06-09T02:07:10.830888Z","shell.execute_reply":"2023-06-09T02:07:57.917202Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"15/15 [==============================] - 47s 3s/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe predicted values are used to create a DataFrame for the submission file. The predictions are organized in columns corresponding to the target variables, and the qa_id column from the test data is included for identification.</div>","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame for the submission file\nsubmission_df = pd.DataFrame(predictions, columns=y_train.columns)\nsubmission_df.insert(0, 'qa_id', test_data['qa_id'])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:07:57.920192Z","iopub.execute_input":"2023-06-09T02:07:57.920601Z","iopub.status.idle":"2023-06-09T02:07:57.926887Z","shell.execute_reply.started":"2023-06-09T02:07:57.920565Z","shell.execute_reply":"2023-06-09T02:07:57.925621Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe submission DataFrame is saved as a CSV file named \"submission.csv\" without including the index column.</div>","metadata":{}},{"cell_type":"code","source":"# Save the submission DataFrame to a CSV file\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:07:57.928379Z","iopub.execute_input":"2023-06-09T02:07:57.929077Z","iopub.status.idle":"2023-06-09T02:07:57.965566Z","shell.execute_reply.started":"2023-06-09T02:07:57.929026Z","shell.execute_reply":"2023-06-09T02:07:57.964598Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:07:57.966985Z","iopub.execute_input":"2023-06-09T02:07:57.967340Z","iopub.status.idle":"2023-06-09T02:07:58.004561Z","shell.execute_reply.started":"2023-06-09T02:07:57.967309Z","shell.execute_reply":"2023-06-09T02:07:58.003413Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"     qa_id  question_asker_intent_understanding  question_body_critical  \\\n0       39                             0.925472                0.749821   \n1       46                             0.857872                0.519394   \n2       70                             0.924793                0.740343   \n3      132                             0.865061                0.517303   \n4      200                             0.913985                0.651699   \n..     ...                                  ...                     ...   \n471   9569                             0.888979                0.574700   \n472   9590                             0.862380                0.514621   \n473   9597                             0.901031                0.598261   \n474   9623                             0.920093                0.732291   \n475   9640                             0.886796                0.574523   \n\n     question_conversational  question_expect_short_answer  \\\n0                   0.113638                      0.763708   \n1                   0.014608                      0.721294   \n2                   0.122803                      0.726969   \n3                   0.009042                      0.720124   \n4                   0.094470                      0.706027   \n..                       ...                           ...   \n471                 0.023743                      0.695837   \n472                 0.006702                      0.729544   \n473                 0.063027                      0.709439   \n474                 0.104342                      0.728492   \n475                 0.023442                      0.697757   \n\n     question_fact_seeking  question_has_commonly_accepted_answer  \\\n0                 0.714202                               0.707339   \n1                 0.785923                               0.851384   \n2                 0.709181                               0.703232   \n3                 0.786819                               0.856836   \n4                 0.683450                               0.656764   \n..                     ...                                    ...   \n471               0.744091                               0.768347   \n472               0.802338                               0.877330   \n473               0.679836                               0.674037   \n474               0.724560                               0.729168   \n475               0.733977                               0.760781   \n\n     question_interestingness_others  question_interestingness_self  \\\n0                           0.681175                       0.611606   \n1                           0.566530                       0.435812   \n2                           0.671121                       0.597929   \n3                           0.558204                       0.417571   \n4                           0.645682                       0.581313   \n..                               ...                            ...   \n471                         0.587762                       0.482918   \n472                         0.548557                       0.396558   \n473                         0.623549                       0.555399   \n474                         0.663263                       0.582019   \n475                         0.587043                       0.484391   \n\n     question_multi_intent  ...  question_well_written  answer_helpful  \\\n0                 0.300275  ...               0.885827        0.918610   \n1                 0.175089  ...               0.732898        0.948181   \n2                 0.289076  ...               0.882591        0.902143   \n3                 0.142056  ...               0.722715        0.954710   \n4                 0.318228  ...               0.822887        0.908777   \n..                     ...  ...                    ...             ...   \n471               0.203907  ...               0.766584        0.939587   \n472               0.123524  ...               0.716780        0.958904   \n473               0.300314  ...               0.789879        0.920005   \n474               0.274256  ...               0.878721        0.908156   \n475               0.205417  ...               0.763548        0.938247   \n\n     answer_level_of_information  answer_plausible  answer_relevance  \\\n0                       0.675812          0.952385          0.965131   \n1                       0.667617          0.970629          0.979528   \n2                       0.665981          0.944119          0.956353   \n3                       0.662942          0.975540          0.982992   \n4                       0.664382          0.946114          0.959311   \n..                           ...               ...               ...   \n471                     0.659721          0.965172          0.975156   \n472                     0.663152          0.978092          0.984923   \n473                     0.666057          0.953035          0.964848   \n474                     0.665253          0.947283          0.958930   \n475                     0.658836          0.965292          0.974650   \n\n     answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n0               0.851143                  0.193529               0.091380   \n1               0.886912                  0.745296               0.125198   \n2               0.843788                  0.198007               0.098120   \n3               0.889610                  0.801359               0.123226   \n4               0.833658                  0.285771               0.123592   \n..                   ...                       ...                    ...   \n471             0.864806                  0.660653               0.140083   \n472             0.898048                  0.837145               0.117742   \n473             0.834566                  0.387828               0.134454   \n474             0.850710                  0.237153               0.097485   \n475             0.859920                  0.662730               0.144258   \n\n     answer_type_reason_explanation  answer_well_written  \n0                          0.572007             0.922787  \n1                          0.467727             0.917758  \n2                          0.580198             0.912617  \n3                          0.433160             0.920122  \n4                          0.551567             0.911655  \n..                              ...                  ...  \n471                        0.462654             0.913447  \n472                        0.412991             0.920057  \n473                        0.530733             0.912921  \n474                        0.566745             0.913482  \n475                        0.463926             0.912668  \n\n[476 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qa_id</th>\n      <th>question_asker_intent_understanding</th>\n      <th>question_body_critical</th>\n      <th>question_conversational</th>\n      <th>question_expect_short_answer</th>\n      <th>question_fact_seeking</th>\n      <th>question_has_commonly_accepted_answer</th>\n      <th>question_interestingness_others</th>\n      <th>question_interestingness_self</th>\n      <th>question_multi_intent</th>\n      <th>...</th>\n      <th>question_well_written</th>\n      <th>answer_helpful</th>\n      <th>answer_level_of_information</th>\n      <th>answer_plausible</th>\n      <th>answer_relevance</th>\n      <th>answer_satisfaction</th>\n      <th>answer_type_instructions</th>\n      <th>answer_type_procedure</th>\n      <th>answer_type_reason_explanation</th>\n      <th>answer_well_written</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>0.925472</td>\n      <td>0.749821</td>\n      <td>0.113638</td>\n      <td>0.763708</td>\n      <td>0.714202</td>\n      <td>0.707339</td>\n      <td>0.681175</td>\n      <td>0.611606</td>\n      <td>0.300275</td>\n      <td>...</td>\n      <td>0.885827</td>\n      <td>0.918610</td>\n      <td>0.675812</td>\n      <td>0.952385</td>\n      <td>0.965131</td>\n      <td>0.851143</td>\n      <td>0.193529</td>\n      <td>0.091380</td>\n      <td>0.572007</td>\n      <td>0.922787</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>46</td>\n      <td>0.857872</td>\n      <td>0.519394</td>\n      <td>0.014608</td>\n      <td>0.721294</td>\n      <td>0.785923</td>\n      <td>0.851384</td>\n      <td>0.566530</td>\n      <td>0.435812</td>\n      <td>0.175089</td>\n      <td>...</td>\n      <td>0.732898</td>\n      <td>0.948181</td>\n      <td>0.667617</td>\n      <td>0.970629</td>\n      <td>0.979528</td>\n      <td>0.886912</td>\n      <td>0.745296</td>\n      <td>0.125198</td>\n      <td>0.467727</td>\n      <td>0.917758</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70</td>\n      <td>0.924793</td>\n      <td>0.740343</td>\n      <td>0.122803</td>\n      <td>0.726969</td>\n      <td>0.709181</td>\n      <td>0.703232</td>\n      <td>0.671121</td>\n      <td>0.597929</td>\n      <td>0.289076</td>\n      <td>...</td>\n      <td>0.882591</td>\n      <td>0.902143</td>\n      <td>0.665981</td>\n      <td>0.944119</td>\n      <td>0.956353</td>\n      <td>0.843788</td>\n      <td>0.198007</td>\n      <td>0.098120</td>\n      <td>0.580198</td>\n      <td>0.912617</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>132</td>\n      <td>0.865061</td>\n      <td>0.517303</td>\n      <td>0.009042</td>\n      <td>0.720124</td>\n      <td>0.786819</td>\n      <td>0.856836</td>\n      <td>0.558204</td>\n      <td>0.417571</td>\n      <td>0.142056</td>\n      <td>...</td>\n      <td>0.722715</td>\n      <td>0.954710</td>\n      <td>0.662942</td>\n      <td>0.975540</td>\n      <td>0.982992</td>\n      <td>0.889610</td>\n      <td>0.801359</td>\n      <td>0.123226</td>\n      <td>0.433160</td>\n      <td>0.920122</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200</td>\n      <td>0.913985</td>\n      <td>0.651699</td>\n      <td>0.094470</td>\n      <td>0.706027</td>\n      <td>0.683450</td>\n      <td>0.656764</td>\n      <td>0.645682</td>\n      <td>0.581313</td>\n      <td>0.318228</td>\n      <td>...</td>\n      <td>0.822887</td>\n      <td>0.908777</td>\n      <td>0.664382</td>\n      <td>0.946114</td>\n      <td>0.959311</td>\n      <td>0.833658</td>\n      <td>0.285771</td>\n      <td>0.123592</td>\n      <td>0.551567</td>\n      <td>0.911655</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>471</th>\n      <td>9569</td>\n      <td>0.888979</td>\n      <td>0.574700</td>\n      <td>0.023743</td>\n      <td>0.695837</td>\n      <td>0.744091</td>\n      <td>0.768347</td>\n      <td>0.587762</td>\n      <td>0.482918</td>\n      <td>0.203907</td>\n      <td>...</td>\n      <td>0.766584</td>\n      <td>0.939587</td>\n      <td>0.659721</td>\n      <td>0.965172</td>\n      <td>0.975156</td>\n      <td>0.864806</td>\n      <td>0.660653</td>\n      <td>0.140083</td>\n      <td>0.462654</td>\n      <td>0.913447</td>\n    </tr>\n    <tr>\n      <th>472</th>\n      <td>9590</td>\n      <td>0.862380</td>\n      <td>0.514621</td>\n      <td>0.006702</td>\n      <td>0.729544</td>\n      <td>0.802338</td>\n      <td>0.877330</td>\n      <td>0.548557</td>\n      <td>0.396558</td>\n      <td>0.123524</td>\n      <td>...</td>\n      <td>0.716780</td>\n      <td>0.958904</td>\n      <td>0.663152</td>\n      <td>0.978092</td>\n      <td>0.984923</td>\n      <td>0.898048</td>\n      <td>0.837145</td>\n      <td>0.117742</td>\n      <td>0.412991</td>\n      <td>0.920057</td>\n    </tr>\n    <tr>\n      <th>473</th>\n      <td>9597</td>\n      <td>0.901031</td>\n      <td>0.598261</td>\n      <td>0.063027</td>\n      <td>0.709439</td>\n      <td>0.679836</td>\n      <td>0.674037</td>\n      <td>0.623549</td>\n      <td>0.555399</td>\n      <td>0.300314</td>\n      <td>...</td>\n      <td>0.789879</td>\n      <td>0.920005</td>\n      <td>0.666057</td>\n      <td>0.953035</td>\n      <td>0.964848</td>\n      <td>0.834566</td>\n      <td>0.387828</td>\n      <td>0.134454</td>\n      <td>0.530733</td>\n      <td>0.912921</td>\n    </tr>\n    <tr>\n      <th>474</th>\n      <td>9623</td>\n      <td>0.920093</td>\n      <td>0.732291</td>\n      <td>0.104342</td>\n      <td>0.728492</td>\n      <td>0.724560</td>\n      <td>0.729168</td>\n      <td>0.663263</td>\n      <td>0.582019</td>\n      <td>0.274256</td>\n      <td>...</td>\n      <td>0.878721</td>\n      <td>0.908156</td>\n      <td>0.665253</td>\n      <td>0.947283</td>\n      <td>0.958930</td>\n      <td>0.850710</td>\n      <td>0.237153</td>\n      <td>0.097485</td>\n      <td>0.566745</td>\n      <td>0.913482</td>\n    </tr>\n    <tr>\n      <th>475</th>\n      <td>9640</td>\n      <td>0.886796</td>\n      <td>0.574523</td>\n      <td>0.023442</td>\n      <td>0.697757</td>\n      <td>0.733977</td>\n      <td>0.760781</td>\n      <td>0.587043</td>\n      <td>0.484391</td>\n      <td>0.205417</td>\n      <td>...</td>\n      <td>0.763548</td>\n      <td>0.938247</td>\n      <td>0.658836</td>\n      <td>0.965292</td>\n      <td>0.974650</td>\n      <td>0.859920</td>\n      <td>0.662730</td>\n      <td>0.144258</td>\n      <td>0.463926</td>\n      <td>0.912668</td>\n    </tr>\n  </tbody>\n</table>\n<p>476 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}