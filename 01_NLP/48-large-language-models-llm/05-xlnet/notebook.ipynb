{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-23T22:39:19.118148Z","iopub.execute_input":"2023-05-23T22:39:19.118893Z","iopub.status.idle":"2023-05-23T22:39:19.135509Z","shell.execute_reply.started":"2023-05-23T22:39:19.118857Z","shell.execute_reply":"2023-05-23T22:39:19.134650Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n/kaggle/input/quora-insincere-questions-classification/train.csv\n/kaggle/input/quora-insincere-questions-classification/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Architectural Overview\n\n> XLNet(eXtreme Lite Transformer)\n\n- It is a state of the art llm developed by CMU and Google AI. It is based on transformer architecture and was designed to overcome the limitations of prev language models such as BERT and GPT2.\n- The architecture of XLNet is similar to BERT with some key diff. The main diff being XLNet uses a permutation based training approach which allows it to model dependencies between all tokens in a sequence rather than just the tokens that come before the current token.\n\n>> The block diagram of XLNet consists of three main components.\n\n- Input embedding layer - This layer takes the input text and converts it into a vector representation that can be processed by the model. The input embedding layer uses a pretrained word embedding model to convert each word in the input text into a vector.\n- Transformer encoder layers - The transformer encoder layers are the core buiding blocks of the model. They use self attn mechanism to process the input text and generate a contextualised representation of each word in the text. The transformer encoder layers are stacked on top of each other to create a deep neural network.\n- Permutation based training - XLNet uses a permutation based training approach which allows it to model dependencies between all tokens in a sequence rather than just the tokens that come before the current token. This is achieved by randomly permuting the input seq during training and using a modified loss function that takes into account all possible permutations of the input sequence\n\n>> The advantages of XLNet are\n\n1. Improved modeling of dependencies. - It is able to model dependencies between all tokens in a sequence rather than just the tokens that come before it. It yields better and coherent text.\n2. It performs better on Question-answering, sentiment analysis and text classification in comparision to bert and GPT2.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer\nfrom sklearn.metrics import f1_score\nimport pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:02:01.350765Z","iopub.execute_input":"2023-05-23T23:02:01.351166Z","iopub.status.idle":"2023-05-23T23:02:13.807121Z","shell.execute_reply.started":"2023-05-23T23:02:01.351133Z","shell.execute_reply":"2023-05-23T23:02:13.806106Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We import the required libraries including PyTorch, XLNetForSequenceClassification, XLNetTokenizer from the transformers package, and pandas for data handling.\n\n","metadata":{}},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:02:13.809220Z","iopub.execute_input":"2023-05-23T23:02:13.809557Z","iopub.status.idle":"2023-05-23T23:02:13.839192Z","shell.execute_reply.started":"2023-05-23T23:02:13.809526Z","shell.execute_reply":"2023-05-23T23:02:13.838217Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"We check if a GPU is available and set the device accordingly. This enables GPU acceleration if available.","metadata":{}},{"cell_type":"code","source":"# Define XLNet model and tokenizer\nmodel = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:02:13.840500Z","iopub.execute_input":"2023-05-23T23:02:13.840922Z","iopub.status.idle":"2023-05-23T23:02:18.079293Z","shell.execute_reply.started":"2023-05-23T23:02:13.840889Z","shell.execute_reply":"2023-05-23T23:02:18.078343Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3cefa7340a1444bac4d775c3041e899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27dbb6644de74259b3df51f30f047958"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d4ded3d8d514e11aa5c9ef2e75e0704"}},"metadata":{}}]},{"cell_type":"markdown","source":"We initialize the XLNet model for sequence classification and its corresponding tokenizer. We use the 'xlnet-base-cased' pre-trained model.","metadata":{}},{"cell_type":"code","source":"# Load Quora Insincere Question Classification dataset\ntrain_df = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/train.csv')\ntest_df = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:02:18.082009Z","iopub.execute_input":"2023-05-23T23:02:18.082363Z","iopub.status.idle":"2023-05-23T23:02:23.203083Z","shell.execute_reply.started":"2023-05-23T23:02:18.082330Z","shell.execute_reply":"2023-05-23T23:02:23.202162Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We read the Quora Insincere Question Classification dataset from CSV files using pandas.","metadata":{}},{"cell_type":"code","source":"# Tokenize and encode sequences\ntrain_encoded = tokenizer.batch_encode_plus(train_df['question_text'].tolist(),\n                                            add_special_tokens=True,\n                                            padding='longest',\n                                            truncation=True,\n                                            return_tensors='pt', max_length=64) #increase max length to 512 if there are no memory restrictions","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:02:23.204486Z","iopub.execute_input":"2023-05-23T23:02:23.204846Z","iopub.status.idle":"2023-05-23T23:07:47.632623Z","shell.execute_reply.started":"2023-05-23T23:02:23.204815Z","shell.execute_reply":"2023-05-23T23:07:47.631006Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We tokenize and encode the question texts using the XLNet tokenizer. The encoded sequences include input IDs, attention masks, and the target labels for the training dataset.","metadata":{}},{"cell_type":"code","source":"test_encoded = tokenizer.batch_encode_plus(test_df['question_text'].tolist(),\n                                           add_special_tokens=True,\n                                           padding='longest',\n                                           truncation=True,\n                                           return_tensors='pt',max_length=64)#increase max length to 512 if there are no memory restrictions\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:07:47.635697Z","iopub.execute_input":"2023-05-23T23:07:47.636008Z","iopub.status.idle":"2023-05-23T23:09:20.174427Z","shell.execute_reply.started":"2023-05-23T23:07:47.635982Z","shell.execute_reply":"2023-05-23T23:09:20.173469Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Prepare data loaders\ntrain_dataset = torch.utils.data.TensorDataset(train_encoded['input_ids'],\n                                               train_encoded['attention_mask'],\n                                               torch.tensor(train_df['target'].tolist()))\ntest_dataset = torch.utils.data.TensorDataset(test_encoded['input_ids'], test_encoded['attention_mask'])\ntrain_loader = DataLoader(train_dataset, batch_size=16*4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16*4, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:09:20.175950Z","iopub.execute_input":"2023-05-23T23:09:20.176305Z","iopub.status.idle":"2023-05-23T23:09:20.580541Z","shell.execute_reply.started":"2023-05-23T23:09:20.176274Z","shell.execute_reply":"2023-05-23T23:09:20.579603Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We create data loaders for both the training and test datasets using the encoded sequences. ","metadata":{}},{"cell_type":"code","source":"for batch in train_loader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:09:20.581736Z","iopub.execute_input":"2023-05-23T23:09:20.582074Z","iopub.status.idle":"2023-05-23T23:09:20.708134Z","shell.execute_reply.started":"2023-05-23T23:09:20.582043Z","shell.execute_reply":"2023-05-23T23:09:20.707186Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[tensor([[ 5,  5,  5,  ..., 82,  4,  3],\n        [ 5,  5,  5,  ..., 82,  4,  3],\n        [ 5,  5,  5,  ..., 82,  4,  3],\n        ...,\n        [ 5,  5,  5,  ..., 82,  4,  3],\n        [ 5,  5,  5,  ..., 82,  4,  3],\n        [ 5,  5,  5,  ..., 82,  4,  3]]), tensor([[0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        ...,\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1]]), tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Performed sanity check of the dataloaders","metadata":{}},{"cell_type":"code","source":"# Set hyperparameters\nnum_epochs = 1\nlearning_rate = 2e-5\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:09:20.709661Z","iopub.execute_input":"2023-05-23T23:09:20.710024Z","iopub.status.idle":"2023-05-23T23:09:20.714586Z","shell.execute_reply.started":"2023-05-23T23:09:20.709993Z","shell.execute_reply":"2023-05-23T23:09:20.713499Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We define the number of training epochs and the learning rate.","metadata":{}},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:09:20.718826Z","iopub.execute_input":"2023-05-23T23:09:20.719357Z","iopub.status.idle":"2023-05-23T23:09:20.726114Z","shell.execute_reply.started":"2023-05-23T23:09:20.719323Z","shell.execute_reply":"2023-05-23T23:09:20.725222Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"We specify the loss function (cross-entropy loss) and the optimizer (AdamW) for model training.","metadata":{}},{"cell_type":"code","source":"# Training loop\nmodel.to(device)\nmodel.train()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:09:20.727535Z","iopub.execute_input":"2023-05-23T23:09:20.727872Z","iopub.status.idle":"2023-05-23T23:09:25.451598Z","shell.execute_reply.started":"2023-05-23T23:09:20.727843Z","shell.execute_reply":"2023-05-23T23:09:25.450630Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"XLNetForSequenceClassification(\n  (transformer): XLNetModel(\n    (word_embedding): Embedding(32000, 768)\n    (layer): ModuleList(\n      (0-11): 12 x XLNetLayer(\n        (rel_attn): XLNetRelativeAttention(\n          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ff): XLNetFeedForward(\n          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation_function): GELUActivation()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (sequence_summary): SequenceSummary(\n    (summary): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n    (first_dropout): Identity()\n    (last_dropout): Dropout(p=0.1, inplace=False)\n  )\n  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for step, (inputs, masks, targets) in enumerate(tqdm(train_loader)):\n        inputs, masks, targets = inputs.to(device), masks.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(input_ids=inputs, attention_mask=masks)[0]\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        if step%1000==0:\n            print(\"Step - {}, Loss - {}\".format(step, loss.item()))\n            break\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:09:25.453090Z","iopub.execute_input":"2023-05-23T23:09:25.453426Z","iopub.status.idle":"2023-05-23T23:09:27.613771Z","shell.execute_reply.started":"2023-05-23T23:09:25.453393Z","shell.execute_reply":"2023-05-23T23:09:27.612834Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"  0%|          | 0/20409 [00:02<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Step - 0, Loss - 0.7137416005134583\nEpoch 1/1, Loss: 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We train the XLNet model by iterating over the training data loader. The model is set to train mode, and the optimizer is zeroed before computing and backpropagating the loss.","metadata":{}},{"cell_type":"markdown","source":"Train more, It's just a proof of concept !","metadata":{}},{"cell_type":"code","source":"# Evaluation\nmodel.eval()\npredictions = []\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:09:27.615409Z","iopub.execute_input":"2023-05-23T23:09:27.616112Z","iopub.status.idle":"2023-05-23T23:09:27.621881Z","shell.execute_reply.started":"2023-05-23T23:09:27.616077Z","shell.execute_reply":"2023-05-23T23:09:27.620856Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We switch the model to evaluation mode and iterate over the test data loader to make predictions on the test dataset.","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    for inputs, masks in tqdm(test_loader):\n        inputs, masks = inputs.to(device), masks.to(device)\n        outputs = model(input_ids=inputs, attention_mask=masks)[0]\n        _, predicted_labels = torch.max(outputs, 1)\n        predictions.extend(predicted_labels.tolist())\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T23:09:27.623353Z","iopub.execute_input":"2023-05-23T23:09:27.623696Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 30%|██▉       | 1748/5872 [04:22<10:19,  6.66it/s]","output_type":"stream"}]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'qid': test_df['qid'], 'prediction': predictions})\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"submission.csv generated","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}