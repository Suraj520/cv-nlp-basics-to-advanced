{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "> Meena\n",
    "\n",
    "It is a state of the art language model developed by OpenAI. Its architecture is based on the transformer model. The transformer model consists of several blocks which shares it resemblance wit other architectures.\n",
    "\n",
    "1. Encoder - The encoder is responsible for processing the input text and extracting meaningful representations from it. In Meena, The encoder is a stack of identical layers, with each layer consisting of two sub layers,a self attn mechanism and a feed forward neural network. The self attention mechanism allows the model to weigh the importance of different words in the input text, while the feed forward network helps capture non linear relationships between the words.\n",
    "\n",
    "2. Decoder - The decoder takes the encoded representations from the encoder and generates the output text. Like the encoder, the decoder in Meena is also a stack of identical layers. Each layer has three sub layers, a masked self attn. mechanism, an encoder decoder attn mechanism and a feed forward network. The masked self attn mechanism ensures that during training, the model doesn't have access to future tokens and can only attend to previously generated ones.\n",
    "\n",
    "3. Self-Attn - Self attn is a mechanism that allows the model to weight the importance of different words within the same input seq. It calculates attn scores for each word by compairing it with all other words in the seq. Meena uses a variant of self attn called the \"Scaled Dot Product Attention\" which scales the attention scores by the squared root of the dimension of the input.\n",
    "\n",
    "4. Multi-head attn - Like other transformers, it also employs multi head attention. It means that the model performs self attention multiple times in parallel but with different learned weights. This allows the model to attend to different aspects of the input simultaneously capturing different types of dependencies.\n",
    "\n",
    "5. Positional encoding - It uses positional encoding to provide information about the order of words in the input seq. It is important because transformers don't have the notion of word order and positional encoding helps the model understand the sequential nature of the input.\n",
    "\n",
    "6. Differences from other architectures: Meena introduced new elements to improve its performance, One key diff is the use of a large number of params which allows it to capture complex patterns and generate coherent responses. As an evolved transformer, It also benefits from more better dataset on which it's trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
