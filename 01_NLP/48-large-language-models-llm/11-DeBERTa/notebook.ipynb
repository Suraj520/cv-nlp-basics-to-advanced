{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Architecture Overview\n\n> DeBERTa\n\nIt is a powerful transformer based LLM that builds upon the original transformer arch, thereby introducing several innovative building blocks. These blocks enhance the model's ability to understand & gen. language by addressing specific limitations found in prev. transformer models.\n\nIt differs from other archs. via following blocks\n\n1. Enhanced Masked Language Modeling(MLM):\nDeBERTa improves upon the traditional MLM used in BERT. In BERT, tokens are randomly masked and the model learns to predict the original tokens. However, DeBERTa introduces a more advanced MLM called Span Boundary Objective(SBO). It masks consecutive spans of tokens, which helps the model capture dependencies across multiple tokens and better understand long range relationships.\n\n2. Intra-sentence and Inter-sentence learning:\nDeBERTa incorporates both intra-sentence and inter-sentence learning. While most transformer models focus on capturing relationships within a sentence(intra levels), DeBERTa also considers the connections between sentences(inter levels) which allows the model to understand document-level semantics and capture global context effectively.\n\n3. Relational Self-Attn:\nIt replaces the traditional self attn mechanism with relational self attn.(RSA). RSA introduces a series of learnable matrices that modulate the attention wts. between tokens. By modelling the relationships between tokens explicitly, RSA enables the model to better handle long dependencies and capture fine-grained interactions.\n\n4. Enhanced Training Techniques: DeBERTa adopts a training strat. called Contrastive Bidirectional Training(CBT). CBT combines pretraining and fine-tuning by leveraging both masked language modelling(MLM) and NSP(Next Sentence prediction) objectives simultaneously. This approach enhances the model's ability to capture context and relationships between sentences, resulting in improved performance on downstream tasks.\n\n5. Cross-layer Parameter Sharing:\nUnlike other transformer archs, DeBERTa applies param sharing across diff. layers of the model. This param sharing allows the model to efficiently learn representations at different levels. By sharing params, DeBERTa reduces the number of total params, making it a more computationally scalable and memory efficient model.\n\n\nThese improvements help the model capture long range dependencies, global context and fine grained interactions, making it a state of the art transformer based LLM.","metadata":{}},{"cell_type":"markdown","source":"The following code snippet is just a proof of concept ! For better results, Train more !","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T15:25:09.382971Z","iopub.execute_input":"2023-05-27T15:25:09.383224Z","iopub.status.idle":"2023-05-27T15:25:18.568844Z","shell.execute_reply.started":"2023-05-27T15:25:09.383200Z","shell.execute_reply":"2023-05-27T15:25:18.567755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Neccessary modules for training and inference are imported in the above snippet of the code.","metadata":{}},{"cell_type":"code","source":"# Load the data\ntrain_df = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntest_df = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T15:25:18.571016Z","iopub.execute_input":"2023-05-27T15:25:18.571789Z","iopub.status.idle":"2023-05-27T15:25:18.738136Z","shell.execute_reply.started":"2023-05-27T15:25:18.571754Z","shell.execute_reply":"2023-05-27T15:25:18.737196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train and test dataframes are read from their location on the disk.","metadata":{}},{"cell_type":"code","source":"# Split the data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_df['premise'].tolist(), train_df['label'].tolist(), test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T15:25:18.740277Z","iopub.execute_input":"2023-05-27T15:25:18.740999Z","iopub.status.idle":"2023-05-27T15:25:18.756072Z","shell.execute_reply.started":"2023-05-27T15:25:18.740964Z","shell.execute_reply":"2023-05-27T15:25:18.755274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code splits the data into training and validation sets using the `train_test_split` function. The `train_texts` and `train_labels` lists contain the training data, while the `val_texts` and `val_labels` lists contain the validation data. The data is divided in a 80:20 ratio, with 80% used for training and 20% for validation.","metadata":{}},{"cell_type":"code","source":"# Load the Deberta model and tokenizer\nmodel_name = 'microsoft/deberta-base'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFAutoModel.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T15:25:18.759683Z","iopub.execute_input":"2023-05-27T15:25:18.759939Z","iopub.status.idle":"2023-05-27T15:25:34.686735Z","shell.execute_reply.started":"2023-05-27T15:25:18.759916Z","shell.execute_reply":"2023-05-27T15:25:34.685846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code loads the Deberta model and tokenizer using the `AutoTokenizer` and `TFAutoModel` classes from the Transformers library. The `model_name` variable specifies the name or identifier of the Deberta model to be loaded, in this case, it is set to 'microsoft/deberta-base'. The `tokenizer` object is initialized with the Deberta tokenizer, which will be used to tokenize the input text. The `model` object is initialized with the Deberta model, which will be used for further processing or fine-tuning.","metadata":{}},{"cell_type":"code","source":"# Tokenize the input texts\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T15:25:34.688327Z","iopub.execute_input":"2023-05-27T15:25:34.688965Z","iopub.status.idle":"2023-05-27T15:25:36.995562Z","shell.execute_reply.started":"2023-05-27T15:25:34.688929Z","shell.execute_reply":"2023-05-27T15:25:36.994589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code tokenizes the input texts using the tokenizer object created in the previous step. The `tokenizer` is applied to the `train_texts` and `val_texts` lists, which contain the training and validation texts respectively. The `truncation=True` argument ensures that the texts are truncated to a maximum length if they exceed the maximum token limit. The `padding=True` argument adds padding tokens to make all input sequences of equal length. The resulting tokenized encodings are stored in the `train_encodings` and `val_encodings` variables respectively.","metadata":{}},{"cell_type":"code","source":"# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\nval_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T15:25:36.997109Z","iopub.execute_input":"2023-05-27T15:25:36.997472Z","iopub.status.idle":"2023-05-27T15:26:21.728733Z","shell.execute_reply.started":"2023-05-27T15:25:36.997439Z","shell.execute_reply":"2023-05-27T15:26:21.727754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code creates TensorFlow datasets from the tokenized encodings and labels. It uses the `tf.data.Dataset.from_tensor_slices()` function to create datasets from the tensors `train_encodings` and `train_labels` for the training dataset, and `val_encodings` and `val_labels` for the validation dataset. Each sample in the dataset consists of a dictionary containing the input encodings (tokenized texts) and the corresponding labels.","metadata":{}},{"cell_type":"code","source":"# Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\noutputs = model({'input_ids': input_ids, 'attention_mask': attention_mask})[0]\noutputs = tf.keras.layers.GlobalMaxPool1D()(outputs)\noutputs = tf.keras.layers.Dropout(0.2)(outputs)\noutputs = tf.keras.layers.Dense(3, activation='softmax')(outputs)\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T15:26:21.730193Z","iopub.execute_input":"2023-05-27T15:26:21.730534Z","iopub.status.idle":"2023-05-27T15:26:33.969491Z","shell.execute_reply.started":"2023-05-27T15:26:21.730503Z","shell.execute_reply":"2023-05-27T15:26:33.968569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code defines the architecture of the model using TensorFlow's Keras API. It starts by defining two input layers, `input_ids` and `attention_mask`, with the specified shapes and data types. The `input_ids` layer represents the input tokenized text, and the `attention_mask` layer represents the attention mask for the input.\n\nNext, the code passes the input layers to the pre-trained Deberta model to obtain the model outputs. The `model` variable represents the pre-trained Deberta model loaded earlier.\n\nThe code then applies a global max pooling layer (`GlobalMaxPool1D`) to pool the output tensor along the time dimension. This operation reduces the dimensionality of the tensor.\n\nA dropout layer with a dropout rate of 0.2 is applied to prevent overfitting.\n\nFinally, a dense layer with 3 units and a softmax activation function is added to produce the final output probabilities for each class. The `model` variable is updated to represent the new model architecture with the defined input and output layers.","metadata":{}},{"cell_type":"code","source":"# Compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T15:26:33.970779Z","iopub.execute_input":"2023-05-27T15:26:33.971198Z","iopub.status.idle":"2023-05-27T15:26:33.995622Z","shell.execute_reply.started":"2023-05-27T15:26:33.971166Z","shell.execute_reply":"2023-05-27T15:26:33.994780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code compiles the model by specifying the optimizer, loss function, and metrics to be used during training.\n\nThe optimizer chosen is Adam with a learning rate of 5e-5. Adam is an optimization algorithm commonly used for training neural networks.\n\nThe loss function is specified as \"sparse_categorical_crossentropy\", which is suitable for multi-class classification problems with integer labels. This loss function calculates the cross-entropy between the predicted probabilities and the true labels.\n\nThe chosen metric for evaluation during training is \"accuracy\", which measures the proportion of correctly classified samples.\n\nBy calling `model.compile()`, the model is prepared for training with the specified optimizer, loss function, and metrics.","metadata":{}},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"The code loads a pretrained checkpoint to resume training from there","metadata":{}},{"cell_type":"code","source":"# from transformers import TFDebertaModel\n\n# # Define the custom layer within the custom_object_scope\n# custom_objects = {'TFDebertaModel': TFDebertaModel}\n\n# # Load the model using the custom_object_scope\n# with tf.keras.utils.custom_object_scope(custom_objects):\n#     model = tf.keras.models.load_model('deberta-trained-model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:55:41.649765Z","iopub.execute_input":"2023-05-27T16:55:41.650112Z","iopub.status.idle":"2023-05-27T16:55:53.511562Z","shell.execute_reply.started":"2023-05-27T16:55:41.650083Z","shell.execute_reply":"2023-05-27T16:55:53.510546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading checkpoint.\n","metadata":{}},{"cell_type":"code","source":"\n# Train the model\nhistory = model.fit(train_dataset.shuffle(1000).batch(8), epochs=10, batch_size=16, validation_data=val_dataset.shuffle(1000).batch(16))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T17:00:09.840257Z","iopub.execute_input":"2023-05-27T17:00:09.840951Z","iopub.status.idle":"2023-05-27T17:03:18.268727Z","shell.execute_reply.started":"2023-05-27T17:00:09.840918Z","shell.execute_reply":"2023-05-27T17:03:18.267215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code trains the model using the `fit()` function in TensorFlow. The args are \n\n- `train_dataset.shuffle(1000).batch(8)`: The training dataset is shuffled with a buffer size of 1000 and batched into batches of size 8. The `shuffle()` function shuffles the examples in the dataset, ensuring randomness during training.\n\n- `epochs=3`: The number of times the entire training dataset will be iterated over during training.\n\n- `batch_size=16`: The number of examples in each training batch.\n\n- `validation_data=val_dataset.shuffle(1000).batch(16)`: The validation dataset is shuffled with a buffer size of 1000 and batched into batches of size 16. The model's performance on this dataset will be evaluated at the end of each training epoch.\n","metadata":{}},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# Save the trained model\nmodel.save('deberta-trained-model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:41:59.426806Z","iopub.execute_input":"2023-05-27T16:41:59.427256Z","iopub.status.idle":"2023-05-27T16:42:04.176362Z","shell.execute_reply.started":"2023-05-27T16:41:59.427197Z","shell.execute_reply":"2023-05-27T16:42:04.175367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saves the trained model for further inferencing.","metadata":{}},{"cell_type":"code","source":"# Make predictions on the test set using the saved model\n#model = tf.keras.models.load_model('deberta-trained-model.h5')\ntest_encodings = tokenizer(test_df['premise'].tolist(), truncation=True, padding=True)\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings)))\ntest_predictions = model.predict(test_dataset.batch(16)).argmax(axis=-1)\n\n# Save the predictions to a CSV file\nsubmission_df = pd.DataFrame({'id': test_df['id'], 'prediction': test_predictions})\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:42:57.431029Z","iopub.execute_input":"2023-05-27T16:42:57.431409Z","iopub.status.idle":"2023-05-27T16:45:43.099826Z","shell.execute_reply.started":"2023-05-27T16:42:57.431376Z","shell.execute_reply":"2023-05-27T16:45:43.098884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code snippet makes predictions on the test set using a saved model. The steps are\n\n\n1. `loaded_model = tf.keras.models.load_model('deberta-trained-model.h5')`: The saved model is loaded from the file \"deberta-trained-model.h5\" using `load_model()` function from TensorFlow.\n\n2. `test_encodings = tokenizer(test_df['premise'].tolist(), truncation=True, padding=True)`: The test set is tokenized using the same tokenizer that was used during training. The tokenizer is applied to the \"premise\" column of the test data, and truncation and padding are applied to ensure consistent input shapes.\n\n3. `test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings)))`: The tokenized test data is converted into a TensorFlow dataset using `from_tensor_slices()` function. The input features are passed as a dictionary.\n\n4. `test_predictions = loaded_model.predict(test_dataset.batch(16)).argmax(axis=-1)`: The loaded model is used to make predictions on the test dataset. The `predict()` function is applied to the test dataset batched into batches of size 16. The `argmax(axis=-1)` method is used to determine the predicted class index for each example.\n\n5. `submission_df = pd.DataFrame({'id': test_df['id'], 'prediction': test_predictions})`: The predicted class indices and corresponding IDs from the test dataset are combined into a pandas DataFrame.\n\n6. `submission_df.to_csv('submission.csv', index=False)`: The predictions are saved to a CSV file named \"submission.csv\" without including the index column.\n\n","metadata":{}},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:45:43.101555Z","iopub.execute_input":"2023-05-27T16:45:43.101910Z","iopub.status.idle":"2023-05-27T16:45:43.120261Z","shell.execute_reply.started":"2023-05-27T16:45:43.101877Z","shell.execute_reply":"2023-05-27T16:45:43.119344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}