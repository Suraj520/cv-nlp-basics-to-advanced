{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#6576FC; color:#19180F; font-size:40px; font-family:Verdana; padding:10px; border: 5px solid #19180F; border-radius: 10px\"> CamemBERT </div>\n<div style=\"background-color:#ADB6FB; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F; border-radius: 10px\"> Architecture Overview </div>\n<div style=\"background-color:#E3E7FF; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F; border-radius: 10px\"> CamemBERT is a French language model based on the BERT architecture (Bidirectional Encoder Representations from Transformers). It was created as part of the Transformers library by the Hugging Face team. CamemBERT was trained on a large French corpus and may be used for text categorization, named entity identification, and question answering.<br><br>\n\nCamemBERT's architecture is made up of several main components:<br><br>\n\nInput Embeddings: CamemBERT receives a series of tokens as input. To begin, each token is turned into a vector representation known as an embedding. Each token in the sequence's contextual information is captured by the embeddings.<br><br>\n\nCamemBERT makes use of a stack of Transformer encoder layers. Each encoder layer includes a multi-head self-attention mechanism as well as a position-wise feed-forward neural network. The self-attention mechanism allows each token to attend to other tokens in the sequence, capturing the dependencies between them.<br><br>\n\nPre-trained Weights: CamemBERT is pre-trained with weights learnt through a language modelling purpose. These weights extract knowledge from a huge French corpus and then apply it to particular downstream tasks.<br><br>\n\nMasked Language Model (MLM): Some tokens in the input sequence are masked during pre-training, and the model is trained to predict the original tokens given the masked tokens. This assignment assists CamemBERT in learning contextual representations that capture the meaning of words in various settings.\n<br><br>\nNext Sentence Prediction (NSP): Predicting whether two sentences appear consecutively in the original corpus is another pre-training assignment. CamemBERT learns the links between sentences and develops its knowledge of sentence-level semantics by completing this assignment.\n<br><br></div>","metadata":{}},{"cell_type":"code","source":"from IPython.display import SVG, display\n\n# Load the SVG file and display it\nsvg_file = '/kaggle/input/notebook-images/camembert.svg'\ndisplay(SVG(filename=svg_file))","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:12.389976Z","iopub.execute_input":"2023-06-19T08:38:12.390429Z","iopub.status.idle":"2023-06-19T08:38:12.416366Z","shell.execute_reply.started":"2023-06-19T08:38:12.390391Z","shell.execute_reply":"2023-06-19T08:38:12.414276Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"930pt\" height=\"186pt\" viewBox=\"0.00 0.00 930.39 186.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 182)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-182 926.386,-182 926.386,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_InputEmbeddings</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-50 8,-127 143.8156,-127 143.8156,-50 8,-50\"/>\n<text text-anchor=\"middle\" x=\"75.9078\" y=\"-110.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Embeddings</text>\n</g>\n<g id=\"clust2\" class=\"cluster\">\n<title>cluster_Transformer</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"163.8156,-50 163.8156,-127 528.5254,-127 528.5254,-50 163.8156,-50\"/>\n<text text-anchor=\"middle\" x=\"346.1705\" y=\"-110.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Transformer Encoder</text>\n</g>\n<g id=\"clust3\" class=\"cluster\">\n<title>cluster_Weights</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"548.5254,-50 548.5254,-127 676.5374,-127 676.5374,-50 548.5254,-50\"/>\n<text text-anchor=\"middle\" x=\"612.5314\" y=\"-110.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Pre-trained Weights</text>\n</g>\n<g id=\"clust4\" class=\"cluster\">\n<title>cluster_MLM</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"702.8442,-93 702.8442,-170 908.0792,-170 908.0792,-93 702.8442,-93\"/>\n<text text-anchor=\"middle\" x=\"805.4617\" y=\"-153.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Masked Language Model (MLM)</text>\n</g>\n<g id=\"clust5\" class=\"cluster\">\n<title>cluster_NSP</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"696.5374,-8 696.5374,-85 914.386,-85 914.386,-8 696.5374,-8\"/>\n<text text-anchor=\"middle\" x=\"805.4617\" y=\"-68.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Next Sentence Prediction (NSP)</text>\n</g>\n<!-- Embedding -->\n<g id=\"node1\" class=\"node\">\n<title>Embedding</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"135.7235,-94 16.0921,-94 16.0921,-58 135.7235,-58 135.7235,-94\"/>\n<text text-anchor=\"middle\" x=\"75.9078\" y=\"-71.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Token Embedding</text>\n</g>\n<!-- Attention -->\n<g id=\"node2\" class=\"node\">\n<title>Attention</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"309.3605,-94 171.9671,-94 171.9671,-58 309.3605,-58 309.3605,-94\"/>\n<text text-anchor=\"middle\" x=\"240.6638\" y=\"-71.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Multi-Head Attention</text>\n</g>\n<!-- Embedding&#45;&gt;Attention -->\n<g id=\"edge1\" class=\"edge\">\n<title>Embedding-&gt;Attention</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M135.8546,-76C144.2531,-76 152.9884,-76 161.6734,-76\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.8225,-79.5001 171.8225,-76 161.8225,-72.5001 161.8225,-79.5001\"/>\n</g>\n<!-- FeedForward -->\n<g id=\"node3\" class=\"node\">\n<title>FeedForward</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"520.5321,-94 345.5053,-94 345.5053,-58 520.5321,-58 520.5321,-94\"/>\n<text text-anchor=\"middle\" x=\"433.0187\" y=\"-71.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Position-wise Feed-Forward</text>\n</g>\n<!-- Attention&#45;&gt;FeedForward -->\n<g id=\"edge2\" class=\"edge\">\n<title>Attention-&gt;FeedForward</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M309.565,-76C317.9331,-76 326.6015,-76 335.2868,-76\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"335.4734,-79.5001 345.4733,-76 335.4733,-72.5001 335.4734,-79.5001\"/>\n</g>\n<!-- Weights -->\n<g id=\"node4\" class=\"node\">\n<title>Weights</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"668.5434,-94 556.5194,-94 556.5194,-58 668.5434,-58 668.5434,-94\"/>\n<text text-anchor=\"middle\" x=\"612.5314\" y=\"-71.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Learned Weights</text>\n</g>\n<!-- FeedForward&#45;&gt;Weights -->\n<g id=\"edge3\" class=\"edge\">\n<title>FeedForward-&gt;Weights</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M520.6716,-76C529.2768,-76 537.9078,-76 546.248,-76\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"546.2558,-79.5001 556.2557,-76 546.2557,-72.5001 546.2558,-79.5001\"/>\n</g>\n<!-- MLM -->\n<g id=\"node5\" class=\"node\">\n<title>MLM</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"896.1969,-137 714.7265,-137 714.7265,-101 896.1969,-101 896.1969,-137\"/>\n<text text-anchor=\"middle\" x=\"805.4617\" y=\"-114.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Prediction of Masked Tokens</text>\n</g>\n<!-- Weights&#45;&gt;MLM -->\n<g id=\"edge4\" class=\"edge\">\n<title>Weights-&gt;MLM</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M668.594,-88.4951C682.9226,-91.6887 698.7027,-95.2057 714.2976,-98.6815\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"713.9798,-102.1965 724.5017,-100.9558 715.5026,-95.3641 713.9798,-102.1965\"/>\n</g>\n<!-- NSP -->\n<g id=\"node6\" class=\"node\">\n<title>NSP</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"906.3104,-52 704.613,-52 704.613,-16 906.3104,-16 906.3104,-52\"/>\n<text text-anchor=\"middle\" x=\"805.4617\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Sentence Relationship Prediction</text>\n</g>\n<!-- Weights&#45;&gt;NSP -->\n<g id=\"edge5\" class=\"edge\">\n<title>Weights-&gt;NSP</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M668.594,-63.7954C682.4663,-60.7755 697.6991,-57.4594 712.8071,-54.1705\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"713.675,-57.5636 722.7017,-52.0165 712.186,-50.7238 713.675,-57.5636\"/>\n</g>\n</g>\n</svg>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\n Importing modules\n    </div>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CamembertTokenizer, CamembertForQuestionAnswering, AdamW","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:12.418086Z","iopub.execute_input":"2023-06-19T08:38:12.418469Z","iopub.status.idle":"2023-06-19T08:38:17.641255Z","shell.execute_reply.started":"2023-06-19T08:38:12.418443Z","shell.execute_reply":"2023-06-19T08:38:17.639568Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nDefining dataset class    </div>","metadata":{}},{"cell_type":"code","source":"# Define the dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer):\n        self.contexts = dataframe['context'].tolist()\n        self.questions = dataframe['question'].tolist()\n        self.answers = dataframe['answers']\n\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.contexts)\n\n    def __getitem__(self, index):\n        context = self.contexts[index]\n        question = self.questions[index]\n        answer = self.answers[index]\n\n        encoding = self.tokenizer.encode_plus(\n            question,\n            context,\n            return_tensors='pt',\n            max_length=512,  # Adjust max_length as per your requirements\n            padding='max_length',\n            truncation=True\n        )\n\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze()\n\n        # Handle string answer\n        if isinstance(answer, str):\n            answer_start = context.find(answer)\n            start_positions = torch.tensor(answer_start, dtype=torch.long)\n            end_positions = torch.tensor(answer_start + len(answer), dtype=torch.long)\n        else:\n            # Handle dictionary answer (if applicable)\n            answer_start = answer['answer_start'][0]  # Convert answer_start to scalar integer\n            start_positions = torch.tensor(answer_start, dtype=torch.long)\n            end_positions = torch.tensor(answer_start + len(answer['text'][0]), dtype=torch.long)\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'start_positions': start_positions,\n            'end_positions': end_positions\n        }","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:17.642628Z","iopub.execute_input":"2023-06-19T08:38:17.642934Z","iopub.status.idle":"2023-06-19T08:38:17.653452Z","shell.execute_reply.started":"2023-06-19T08:38:17.642909Z","shell.execute_reply":"2023-06-19T08:38:17.651975Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nReading the dataframe    </div>","metadata":{}},{"cell_type":"code","source":"dataframe = pd.read_csv('/kaggle/input/unlock-the-power-of-reading-comprehension-with-p/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:17.656592Z","iopub.execute_input":"2023-06-19T08:38:17.656917Z","iopub.status.idle":"2023-06-19T08:38:17.716132Z","shell.execute_reply.started":"2023-06-19T08:38:17.656891Z","shell.execute_reply":"2023-06-19T08:38:17.714366Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nInitializing the tokenizer and model    </div>","metadata":{}},{"cell_type":"code","source":"tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\nmodel = CamembertForQuestionAnswering.from_pretrained('camembert-base')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:17.717365Z","iopub.execute_input":"2023-06-19T08:38:17.717640Z","iopub.status.idle":"2023-06-19T08:38:19.059209Z","shell.execute_reply.started":"2023-06-19T08:38:17.717617Z","shell.execute_reply":"2023-06-19T08:38:19.057465Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n- This IS expected if you are initializing CamembertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of CamembertForQuestionAnswering were not initialized from the model checkpoint at camembert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nCreating train dataset and dataloader    </div>","metadata":{}},{"cell_type":"code","source":"train_dataset = CustomDataset(dataframe, tokenizer)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:19.061420Z","iopub.execute_input":"2023-06-19T08:38:19.061876Z","iopub.status.idle":"2023-06-19T08:38:19.070694Z","shell.execute_reply.started":"2023-06-19T08:38:19.061841Z","shell.execute_reply":"2023-06-19T08:38:19.068727Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nPerforming sanity check of the dataloader    </div>","metadata":{}},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:19.072625Z","iopub.execute_input":"2023-06-19T08:38:19.072962Z","iopub.status.idle":"2023-06-19T08:38:19.102112Z","shell.execute_reply.started":"2023-06-19T08:38:19.072935Z","shell.execute_reply":"2023-06-19T08:38:19.100686Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[   5, 8965,  102,  ...,    1,    1,    1],\n        [   5,  137,  484,  ...,    1,    1,    1],\n        [   5, 1034,   11,  ...,    1,    1,    1],\n        ...,\n        [   5, 7363,  530,  ...,    1,    1,    1],\n        [   5, 6753,   13,  ...,    1,    1,    1],\n        [   5,  877,   19,  ...,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([-1, -1, -1, -1, -1, -1, -1, -1]), 'end_positions': tensor([ 79,  87,  96, 202, 193, 139, 107,  80])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nSetting device and moving model to device    </div>","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:19.103646Z","iopub.execute_input":"2023-06-19T08:38:19.103997Z","iopub.status.idle":"2023-06-19T08:38:19.118279Z","shell.execute_reply.started":"2023-06-19T08:38:19.103970Z","shell.execute_reply":"2023-06-19T08:38:19.117085Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"CamembertForQuestionAnswering(\n  (roberta): CamembertModel(\n    (embeddings): CamembertEmbeddings(\n      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): CamembertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x CamembertLayer(\n          (attention): CamembertAttention(\n            (self): CamembertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): CamembertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): CamembertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): CamembertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nDefining optimizer and learning rate scheduler    </div>","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=2e-5)\nnum_epochs = 10","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:19.120071Z","iopub.execute_input":"2023-06-19T08:38:19.120379Z","iopub.status.idle":"2023-06-19T08:38:19.129932Z","shell.execute_reply.started":"2023-06-19T08:38:19.120355Z","shell.execute_reply":"2023-06-19T08:38:19.128659Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nChecking number of steps per epoch    </div>","metadata":{}},{"cell_type":"code","source":"print(len(train_dataloader))","metadata":{"execution":{"iopub.status.busy":"2023-06-19T08:38:19.131101Z","iopub.execute_input":"2023-06-19T08:38:19.131414Z","iopub.status.idle":"2023-06-19T08:38:19.139970Z","shell.execute_reply.started":"2023-06-19T08:38:19.131391Z","shell.execute_reply":"2023-06-19T08:38:19.138481Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"480\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nTraining the model    </div>","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for step,batch in tqdm(enumerate(train_dataloader)):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            start_positions=start_positions,\n            end_positions=end_positions\n        )\n\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        loss.backward()\n        if step%500==0:\n            print(\"Step-{},Loss-{}\".format(step,loss.item()))\n            break\n        \n        optimizer.step()\n\n    average_loss = total_loss / len(train_dataloader)\n    print(f'Epoch {epoch+1}/{num_epochs} - Loss: {average_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-06-19T09:13:17.687837Z","iopub.execute_input":"2023-06-19T09:13:17.688283Z","iopub.status.idle":"2023-06-19T09:16:11.477810Z","shell.execute_reply.started":"2023-06-19T09:13:17.688232Z","shell.execute_reply":"2023-06-19T09:16:11.476847Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"0it [00:18, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.641871929168701\nEpoch 1/10 - Loss: 0.0097\n","output_type":"stream"},{"name":"stderr","text":"0it [00:16, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.59449577331543\nEpoch 2/10 - Loss: 0.0096\n","output_type":"stream"},{"name":"stderr","text":"0it [00:16, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.617000579833984\nEpoch 3/10 - Loss: 0.0096\n","output_type":"stream"},{"name":"stderr","text":"0it [00:17, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.635828018188477\nEpoch 4/10 - Loss: 0.0097\n","output_type":"stream"},{"name":"stderr","text":"0it [00:17, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.58721923828125\nEpoch 5/10 - Loss: 0.0096\n","output_type":"stream"},{"name":"stderr","text":"0it [00:17, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.5945892333984375\nEpoch 6/10 - Loss: 0.0096\n","output_type":"stream"},{"name":"stderr","text":"0it [00:17, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.846480369567871\nEpoch 7/10 - Loss: 0.0101\n","output_type":"stream"},{"name":"stderr","text":"0it [00:17, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.552581787109375\nEpoch 8/10 - Loss: 0.0095\n","output_type":"stream"},{"name":"stderr","text":"0it [00:17, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.567978858947754\nEpoch 9/10 - Loss: 0.0095\n","output_type":"stream"},{"name":"stderr","text":"0it [00:18, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Step-0,Loss-4.566254138946533\nEpoch 10/10 - Loss: 0.0095\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nDefining the function for evaluation    </div>","metadata":{}},{"cell_type":"code","source":"def french_question_answering(question, context):\n    # Load the tokenizer and model\n    tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n    model = CamembertForQuestionAnswering.from_pretrained('camembert-base')\n\n    # Tokenize the question and context\n    encoding = tokenizer.encode_plus(\n        question,\n        context,\n        return_tensors='pt',\n        max_length=512,\n        padding='max_length',\n        truncation=True\n    )\n\n    # Perform inference\n    input_ids = encoding['input_ids']\n    attention_mask = encoding['attention_mask']\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n    # Get the predicted start and end positions\n    start_logits = outputs.start_logits\n    end_logits = outputs.end_logits\n\n    # Find the start and end positions with the maximum logits\n    start_index = torch.argmax(start_logits)\n    end_index = torch.argmax(end_logits)\n\n    # Convert the token IDs to tokens\n    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n    answer = tokenizer.convert_tokens_to_string(tokens[start_index:end_index+1])\n\n    return answer","metadata":{"execution":{"iopub.status.busy":"2023-06-19T09:16:11.479747Z","iopub.execute_input":"2023-06-19T09:16:11.481710Z","iopub.status.idle":"2023-06-19T09:16:11.490530Z","shell.execute_reply.started":"2023-06-19T09:16:11.481670Z","shell.execute_reply":"2023-06-19T09:16:11.488908Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nEvaluating the model    </div>","metadata":{}},{"cell_type":"code","source":"question = \"Qu'est-ce que le Camembert ?\"\ncontext = \"Le Camembert est un fromage franÃ§ais Ã  pÃ¢te molle et Ã  croÃ»te fleurie.\"\n\nanswer = french_question_answering(question, context)\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T09:16:11.492538Z","iopub.execute_input":"2023-06-19T09:16:11.492871Z","iopub.status.idle":"2023-06-19T09:16:13.495898Z","shell.execute_reply.started":"2023-06-19T09:16:11.492849Z","shell.execute_reply":"2023-06-19T09:16:13.495120Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n- This IS expected if you are initializing CamembertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of CamembertForQuestionAnswering were not initialized from the model checkpoint at camembert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"</s>Le Camembert est\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}