{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## About\n",
        "\n",
        "> GPT3(Generative Pre-trained Transformer 3)\n",
        "\n",
        "- It is the state of the art language model developed by OpenAI. It belongs to the transformer architecture hierarchy.\n",
        "- The basic building block of GPT3 architecture consists of an input embedding layer, transformer encoder layers, position encoding layer, decoder layer and output embedding layer.\n",
        "- The input embedding layer takes in the input text and converts it into a vector representation that can be processed by the model. The input embedding layer uses a pretrained word embedding model to convert each word in the input text into a vector.\n",
        "- Transformer encoder layers are the main part of the GPT3 model. They use self attentn mechanisms to process the input text and generate a contextualised representation of each word in the text. The transformer encoder layers are stacked over each other to create a DNN.\n",
        "-Positional encoding layer adds positional information to the input text. This is important as the transformer encoder layers don't have the understanding of word order.\n",
        "- Decoder layer - The decoder takes the output of the transformer encoder layers and generate the final output text. The decoder layer uses softmax fun. to generate a prob distribution over the vocab of the language model.\n",
        "- The output embedding layer converts the output text into a vector representation that can be compared to the input text.\n",
        "\n",
        "> It's difference compared to GPT2 \n",
        "\n",
        "- Size - GPT3 is much much larger than GPT2 with 175 b params compared to 1.5 b parameters of GPT2.\n",
        "- Training data - The corpus over which GPT3 is trained is much much larger than GPT2.\n",
        "- GPT3 is better at few shot learning than GPT2. It is basically the ability of a LLM to learn from small amount of data.\n",
        "- GPT3 can perform zero shot learning i.e perform tasks that it has never seen before.\n",
        "- Accuracy of GPT3 >> GPT2\n",
        "-It is the 3rd-generation language prediction model in the GPT-n series and has a decoder-only transformer network with a 2048-token-long context and 175 billion parameters, requiring 800GB to store.\n",
        "\n",
        "For more details, Refer https://dugas.ch/artificial_curiosity/GPT_architecture.html"
      ],
      "metadata": {
        "id": "KzYsrcsiHikl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLZIX4ipMXLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}