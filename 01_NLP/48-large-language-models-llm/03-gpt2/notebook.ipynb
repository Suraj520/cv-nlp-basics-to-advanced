{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#035FCA; color:#19180F; font-size:40px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> GPT2(Generative Pre-trained Transformer 2) </div>\n<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> Architectural Overview.\n </div>\n<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"><br>\n\nIt is a language model that utilizes a transformer based architecture and comprises of several key components like Input Embeddings, Encoder layers, Decoder layers and Output Layers.<br>\n1. Input Embedding : In this the input text is converted to numerical representations that can be understood by the model. The embedding layer is being deployed for this task which maps each word or token in the input seq to a high dim vector.<br>\n2. Encoder layer - GPT2 consists of multiple identical encoder layers stacked over each other. Each encoder layer has two sub layers which are a self attention mechanism and feed forwd network. The self attention mechanism allows the model to weigh the importance of diff words or tokens with inp. seq thereby capturing the dependencies and relationships betw. them. The feed forward network processes the self attn outputs to gen more complex representations.<br>\n3. Decoder layer - It follows the encoder layers and has a similar structure as it also consists of self attention and feed forward layers. Just that in this the decoder layer is conditioned on the context from the prev. tokens enabling autoregressive generation. This means the model predicts the next word in the seq based on the context it has learned so far.<br>\n4. Output layer - The final layer of GPT2 is a linear transformation followed by a softmax activation function. This layer produces the prob. distribution over the vocab for the next word in the sequence. It alows the model to generate text by sampling from the distribution or choosing the word with the highest probability.<br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> Architecture Diagram.\n </div>","metadata":{}},{"cell_type":"code","source":"from IPython.display import SVG, display\n\n# Load the SVG file and display it\nsvg_file = '/kaggle/input/notebook-images/gpt2.svg'\ndisplay(SVG(filename=svg_file))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:26:20.174463Z","iopub.execute_input":"2023-06-08T23:26:20.174853Z","iopub.status.idle":"2023-06-08T23:26:20.198703Z","shell.execute_reply.started":"2023-06-08T23:26:20.174821Z","shell.execute_reply":"2023-06-08T23:26:20.197711Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1285pt\" height=\"983pt\" viewBox=\"0.00 0.00 1285.00 983.40\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 979.4)\">\n<title>gpt2</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-979.4 1281,-979.4 1281,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_input</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"22,-857.8 22,-934.6 140,-934.6 140,-857.8 22,-857.8\"/>\n<text text-anchor=\"middle\" x=\"81\" y=\"-918\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Layer</text>\n</g>\n<g id=\"clust2\" class=\"cluster\">\n<title>cluster_encoder</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"204,-214.8 204,-967.4 652,-967.4 652,-214.8 204,-214.8\"/>\n<text text-anchor=\"middle\" x=\"428\" y=\"-950.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Transformer Encoder</text>\n</g>\n<g id=\"clust3\" class=\"cluster\">\n<title>cluster_embeddings</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"457,-857.8 457,-934.6 576,-934.6 576,-857.8 457,-857.8\"/>\n<text text-anchor=\"middle\" x=\"516.5\" y=\"-918\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Embeddings</text>\n</g>\n<g id=\"clust4\" class=\"cluster\">\n<title>cluster_positional_encoding</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"303,-857.8 303,-934.6 449,-934.6 449,-857.8 303,-857.8\"/>\n<text text-anchor=\"middle\" x=\"376\" y=\"-918\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Positional Encoding</text>\n</g>\n<g id=\"clust5\" class=\"cluster\">\n<title>cluster_encoder_layers</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"238,-433.2 238,-510 621,-510 621,-433.2 238,-433.2\"/>\n<text text-anchor=\"middle\" x=\"429.5\" y=\"-493.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder Layers</text>\n</g>\n<g id=\"clust6\" class=\"cluster\">\n<title>cluster_residual</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"212,-640.6 212,-734.2 644,-734.2 644,-640.6 212,-640.6\"/>\n<text text-anchor=\"middle\" x=\"428\" y=\"-717.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Residual Connections</text>\n<text text-anchor=\"middle\" x=\"428\" y=\"-700.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">and Layer Normalization</text>\n</g>\n<g id=\"clust7\" class=\"cluster\">\n<title>cluster_decoder</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"660,-8 660,-414.2 1269,-414.2 1269,-8 660,-8\"/>\n<text text-anchor=\"middle\" x=\"964.5\" y=\"-397.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Transformer Decoder</text>\n</g>\n<g id=\"clust8\" class=\"cluster\">\n<title>cluster_output_embeddings</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"668,-287.8 668,-364.6 812,-364.6 812,-287.8 668,-287.8\"/>\n<text text-anchor=\"middle\" x=\"740\" y=\"-348\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Output Embeddings</text>\n</g>\n<g id=\"clust9\" class=\"cluster\">\n<title>cluster_positional_encoding_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"820,-287.8 820,-364.6 966,-364.6 966,-287.8 820,-287.8\"/>\n<text text-anchor=\"middle\" x=\"893\" y=\"-348\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Positional Encoding</text>\n</g>\n<g id=\"clust10\" class=\"cluster\">\n<title>cluster_decoder_layers</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"865,-119 865,-195.8 1251,-195.8 1251,-119 865,-119\"/>\n<text text-anchor=\"middle\" x=\"1058\" y=\"-179.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder Layers</text>\n</g>\n<g id=\"clust11\" class=\"cluster\">\n<title>cluster_residual_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"974,-287.8 974,-381.4 1261,-381.4 1261,-287.8 974,-287.8\"/>\n<text text-anchor=\"middle\" x=\"1117.5\" y=\"-364.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Residual Connections</text>\n<text text-anchor=\"middle\" x=\"1117.5\" y=\"-348\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">and Layer Normalization</text>\n</g>\n<g id=\"clust12\" class=\"cluster\">\n<title>cluster_output</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-529 8,-605.8 154,-605.8 154,-529 8,-529\"/>\n<text text-anchor=\"middle\" x=\"81\" y=\"-589.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Output Layer</text>\n</g>\n<!-- input -->\n<g id=\"node1\" class=\"node\">\n<title>input</title>\n<text text-anchor=\"middle\" x=\"81\" y=\"-879.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Sequence</text>\n</g>\n<!-- cluster_encoder -->\n<g id=\"node20\" class=\"node\">\n<title>cluster_encoder</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"81\" cy=\"-795\" rx=\"72.3303\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"81\" y=\"-790.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cluster_encoder</text>\n</g>\n<!-- input&#45;&gt;cluster_encoder -->\n<g id=\"edge15\" class=\"edge\">\n<title>input-&gt;cluster_encoder</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M81,-865.4006C81,-853.2949 81,-837.2076 81,-823.4674\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.5001,-823.072 81,-813.072 77.5001,-823.0721 84.5001,-823.072\"/>\n<text text-anchor=\"middle\" x=\"124.3461\" y=\"-835.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Sequence</text>\n</g>\n<!-- embeddings -->\n<g id=\"node2\" class=\"node\">\n<title>embeddings</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"550.993,-901.8 465.007,-901.8 465.007,-865.8 550.993,-865.8 550.993,-901.8\"/>\n<text text-anchor=\"middle\" x=\"508\" y=\"-879.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Embeddings</text>\n</g>\n<!-- add_norm_1 -->\n<g id=\"node7\" class=\"node\">\n<title>add_norm_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"491.2976,-684.6 364.7024,-684.6 364.7024,-648.6 491.2976,-648.6 491.2976,-684.6\"/>\n<text text-anchor=\"middle\" x=\"428\" y=\"-662.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- embeddings&#45;&gt;add_norm_1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>embeddings-&gt;add_norm_1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M501.3168,-865.6552C487.3169,-827.6453 454.8335,-739.4531 438.2061,-694.3095\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"441.461,-693.0199 434.7204,-684.8459 434.8924,-695.4393 441.461,-693.0199\"/>\n</g>\n<!-- pe -->\n<g id=\"node3\" class=\"node\">\n<title>pe</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"440.6621,-901.8 311.3379,-901.8 311.3379,-865.8 440.6621,-865.8 440.6621,-901.8\"/>\n<text text-anchor=\"middle\" x=\"376\" y=\"-879.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Positional Encoding</text>\n</g>\n<!-- pe&#45;&gt;add_norm_1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>pe-&gt;add_norm_1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M380.3441,-865.6552C389.4249,-827.7252 410.4695,-739.8234 421.2978,-694.5946\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"424.7072,-695.386 423.6317,-684.8459 417.8995,-693.7561 424.7072,-695.386\"/>\n</g>\n<!-- encoder_1 -->\n<g id=\"node4\" class=\"node\">\n<title>encoder_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"482.7852,-477.2 373.2148,-477.2 373.2148,-441.2 482.7852,-441.2 482.7852,-477.2\"/>\n<text text-anchor=\"middle\" x=\"428\" y=\"-455\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder Layer 1</text>\n</g>\n<!-- add_norm_2 -->\n<g id=\"node8\" class=\"node\">\n<title>add_norm_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"346.2976,-684.6 219.7024,-684.6 219.7024,-648.6 346.2976,-648.6 346.2976,-684.6\"/>\n<text text-anchor=\"middle\" x=\"283\" y=\"-662.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- encoder_1&#45;&gt;add_norm_2 -->\n<g id=\"edge4\" class=\"edge\">\n<title>encoder_1-&gt;add_norm_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M415.3118,-477.3485C389.5936,-514.1344 331.2332,-597.6099 301.3622,-640.3357\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"298.4755,-638.3563 295.6141,-648.5574 304.2125,-642.3672 298.4755,-638.3563\"/>\n</g>\n<!-- encoder_2 -->\n<g id=\"node5\" class=\"node\">\n<title>encoder_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"355.7852,-477.2 246.2148,-477.2 246.2148,-441.2 355.7852,-441.2 355.7852,-477.2\"/>\n<text text-anchor=\"middle\" x=\"301\" y=\"-455\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder Layer 2</text>\n</g>\n<!-- add_norm_3 -->\n<g id=\"node10\" class=\"node\">\n<title>add_norm_3</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"644.2976,-258.8 517.7024,-258.8 517.7024,-222.8 644.2976,-222.8 644.2976,-258.8\"/>\n<text text-anchor=\"middle\" x=\"581\" y=\"-236.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- encoder_2&#45;&gt;add_norm_3 -->\n<g id=\"edge6\" class=\"edge\">\n<title>encoder_2-&gt;add_norm_3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M324.391,-440.955C374.6483,-401.7543 492.96,-309.4712 549.6471,-265.2552\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"552.14,-267.7496 557.8724,-258.8395 547.8348,-262.2301 552.14,-267.7496\"/>\n</g>\n<!-- encoder_n -->\n<g id=\"node6\" class=\"node\">\n<title>encoder_n</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"613.3921,-477.2 500.6079,-477.2 500.6079,-441.2 613.3921,-441.2 613.3921,-477.2\"/>\n<text text-anchor=\"middle\" x=\"557\" y=\"-455\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder Layer N</text>\n</g>\n<!-- add_norm_1&#45;&gt;encoder_1 -->\n<g id=\"edge3\" class=\"edge\">\n<title>add_norm_1-&gt;encoder_1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M428,-648.5574C428,-612.3977 428,-530.8214 428,-487.4901\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"431.5001,-487.3485 428,-477.3485 424.5001,-487.3486 431.5001,-487.3485\"/>\n</g>\n<!-- add_norm_2&#45;&gt;encoder_2 -->\n<g id=\"edge5\" class=\"edge\">\n<title>add_norm_2-&gt;encoder_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M284.5659,-648.5574C287.7042,-612.3977 294.7841,-530.8214 298.5447,-487.4901\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"302.0471,-487.6138 299.4249,-477.3485 295.0733,-487.0084 302.0471,-487.6138\"/>\n</g>\n<!-- add_norm_n -->\n<g id=\"node9\" class=\"node\">\n<title>add_norm_n</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"636.2976,-684.6 509.7024,-684.6 509.7024,-648.6 636.2976,-648.6 636.2976,-684.6\"/>\n<text text-anchor=\"middle\" x=\"573\" y=\"-662.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- add_norm_n&#45;&gt;encoder_n -->\n<g id=\"edge7\" class=\"edge\">\n<title>add_norm_n-&gt;encoder_n</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M571.6081,-648.5574C568.8185,-612.3977 562.5253,-530.8214 559.1825,-487.4901\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"562.659,-487.0497 558.4001,-477.3485 555.6797,-487.5882 562.659,-487.0497\"/>\n</g>\n<!-- decoder_1 -->\n<g id=\"node13\" class=\"node\">\n<title>decoder_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"983.0534,-163 872.9466,-163 872.9466,-127 983.0534,-127 983.0534,-163\"/>\n<text text-anchor=\"middle\" x=\"928\" y=\"-140.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder Layer 1</text>\n</g>\n<!-- add_norm_3&#45;&gt;decoder_1 -->\n<g id=\"edge10\" class=\"edge\">\n<title>add_norm_3-&gt;decoder_1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M644.6451,-223.2288C706.6758,-206.1033 800.5322,-180.1914 863.0159,-162.9408\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"864.0572,-166.2844 872.7651,-160.2493 862.1943,-159.5368 864.0572,-166.2844\"/>\n</g>\n<!-- output_embeddings -->\n<g id=\"node11\" class=\"node\">\n<title>output_embeddings</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"804.3814,-331.8 675.6186,-331.8 675.6186,-295.8 804.3814,-295.8 804.3814,-331.8\"/>\n<text text-anchor=\"middle\" x=\"740\" y=\"-309.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Output Embeddings</text>\n</g>\n<!-- output_embeddings&#45;&gt;add_norm_3 -->\n<g id=\"edge8\" class=\"edge\">\n<title>output_embeddings-&gt;add_norm_3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M700.6966,-295.7551C679.197,-285.8841 652.3747,-273.5695 629.5734,-263.101\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"630.9506,-259.8821 620.4023,-258.8904 628.0299,-266.2437 630.9506,-259.8821\"/>\n</g>\n<!-- pe_dec -->\n<g id=\"node12\" class=\"node\">\n<title>pe_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"957.6621,-331.8 828.3379,-331.8 828.3379,-295.8 957.6621,-295.8 957.6621,-331.8\"/>\n<text text-anchor=\"middle\" x=\"893\" y=\"-309.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Positional Encoding</text>\n</g>\n<!-- pe_dec&#45;&gt;add_norm_3 -->\n<g id=\"edge9\" class=\"edge\">\n<title>pe_dec-&gt;add_norm_3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M842.3362,-295.6676C833.6106,-292.8399 824.5868,-290.0982 816,-287.8 762.4545,-273.4686 700.9962,-261.3248 654.3623,-252.973\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"654.803,-249.4966 644.3454,-251.1964 653.5805,-256.3891 654.803,-249.4966\"/>\n</g>\n<!-- add_norm_4 -->\n<g id=\"node16\" class=\"node\">\n<title>add_norm_4</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1108.2976,-331.8 981.7024,-331.8 981.7024,-295.8 1108.2976,-295.8 1108.2976,-331.8\"/>\n<text text-anchor=\"middle\" x=\"1045\" y=\"-309.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- decoder_1&#45;&gt;add_norm_4 -->\n<g id=\"edge11\" class=\"edge\">\n<title>decoder_1-&gt;add_norm_4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M940.6079,-163.1898C961.2833,-193.019 1002.4901,-252.4695 1026.4854,-287.0884\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1023.9057,-289.5105 1032.479,-295.7354 1029.6588,-285.5228 1023.9057,-289.5105\"/>\n</g>\n<!-- decoder_2 -->\n<g id=\"node14\" class=\"node\">\n<title>decoder_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1111.0534,-163 1000.9466,-163 1000.9466,-127 1111.0534,-127 1111.0534,-163\"/>\n<text text-anchor=\"middle\" x=\"1056\" y=\"-140.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder Layer 2</text>\n</g>\n<!-- add_norm_5 -->\n<g id=\"node18\" class=\"node\">\n<title>add_norm_5</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1056\" cy=\"-34\" rx=\"59.6993\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1056\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">add_norm_5</text>\n</g>\n<!-- decoder_2&#45;&gt;add_norm_5 -->\n<g id=\"edge13\" class=\"edge\">\n<title>decoder_2-&gt;add_norm_5</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1056,-126.6706C1056,-109.2373 1056,-82.7482 1056,-62.5489\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1059.5001,-62.3566 1056,-52.3566 1052.5001,-62.3567 1059.5001,-62.3566\"/>\n</g>\n<!-- decoder_n -->\n<g id=\"node15\" class=\"node\">\n<title>decoder_n</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1242.6601,-163 1129.3399,-163 1129.3399,-127 1242.6601,-127 1242.6601,-163\"/>\n<text text-anchor=\"middle\" x=\"1186\" y=\"-140.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder Layer N</text>\n</g>\n<!-- add_norm_4&#45;&gt;decoder_2 -->\n<g id=\"edge12\" class=\"edge\">\n<title>add_norm_4-&gt;decoder_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1046.1772,-295.7354C1048.0909,-266.3691 1051.8907,-208.0586 1054.1625,-173.1978\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1057.6569,-173.3963 1054.8146,-163.1898 1050.6717,-172.941 1057.6569,-173.3963\"/>\n</g>\n<!-- add_norm_nplus1 -->\n<g id=\"node17\" class=\"node\">\n<title>add_norm_nplus1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1253.2976,-331.8 1126.7024,-331.8 1126.7024,-295.8 1253.2976,-295.8 1253.2976,-331.8\"/>\n<text text-anchor=\"middle\" x=\"1190\" y=\"-309.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- add_norm_nplus1&#45;&gt;decoder_n -->\n<g id=\"edge14\" class=\"edge\">\n<title>add_norm_nplus1-&gt;decoder_n</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1189.5719,-295.7354C1188.876,-266.3691 1187.4943,-208.0586 1186.6682,-173.1978\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1190.1671,-173.1041 1186.431,-163.1898 1183.169,-173.2699 1190.1671,-173.1041\"/>\n</g>\n<!-- output -->\n<g id=\"node19\" class=\"node\">\n<title>output</title>\n<text text-anchor=\"middle\" x=\"81\" y=\"-550.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Generated Sequence</text>\n</g>\n<!-- cluster_decoder -->\n<g id=\"node21\" class=\"node\">\n<title>cluster_decoder</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"81\" cy=\"-666.6\" rx=\"72.3303\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"81\" y=\"-662.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cluster_decoder</text>\n</g>\n<!-- cluster_encoder&#45;&gt;cluster_decoder -->\n<g id=\"edge16\" class=\"edge\">\n<title>cluster_encoder-&gt;cluster_decoder</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M81,-776.8757C81,-755.5589 81,-719.918 81,-694.9396\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.5001,-694.632 81,-684.6321 77.5001,-694.6321 84.5001,-694.632\"/>\n<text text-anchor=\"middle\" x=\"128.2374\" y=\"-746.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder Outputs</text>\n</g>\n<!-- cluster_decoder&#45;&gt;output -->\n<g id=\"edge17\" class=\"edge\">\n<title>cluster_decoder-&gt;output</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M81,-648.1715C81,-630.539 81,-603.6924 81,-583.3391\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.5001,-583.0855 81,-573.0856 77.5001,-583.0856 84.5001,-583.0855\"/>\n<text text-anchor=\"middle\" x=\"138.3286\" y=\"-618\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Generated Sequence</text>\n</g>\n</g>\n</svg>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">Importing modules</div>","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\nfrom datasets import load_dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:15:41.274594Z","iopub.execute_input":"2023-06-08T23:15:41.275607Z","iopub.status.idle":"2023-06-08T23:15:53.911112Z","shell.execute_reply.started":"2023-06-08T23:15:41.275566Z","shell.execute_reply":"2023-06-08T23:15:53.909988Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">This snippet imports the necessary libraries and modules for the code. We import torch for PyTorch functionality, DataLoader for creating data loaders, GPT2LMHeadModel and GPT2Tokenizer from transformers for the GPT-2 model and tokenizer, and AdamW for the optimizer. We also import load_dataset from datasets to load the Kaggle dataset.\n</div>","metadata":{}},{"cell_type":"code","source":"# Load and preprocess the dataset\ndataset = load_dataset(\"csv\", data_files=\"/kaggle/input/all-the-news/articles1.csv\")\ntext_samples = dataset[\"train\"][\"content\"]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:15:53.917414Z","iopub.execute_input":"2023-06-08T23:15:53.918245Z","iopub.status.idle":"2023-06-08T23:16:00.318760Z","shell.execute_reply.started":"2023-06-08T23:15:53.918208Z","shell.execute_reply":"2023-06-08T23:16:00.317737Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-4efec2533fba91d5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef84a88775e405d80d6370ff8196a0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"487c8984f6924949bba13e9bbfa46038"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-4efec2533fba91d5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40d564f463164d00951b9878904fbf66"}},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">This code snippet loads and preprocesses the dataset. We use the `load_dataset` function from the `datasets` library to load the dataset from the CSV file. We then extract the text samples from the training split of the dataset and store them in the `text_samples` variable.\n</div>","metadata":{}},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:16:00.320328Z","iopub.execute_input":"2023-06-08T23:16:00.321058Z","iopub.status.idle":"2023-06-08T23:16:00.331275Z","shell.execute_reply.started":"2023-06-08T23:16:00.321021Z","shell.execute_reply":"2023-06-08T23:16:00.330131Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url', 'content'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize the tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:16:00.334886Z","iopub.execute_input":"2023-06-08T23:16:00.335879Z","iopub.status.idle":"2023-06-08T23:16:06.449493Z","shell.execute_reply.started":"2023-06-08T23:16:00.335844Z","shell.execute_reply":"2023-06-08T23:16:06.448547Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"844287f7ac1d40a28db314cc45bcd3c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d62a15da1ea469298a2a513f614add7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f558ecb4d74d4326848ae7e2eb2a723c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bbc99ba55144aa48877d65e88738c5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2dd127640064ec18b3f0a1c61f58053"}},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">In this snippet, we initialize the GPT-2 tokenizer and model. We use `GPT2Tokenizer.from_pretrained` to load the GPT-2 tokenizer from the 'gpt2' pre-trained model. Similarly, we use `GPT2LMHeadModel.from_pretrained` to load the GPT-2 model. We also add the pad token as `eos_token`</div>","metadata":{}},{"cell_type":"code","source":"# Tokenize and encode the dataset\ndef tokenize_function(example):\n    return tokenizer(example[\"content\"], truncation=True, max_length=512, padding=\"max_length\")\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:16:06.451247Z","iopub.execute_input":"2023-06-08T23:16:06.451917Z","iopub.status.idle":"2023-06-08T23:22:32.322432Z","shell.execute_reply.started":"2023-06-08T23:16:06.451881Z","shell.execute_reply":"2023-06-08T23:22:32.321379Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35bc0cafccd44a89ac335adce9808765"}},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">This code snippet tokenizes and encodes the dataset using the tokenizer. We define a `tokenize_function` that takes an example as input and applies the tokenizer to the 'content' field of the example. The tokenizer tokenizes the text, truncates it to a maximum length of 512 tokens, and pads the sequences to the same length using the `padding=\"max_length\"` argument. Finally, we apply the `tokenize_function` to the dataset using the `map` method, with `batched=True` to process the examples in batches.\n</div>","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = [item[\"input_ids\"] for item in batch]\n    attention_masks = [item[\"attention_mask\"] for item in batch]\n    labels = [item[\"input_ids\"] for item in batch]\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n    labels = torch.tensor(labels)\n\n    # Pad sequences to the same length\n    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True)\n    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_masks,\n        \"labels\": labels,\n    }","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:32.324097Z","iopub.execute_input":"2023-06-08T23:22:32.324464Z","iopub.status.idle":"2023-06-08T23:22:32.331915Z","shell.execute_reply.started":"2023-06-08T23:22:32.324431Z","shell.execute_reply":"2023-06-08T23:22:32.331011Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n<br>The `collate_fn` function described here is a custom collate function for a PyTorch DataLoader. It takes a batch of data samples and processes them to ensure that sequences within the batch have the same length, suitable for training a language model like GPT-2.<br>\n<br>\nHere is a step-by-step description of the `collate_fn` function:<br>\n    \n1. Extracts the `\"input_ids\"`, `\"attention_mask\"`, and `\"labels\"` from each item in the batch.<br>\n\n2. Converts the extracted lists into tensors using `torch.tensor()`. This step is necessary because `pad_sequence` expects tensors as input.<br>\n\n3. Applies `torch.nn.utils.rnn.pad_sequence()` to the `input_ids`, `attention_masks`, and `labels` tensors to pad the sequences to the same length. The `pad_sequence` function pads sequences with zeros along the batch dimension, ensuring that all sequences in the batch have the same length.<br>\n\n4. Returns a dictionary containing the padded `input_ids`, `attention_mask`, and `labels` tensors.<br>\n<br></div>","metadata":{}},{"cell_type":"code","source":"# Prepare the data for training\ntrain_dataset = tokenized_dataset[\"train\"]\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:32.333461Z","iopub.execute_input":"2023-06-08T23:22:32.334142Z","iopub.status.idle":"2023-06-08T23:22:32.400249Z","shell.execute_reply.started":"2023-06-08T23:22:32.334108Z","shell.execute_reply":"2023-06-08T23:22:32.399224Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThis snippet prepares the tokenized dataset for training. We extract the 'train' split of the tokenized dataset and assign it to the `train_dataset` variable. Then, we create a data loader using `DataLoader`, passing the `train_dataset`, setting the `batch_size` to 4 and enabling shuffling of the data with `shuffle=True`. We didn't mention the collate_fn here since tokenizer already takes care of max_length.</div>\n","metadata":{}},{"cell_type":"code","source":"# Set up the training parameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:32.401951Z","iopub.execute_input":"2023-06-08T23:22:32.402313Z","iopub.status.idle":"2023-06-08T23:22:37.236627Z","shell.execute_reply.started":"2023-06-08T23:22:32.402278Z","shell.execute_reply":"2023-06-08T23:22:37.233860Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThis code snippet sets up the training parameters. It checks if a GPU is available and assigns the appropriate device to the `device` variable. Then, we move the model to the selected device using the `to(device)` method. We also initialize the AdamW optimizer with the model parameters and a learning rate of `1e-5`.</div>\n","metadata":{}},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:37.238230Z","iopub.execute_input":"2023-06-08T23:22:37.238566Z","iopub.status.idle":"2023-06-08T23:22:37.272231Z","shell.execute_reply.started":"2023-06-08T23:22:37.238532Z","shell.execute_reply":"2023-06-08T23:22:37.270925Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[21793,   220,   851,  ..., 12069,   416,  3899],\n        [31567, 43535, 14015,  ...,   447,   247,    82],\n        [31632,   447,   247,  ...,   286,   262,   995],\n        [ 2215,   257,  4744,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[21793,   220,   851,  ..., 12069,   416,  3899],\n        [31567, 43535, 14015,  ...,   447,   247,    82],\n        [31632,   447,   247,  ...,   286,   262,   995],\n        [ 2215,   257,  4744,  ..., 50256, 50256, 50256]])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nIn this code snippet, We perform a sanity check of the dataloader.</div>","metadata":{}},{"cell_type":"code","source":"# Training loop\nmodel.train()\nnum_epochs=1\nfor epoch in range(num_epochs):\n    for step,batch in enumerate(train_dataloader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"input_ids\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        if step%400==0:\n            print(\"Step-{},Loss-{}\".format(step,loss.item()))\n            break\n        loss.backward()\n        optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:37.275889Z","iopub.execute_input":"2023-06-08T23:22:37.276257Z","iopub.status.idle":"2023-06-08T23:22:38.882758Z","shell.execute_reply.started":"2023-06-08T23:22:37.276222Z","shell.execute_reply":"2023-06-08T23:22:38.881735Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Step-0,Loss-5.218831539154053\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThis code snippet defines the training loop. We set the model to training mode using `model.train()`. Then, for each epoch in the specified number of epochs, we iterate over the batches in the `train_dataloader`. Inside the loop, we move the input tensors (`input_ids`, `attention_mask`, and `labels`) to the appropriate device. We zero the gradients with `optimizer.zero_grad()`, forward pass the inputs through the model, compute the loss, perform backward propagation with `loss.backward()`, and update the model parameters using `optimizer.step()`.</div>\n","metadata":{}},{"cell_type":"code","source":"# Save the trained model\noutput_path = '/kaggle/working/GPT2-model.pth'\ntorch.save(model.state_dict(), output_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:38.884216Z","iopub.execute_input":"2023-06-08T23:22:38.885191Z","iopub.status.idle":"2023-06-08T23:22:39.651079Z","shell.execute_reply.started":"2023-06-08T23:22:38.885155Z","shell.execute_reply":"2023-06-08T23:22:39.650106Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThis code snippet saves the trained model to a file.The `state_dict()` method of the model returns a dictionary containing the model's parameters, which is then saved using `torch.save()`.\n</div>","metadata":{}},{"cell_type":"code","source":"# Load the trained model\nmodel_path = '/kaggle/working/GPT2-model.pth'\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.load_state_dict(torch.load(model_path))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:39.652546Z","iopub.execute_input":"2023-06-08T23:22:39.652900Z","iopub.status.idle":"2023-06-08T23:22:42.562409Z","shell.execute_reply.started":"2023-06-08T23:22:39.652866Z","shell.execute_reply":"2023-06-08T23:22:42.561484Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThis code snippet loads the trained model from the saved checkpoint for further inferencing.</div>","metadata":{}},{"cell_type":"code","source":"# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n# Set the tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:42.563949Z","iopub.execute_input":"2023-06-08T23:22:42.564309Z","iopub.status.idle":"2023-06-08T23:22:42.937029Z","shell.execute_reply.started":"2023-06-08T23:22:42.564277Z","shell.execute_reply":"2023-06-08T23:22:42.936052Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe code snippet abve sets the device to GPU if available else use the CPU. It then moves the model to device. It also sets the model to evaluation mode.Initialization of tokenizer is done at the last.</div>","metadata":{}},{"cell_type":"code","source":"# Generate text\nprompt = \"Once upon a time\"\ninput_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:42.939693Z","iopub.execute_input":"2023-06-08T23:22:42.940322Z","iopub.status.idle":"2023-06-08T23:22:45.163678Z","shell.execute_reply.started":"2023-06-08T23:22:42.940285Z","shell.execute_reply":"2023-06-08T23:22:45.162715Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n\nThe code snippet defined above does the following :<br>\n   - Set the `prompt` variable to the desired starting text.<br>\n   - Encode the prompt using the tokenizer and convert it to a PyTorch tensor.<br>\n   - Generate text using the trained model by calling `model.generate()`. Adjust the `max_length` parameter to control the length of the generated text, and `num_return_sequences` to control the number of different texts generated.<br>\n<br>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Decode and print the generated text\nfor i, generated in enumerate(output):\n    text = tokenizer.decode(generated, skip_special_tokens=True)\n    print(f\"Generated text {i+1}: {text}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:22:45.166222Z","iopub.execute_input":"2023-06-08T23:22:45.166588Z","iopub.status.idle":"2023-06-08T23:22:45.178239Z","shell.execute_reply.started":"2023-06-08T23:22:45.166553Z","shell.execute_reply":"2023-06-08T23:22:45.177014Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Generated text 1: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe code snippet decodes the generated tensor into readable text using the tokenizer's `decode()` function and prints out the generated text.</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nConclusion<br>\nWe trained the model on the corpus for 400 steps of Epoch 0, only ! You can train more to expect much better accuracy ! :D <br>\n    </div>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}