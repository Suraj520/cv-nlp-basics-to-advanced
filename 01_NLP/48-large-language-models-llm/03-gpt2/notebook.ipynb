{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Architectural Overview.\n\n> GPT2(Generative Pre-trained Transformer 2)\n\n- It is a language model that utilizes a transformer based architecture and comprises of several key components like Input Embeddings, Encoder layers, Decoder layers and Output Layers.\n- Input Embedding : In this the input text is converted to numerical representations that can be understood by the model. The embedding layer is being deployed for this task which maps each word or token in the input seq to a high dim vector.\n- Encoder layer - GPT2 consists of multiple identical encoder layers stacked over each other. Each encoder layer has two sub layers which are a self attention mechanism and feed forwd network. The self attention mechanism allows the model to weigh the importance of diff words or tokens with inp. seq thereby capturing the dependencies and relationships betw. them. The feed forward network processes the self attn outputs to gen more complex representations.\n- Decoder layer - It follows the encoder layers and has a similar structure as it also consists of self attention and feed forward layers. Jus that in this the decoder layer is conditioned on the context from the prev. tokens enabling autoregressive generation. This means the model predicts the next word in the seq based on the context it has learned so far.\n- Output layer - The final layer of GPT2 is a linear transformation followed by a softmax activation function. This layer produces the prob. distribution over the vocab for the next word in the sequence. It alows the model to generate text by sampling from the distribution or choosing the word with the highest probability.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\nfrom datasets import load_dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:22:33.468283Z","iopub.execute_input":"2023-05-23T20:22:33.468632Z","iopub.status.idle":"2023-05-23T20:22:46.009992Z","shell.execute_reply.started":"2023-05-23T20:22:33.468603Z","shell.execute_reply":"2023-05-23T20:22:46.006905Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This snippet imports the necessary libraries and modules for the code. We import torch for PyTorch functionality, DataLoader for creating data loaders, GPT2LMHeadModel and GPT2Tokenizer from transformers for the GPT-2 model and tokenizer, and AdamW for the optimizer. We also import load_dataset from datasets to load the Kaggle dataset.\n","metadata":{}},{"cell_type":"code","source":"# Load and preprocess the dataset\ndataset = load_dataset(\"csv\", data_files=\"/kaggle/input/all-the-news/articles1.csv\")\ntext_samples = dataset[\"train\"][\"content\"]\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:22:46.012090Z","iopub.execute_input":"2023-05-23T20:22:46.012946Z","iopub.status.idle":"2023-05-23T20:22:53.035925Z","shell.execute_reply.started":"2023-05-23T20:22:46.012908Z","shell.execute_reply":"2023-05-23T20:22:53.034935Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-fda816395acb38ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e55ac38a53b4cb281d9c0cfda2a8de2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279de2bfff0449c2af09b7a42b77ef1d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fda816395acb38ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094a40700d6040aa8bd64c248d8e61d4"}},"metadata":{}}]},{"cell_type":"markdown","source":"This code snippet loads and preprocesses the dataset. We use the `load_dataset` function from the `datasets` library to load the dataset from the CSV file. We then extract the text samples from the training split of the dataset and store them in the `text_samples` variable.\n","metadata":{}},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:22:53.040689Z","iopub.execute_input":"2023-05-23T20:22:53.042981Z","iopub.status.idle":"2023-05-23T20:22:53.053606Z","shell.execute_reply.started":"2023-05-23T20:22:53.042944Z","shell.execute_reply":"2023-05-23T20:22:53.052631Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url', 'content'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize the tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:22:53.059786Z","iopub.execute_input":"2023-05-23T20:22:53.061972Z","iopub.status.idle":"2023-05-23T20:23:00.356515Z","shell.execute_reply.started":"2023-05-23T20:22:53.061937Z","shell.execute_reply":"2023-05-23T20:23:00.355635Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4003f2eb8484fdaa0fabb3efac632d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2edf458b2b8d4eefa93ab685afe1121d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38bcbab4b014c37aff99b40dc24e43e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e175e0416d4ed1b5f6cb5b6910934f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e1aebe5ec94843af226c689a9b843a"}},"metadata":{}}]},{"cell_type":"markdown","source":"In this snippet, we initialize the GPT-2 tokenizer and model. We use `GPT2Tokenizer.from_pretrained` to load the GPT-2 tokenizer from the 'gpt2' pre-trained model. Similarly, we use `GPT2LMHeadModel.from_pretrained` to load the GPT-2 model. We also add the pad token as `eos_token`","metadata":{}},{"cell_type":"code","source":"# Tokenize and encode the dataset\ndef tokenize_function(example):\n    return tokenizer(example[\"content\"], truncation=True, max_length=512, padding=\"max_length\")\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:23:00.358259Z","iopub.execute_input":"2023-05-23T20:23:00.358902Z","iopub.status.idle":"2023-05-23T20:29:24.542600Z","shell.execute_reply.started":"2023-05-23T20:23:00.358843Z","shell.execute_reply":"2023-05-23T20:29:24.541587Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7812965c2645df88c0ff3768dbf11a"}},"metadata":{}}]},{"cell_type":"markdown","source":"This code snippet tokenizes and encodes the dataset using the tokenizer. We define a `tokenize_function` that takes an example as input and applies the tokenizer to the 'content' field of the example. The tokenizer tokenizes the text, truncates it to a maximum length of 512 tokens, and pads the sequences to the same length using the `padding=\"max_length\"` argument. Finally, we apply the `tokenize_function` to the dataset using the `map` method, with `batched=True` to process the examples in batches.\n","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = [item[\"input_ids\"] for item in batch]\n    attention_masks = [item[\"attention_mask\"] for item in batch]\n    labels = [item[\"input_ids\"] for item in batch]\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n    labels = torch.tensor(labels)\n\n    # Pad sequences to the same length\n    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True)\n    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_masks,\n        \"labels\": labels,\n    }","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:29:24.543923Z","iopub.execute_input":"2023-05-23T20:29:24.544917Z","iopub.status.idle":"2023-05-23T20:29:24.553986Z","shell.execute_reply.started":"2023-05-23T20:29:24.544879Z","shell.execute_reply":"2023-05-23T20:29:24.551806Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The `collate_fn` function described here is a custom collate function for a PyTorch DataLoader. It takes a batch of data samples and processes them to ensure that sequences within the batch have the same length, suitable for training a language model like GPT-2.\n\nHere is a step-by-step description of the `collate_fn` function:\n\n1. Extracts the `\"input_ids\"`, `\"attention_mask\"`, and `\"labels\"` from each item in the batch.\n\n2. Converts the extracted lists into tensors using `torch.tensor()`. This step is necessary because `pad_sequence` expects tensors as input.\n\n3. Applies `torch.nn.utils.rnn.pad_sequence()` to the `input_ids`, `attention_masks`, and `labels` tensors to pad the sequences to the same length. The `pad_sequence` function pads sequences with zeros along the batch dimension, ensuring that all sequences in the batch have the same length.\n\n4. Returns a dictionary containing the padded `input_ids`, `attention_mask`, and `labels` tensors.\n","metadata":{}},{"cell_type":"code","source":"# Prepare the data for training\ntrain_dataset = tokenized_dataset[\"train\"]\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:29:24.555381Z","iopub.execute_input":"2023-05-23T20:29:24.555714Z","iopub.status.idle":"2023-05-23T20:29:24.594280Z","shell.execute_reply.started":"2023-05-23T20:29:24.555683Z","shell.execute_reply":"2023-05-23T20:29:24.593401Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"This snippet prepares the tokenized dataset for training. We extract the 'train' split of the tokenized dataset and assign it to the `train_dataset` variable. Then, we create a data loader using `DataLoader`, passing the `train_dataset`, setting the `batch_size` to 4 and enabling shuffling of the data with `shuffle=True`. We didn't mention the collate_fn here since tokenizer already takes care of max_length\n","metadata":{}},{"cell_type":"code","source":"# Set up the training parameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:29:24.596671Z","iopub.execute_input":"2023-05-23T20:29:24.597423Z","iopub.status.idle":"2023-05-23T20:29:29.326590Z","shell.execute_reply.started":"2023-05-23T20:29:24.597389Z","shell.execute_reply":"2023-05-23T20:29:29.324576Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This code snippet sets up the training parameters. It checks if a GPU is available and assigns the appropriate device to the `device` variable. Then, we move the model to the selected device using the `to(device)` method. We also initialize the AdamW optimizer with the model parameters and a learning rate of `1e-5`.\n","metadata":{}},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:29:29.328011Z","iopub.execute_input":"2023-05-23T20:29:29.328355Z","iopub.status.idle":"2023-05-23T20:29:29.362484Z","shell.execute_reply.started":"2023-05-23T20:29:29.328321Z","shell.execute_reply":"2023-05-23T20:29:29.361582Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  357, 18474,     8,  ..., 11868,   286,  2042],\n        [14282, 16381,  8783,  ..., 50256, 50256, 50256],\n        [  357, 18474,     8,  ...,   290,   384, 25924],\n        [12256,  2097, 20163,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  357, 18474,     8,  ..., 11868,   286,  2042],\n        [14282, 16381,  8783,  ..., 50256, 50256, 50256],\n        [  357, 18474,     8,  ...,   290,   384, 25924],\n        [12256,  2097, 20163,  ..., 50256, 50256, 50256]])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this code snippet, We perform a sanity check of the dataloader.","metadata":{}},{"cell_type":"code","source":"# Training loop\nmodel.train()\nnum_epochs=1\nfor epoch in range(num_epochs):\n    for step,batch in enumerate(train_dataloader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"input_ids\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        if step%100==0:\n            print(\"Step-{},Loss-{}\".format(step,loss.item()))\n        loss.backward()\n        optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:29:29.365420Z","iopub.execute_input":"2023-05-23T20:29:29.366051Z","iopub.status.idle":"2023-05-23T20:37:12.362073Z","shell.execute_reply.started":"2023-05-23T20:29:29.366025Z","shell.execute_reply":"2023-05-23T20:37:12.360647Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Step-0,Loss-4.9859137535095215\nStep-100,Loss-3.182889461517334\nStep-200,Loss-3.055086612701416\nStep-300,Loss-2.802694320678711\nStep-400,Loss-2.550711154937744\nStep-500,Loss-2.6914784908294678\nStep-600,Loss-1.813125491142273\nStep-700,Loss-2.259305000305176\nStep-800,Loss-2.0252110958099365\nStep-900,Loss-2.8783481121063232\nStep-1000,Loss-1.7711089849472046\nStep-1100,Loss-2.5594494342803955\nStep-1200,Loss-1.7512391805648804\nStep-1300,Loss-2.6287434101104736\nStep-1400,Loss-2.5168161392211914\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,Loss-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(step,loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:455\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    452\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    453\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m step_size \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2) \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[0;32m--> 455\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Add weight decay at the end (fixed version)\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"This code snippet defines the training loop. We set the model to training mode using `model.train()`. Then, for each epoch in the specified number of epochs, we iterate over the batches in the `train_dataloader`. Inside the loop, we move the input tensors (`input_ids`, `attention_mask`, and `labels`) to the appropriate device. We zero the gradients with `optimizer.zero_grad()`, forward pass the inputs through the model, compute the loss, perform backward propagation with `loss.backward()`, and update the model parameters using `optimizer.step()`.\n","metadata":{}},{"cell_type":"code","source":"# Save the trained model\noutput_path = '/kaggle/working/GPT2-model.pth'\ntorch.save(model.state_dict(), output_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:37:16.101739Z","iopub.execute_input":"2023-05-23T20:37:16.102616Z","iopub.status.idle":"2023-05-23T20:37:17.262405Z","shell.execute_reply.started":"2023-05-23T20:37:16.102565Z","shell.execute_reply":"2023-05-23T20:37:17.261452Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"This code snippet saves the trained model to a file.The `state_dict()` method of the model returns a dictionary containing the model's parameters, which is then saved using `torch.save()`.\n","metadata":{}},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# Load the trained model\nmodel_path = '/kaggle/working/GPT2-model.pth'\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.load_state_dict(torch.load(model_path))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:37:25.863598Z","iopub.execute_input":"2023-05-23T20:37:25.863977Z","iopub.status.idle":"2023-05-23T20:37:29.499705Z","shell.execute_reply.started":"2023-05-23T20:37:25.863944Z","shell.execute_reply":"2023-05-23T20:37:29.498737Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"This code snippet loads the trained model from the saved checkpoint for further inferencing.","metadata":{}},{"cell_type":"code","source":"# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n# Set the tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:37:31.678574Z","iopub.execute_input":"2023-05-23T20:37:31.678954Z","iopub.status.idle":"2023-05-23T20:37:32.084776Z","shell.execute_reply.started":"2023-05-23T20:37:31.678923Z","shell.execute_reply":"2023-05-23T20:37:32.083804Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"The code snippet abve sets the device to GPU if available else use the CPU. It then moves the model to device. It also sets the model to evaluation mode.\n\nInitialization of tokenizer is done at the last.","metadata":{}},{"cell_type":"code","source":"# Generate text\nprompt = \"Once upon a time\"\ninput_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:37:43.188633Z","iopub.execute_input":"2023-05-23T20:37:43.189007Z","iopub.status.idle":"2023-05-23T20:37:45.387252Z","shell.execute_reply.started":"2023-05-23T20:37:43.188975Z","shell.execute_reply":"2023-05-23T20:37:45.386303Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code snippet defined above does the following :\n   - Set the `prompt` variable to the desired starting text.\n   - Encode the prompt using the tokenizer and convert it to a PyTorch tensor.\n   - Generate text using the trained model by calling `model.generate()`. Adjust the `max_length` parameter to control the length of the generated text, and `num_return_sequences` to control the number of different texts generated.\n\n","metadata":{}},{"cell_type":"code","source":"# Decode and print the generated text\nfor i, generated in enumerate(output):\n    text = tokenizer.decode(generated, skip_special_tokens=True)\n    print(f\"Generated text {i+1}: {text}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-23T20:37:47.356315Z","iopub.execute_input":"2023-05-23T20:37:47.356684Z","iopub.status.idle":"2023-05-23T20:37:47.367186Z","shell.execute_reply.started":"2023-05-23T20:37:47.356651Z","shell.execute_reply":"2023-05-23T20:37:47.366097Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Generated text 1: Once upon a time, the world was awash in the   of the                                                                                    \n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code snippet decodes the generated tensor into readable text using the tokenizer's `decode()` function and prints out the generated text.","metadata":{}},{"cell_type":"markdown","source":"Conclusion -\n\nWe trained the model on the corpus for 1400 steps of Epoch 0, only ! You can train more to expect much better accuracy ! :D","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}