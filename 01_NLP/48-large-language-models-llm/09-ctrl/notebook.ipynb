{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#035FCA; color:#19180F; font-size:40px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> CTRL(Conditional Transformer Language Model) </div>\n\n<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">üîßArchitecture Overview‚öôÔ∏è\n </div>\n<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nIt is a language model architecture designed to generate human like text based on a given control code or prompt. It combines the power of transformers with the ability to condition the model's output based on specific instructions.<br>\n\nThe architecture of CTRL consists of various building blocks which are explained below<br>\n\n1. Input Encoding : The input text is first tokenized. These tokens capture the meaning & structure of the text. Each token is then converted into a high dimensional vector representation.<br>\n\n2. Transformer Encoder - The encoded input tokens are passed through multiple layers of transformer encoders. Each encoder layer consists of two sub layers - A multi headed self attn mechanism and a position wise feedfwd neural network. The self attn mechanism helps the model capture dependencies between diff words in the i/p whereas the feed forward network applies non linear transformations to each position in the sequence.<br>\n\n3. Control code embedding : CTRL introduces an additional control code embedding to allow conditioning the model's output on specific instructions. The control code is appenedd to the input tokens and has its own learned embedding representations. This enables fine grained control over the generated text.<br>\n\n4. Transformer Decoder : The output of the control code embedding is passed through a series of transformer decoder layers. Each decoder layer also consists of self attn and feed forward sub layers. The decoder layers refine the representation of the control code and generate the final output tokens.<br>\n\n5. Output decoding - The decoder's o/p tokens are decoded to generate the final text. The decoding process involves converting the token representations back into text.<br>\n\n\n<b>Architectural differences w.r.t ELECTRA, ALBERT, BERT, RoBERTa, GPT2, XLNet and T5</b><br>\n\n- Controlled Generation - CTRL is specifically designed for controlled text generation where a control code or prompt guides the output. It allows users to specify the desired style, topic or other attributes of the generated text.<br>\n\n- Bidirectionality - Unlike models like GPT2 and XLNet, which are unidirectional llms, CTRL is bidirectional i.e It can utilize both the left and right contexts of a token to generate its representations. This allows the model to have a better understanding of the dependencies within the text like Bidirectional LSTM does.<br>\n\n- Control code conditioning - It introduces control code embedding which is a unique component in its architecture. It enables the model to condition its output based on the given control code making it highly flexible for various language generation tasks.<br>\n\n- Diff pretraining objective- UNlike ELECTRA and BERT which employs MLM as their pretraining objective, CTRL employs a new objective called \"relevance ranking\" which involves compairing different parts of the input text to identify the most relevant content.<br>\n\n- Domain adaptation - CTRL supports domain adaptation, allowing fine tuning of the model on specific domains or datasets. This improves the model's performance and adaptability for particular applications.<br>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> üè¢ Architecture Diagram.\n </div>\n<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n\n1. Input: This is the text that is entered into the CTRL Transformer architecture.<br>\n\n2. Tokenization: The tokenizer performs the tokenization procedure on the supplied text. Tokenization divides the input text into individual tokens, which are the basic units of input for the language model.<br>\n\n3. Encoder: The essential component of the CTRL Transformer design is the encoder. It is made up of many encoder blocks, each of which is responsible for processing a piece of the input text. Four encoder blocks are presented in pairs in this figure.<br>\n\n4. Encoder Blocks: The encoder blocks take the tokenized input text and conduct different actions on it in order to capture the tokens' contextual information. Each encoder block has a number of components that are shared by the pairings.<br>\n\n5. Encoder Components: The following are the major components found in each encoder block:<br>\n   - Self-Attention: This component enables the model to focus on various segments of the input sequence while taking into account token connections.<br>\n   - Multi-Head Attention: It expands self-attention by attending to and mixing distinct representations (heads) of the information.<br>\n   - Feed-Forward Network: This component processes attended representations and extracts higher-level features using a collection of fully linked layers.<br>\n   - Residual Connection: It improves information flow by connecting the feed-forward network's output to the encoder block's input.<br>\n   - Layer Normalisation: This component normalises the encoder block's output, assisting in the stabilisation of the training process.<br>\n\n6. Connections: The connections between the parts show how information and data move across the encoder blocks. The data transmission direction is shown by the arrows.<br>\n\n7. Output: The processed representations go to the last layer normalisation component after passing via the encoder blocks, where they produce the output text.<br>\n\nThe illustration shows the process by which the input text is tokenized, then processed by the encoder blocks with their corresponding components, and ultimately turned into the output text.<br>\n</div>\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import SVG, display\n\n# Load the SVG file and display it\nsvg_file = '/kaggle/input/notebook-images/ctrl.svg'\ndisplay(SVG(filename=svg_file))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:26:57.188194Z","iopub.execute_input":"2023-06-10T22:26:57.188514Z","iopub.status.idle":"2023-06-10T22:26:57.221634Z","shell.execute_reply.started":"2023-06-10T22:26:57.188483Z","shell.execute_reply":"2023-06-10T22:26:57.220820Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"588pt\" height=\"910pt\" viewBox=\"0.00 0.00 588.00 910.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 906)\">\n<title>CTRL_Transformer_Architecture</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-906 584,-906 584,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_input</title>\n<path fill=\"none\" stroke=\"#add8e6\" d=\"M52,-817.2C52,-817.2 114,-817.2 114,-817.2 120,-817.2 126,-823.2 126,-829.2 126,-829.2 126,-882 126,-882 126,-888 120,-894 114,-894 114,-894 52,-894 52,-894 46,-894 40,-888 40,-882 40,-882 40,-829.2 40,-829.2 40,-823.2 46,-817.2 52,-817.2\"/>\n<text text-anchor=\"middle\" x=\"83\" y=\"-877.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input</text>\n</g>\n<g id=\"clust2\" class=\"cluster\">\n<title>cluster_tokenization</title>\n<path fill=\"none\" stroke=\"#d3d3d3\" d=\"M51,-732.4C51,-732.4 116,-732.4 116,-732.4 122,-732.4 128,-738.4 128,-744.4 128,-744.4 128,-797.2 128,-797.2 128,-803.2 122,-809.2 116,-809.2 116,-809.2 51,-809.2 51,-809.2 45,-809.2 39,-803.2 39,-797.2 39,-797.2 39,-744.4 39,-744.4 39,-738.4 45,-732.4 51,-732.4\"/>\n<text text-anchor=\"middle\" x=\"83.5\" y=\"-792.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Tokenization</text>\n</g>\n<g id=\"clust3\" class=\"cluster\">\n<title>cluster_encoder</title>\n<path fill=\"none\" stroke=\"#ffffe0\" d=\"M20,-92.8C20,-92.8 560,-92.8 560,-92.8 566,-92.8 572,-98.8 572,-104.8 572,-104.8 572,-712.4 572,-712.4 572,-718.4 566,-724.4 560,-724.4 560,-724.4 20,-724.4 20,-724.4 14,-724.4 8,-718.4 8,-712.4 8,-712.4 8,-104.8 8,-104.8 8,-98.8 14,-92.8 20,-92.8\"/>\n<text text-anchor=\"middle\" x=\"290\" y=\"-707.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder</text>\n</g>\n<g id=\"clust4\" class=\"cluster\">\n<title>cluster_encoder_blocks</title>\n<path fill=\"none\" stroke=\"#add8e6\" d=\"M28,-391.2C28,-391.2 109,-391.2 109,-391.2 115,-391.2 121,-397.2 121,-403.2 121,-403.2 121,-679.6 121,-679.6 121,-685.6 115,-691.6 109,-691.6 109,-691.6 28,-691.6 28,-691.6 22,-691.6 16,-685.6 16,-679.6 16,-679.6 16,-403.2 16,-403.2 16,-397.2 22,-391.2 28,-391.2\"/>\n<text text-anchor=\"middle\" x=\"68.5\" y=\"-675\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder Blocks</text>\n</g>\n<g id=\"clust5\" class=\"cluster\">\n<title>cluster_encoder_components</title>\n<path fill=\"none\" stroke=\"#d3d3d3\" d=\"M141,-100.8C141,-100.8 552,-100.8 552,-100.8 558,-100.8 564,-106.8 564,-112.8 564,-112.8 564,-456.4 564,-456.4 564,-462.4 558,-468.4 552,-468.4 552,-468.4 141,-468.4 141,-468.4 135,-468.4 129,-462.4 129,-456.4 129,-456.4 129,-112.8 129,-112.8 129,-106.8 135,-100.8 141,-100.8\"/>\n<text text-anchor=\"middle\" x=\"346.5\" y=\"-451.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder Components</text>\n</g>\n<g id=\"clust6\" class=\"cluster\">\n<title>cluster_output</title>\n<path fill=\"none\" stroke=\"#add8e6\" d=\"M255,-8C255,-8 325,-8 325,-8 331,-8 337,-14 337,-20 337,-20 337,-72.8 337,-72.8 337,-78.8 331,-84.8 325,-84.8 325,-84.8 255,-84.8 255,-84.8 249,-84.8 243,-78.8 243,-72.8 243,-72.8 243,-20 243,-20 243,-14 249,-8 255,-8\"/>\n<text text-anchor=\"middle\" x=\"290\" y=\"-68.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Output</text>\n</g>\n<!-- input -->\n<g id=\"node1\" class=\"node\">\n<title>input</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"117.8541,-861.2 48.1459,-861.2 48.1459,-825.2 117.8541,-825.2 117.8541,-861.2\"/>\n<text text-anchor=\"middle\" x=\"83\" y=\"-839.6\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Input Text</text>\n</g>\n<!-- tokenizer -->\n<g id=\"node2\" class=\"node\">\n<title>tokenizer</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"117.1724,-776.4 48.8276,-776.4 48.8276,-740.4 117.1724,-740.4 117.1724,-776.4\"/>\n<text text-anchor=\"middle\" x=\"83\" y=\"-754.8\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Tokenizer</text>\n</g>\n<!-- input&#45;&gt;tokenizer -->\n<g id=\"edge1\" class=\"edge\">\n<title>input-&gt;tokenizer</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M77.3216,-824.7997C76.1968,-813.6112 75.9575,-799.0983 76.6037,-786.4954\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"80.0976,-786.7067 77.3548,-776.4731 73.1172,-786.1835 80.0976,-786.7067\"/>\n</g>\n<!-- input&#45;&gt;tokenizer -->\n<g id=\"edge2\" class=\"edge\">\n<title>input-&gt;tokenizer</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M88.6784,-824.7997C90.1182,-810.4784 90.1071,-790.7105 88.6452,-776.4731\"/>\n</g>\n<!-- encoder_block1 -->\n<g id=\"node3\" class=\"node\">\n<title>encoder_block1</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"113.1816,-658.7005 52.8184,-658.7005 52.8184,-622.0995 113.1816,-622.0995 113.1816,-658.7005\"/>\n<text text-anchor=\"middle\" x=\"83\" y=\"-644\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Encoder</text>\n<text text-anchor=\"middle\" x=\"83\" y=\"-629.6\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Block</text>\n</g>\n<!-- tokenizer&#45;&gt;encoder_block1 -->\n<g id=\"edge3\" class=\"edge\">\n<title>tokenizer-&gt;encoder_block1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M83,-740.2211C83,-718.1114 83,-681.0005 83,-658.794\"/>\n</g>\n<!-- encoder_block2 -->\n<g id=\"node4\" class=\"node\">\n<title>encoder_block2</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"113.1816,-585.9005 52.8184,-585.9005 52.8184,-549.2995 113.1816,-549.2995 113.1816,-585.9005\"/>\n<text text-anchor=\"middle\" x=\"83\" y=\"-571.2\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Encoder</text>\n<text text-anchor=\"middle\" x=\"83\" y=\"-556.8\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Block</text>\n</g>\n<!-- encoder_block1&#45;&gt;encoder_block2 -->\n<g id=\"edge4\" class=\"edge\">\n<title>encoder_block1-&gt;encoder_block2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M83,-622.0295C83,-610.9936 83,-596.9995 83,-585.9646\"/>\n</g>\n<!-- self_attn1 -->\n<g id=\"node7\" class=\"node\">\n<title>self_attn1</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"551.0192,-435.6 462.9808,-435.6 462.9808,-399.6 551.0192,-399.6 551.0192,-435.6\"/>\n<text text-anchor=\"middle\" x=\"507\" y=\"-414\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- encoder_block1&#45;&gt;self_attn1 -->\n<g id=\"edge7\" class=\"edge\">\n<title>encoder_block1-&gt;self_attn1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M113.2061,-629.8233C178.179,-606.3349 334.7376,-545.7374 451,-468.4 464.9895,-459.0942 478.9895,-446.3942 489.4921,-436.0207\"/>\n</g>\n<!-- encoder_block3 -->\n<g id=\"node5\" class=\"node\">\n<title>encoder_block3</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"113.1816,-513.1005 52.8184,-513.1005 52.8184,-476.4995 113.1816,-476.4995 113.1816,-513.1005\"/>\n<text text-anchor=\"middle\" x=\"83\" y=\"-498.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Encoder</text>\n<text text-anchor=\"middle\" x=\"83\" y=\"-484\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Block</text>\n</g>\n<!-- encoder_block2&#45;&gt;encoder_block3 -->\n<g id=\"edge5\" class=\"edge\">\n<title>encoder_block2-&gt;encoder_block3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M83,-549.2295C83,-538.1936 83,-524.1995 83,-513.1646\"/>\n</g>\n<!-- self_attn2 -->\n<g id=\"node8\" class=\"node\">\n<title>self_attn2</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"442.0192,-435.6 353.9808,-435.6 353.9808,-399.6 442.0192,-399.6 442.0192,-435.6\"/>\n<text text-anchor=\"middle\" x=\"398\" y=\"-414\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- encoder_block2&#45;&gt;self_attn2 -->\n<g id=\"edge8\" class=\"edge\">\n<title>encoder_block2-&gt;self_attn2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M113.5582,-560.0283C163.9838,-546.6671 266.6315,-515.7602 343,-468.4 357.3606,-459.4942 371.3193,-446.5334 381.581,-435.9325\"/>\n</g>\n<!-- encoder_block4 -->\n<g id=\"node6\" class=\"node\">\n<title>encoder_block4</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"99.1816,-435.9005 38.8184,-435.9005 38.8184,-399.2995 99.1816,-399.2995 99.1816,-435.9005\"/>\n<text text-anchor=\"middle\" x=\"69\" y=\"-421.2\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Encoder</text>\n<text text-anchor=\"middle\" x=\"69\" y=\"-406.8\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Block</text>\n</g>\n<!-- encoder_block3&#45;&gt;encoder_block4 -->\n<g id=\"edge6\" class=\"edge\">\n<title>encoder_block3-&gt;encoder_block4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M79.611,-476.112C77.4098,-463.9743 74.5527,-448.2192 72.3578,-436.1159\"/>\n</g>\n<!-- self_attn3 -->\n<g id=\"node9\" class=\"node\">\n<title>self_attn3</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"334.0192,-435.6 245.9808,-435.6 245.9808,-399.6 334.0192,-399.6 334.0192,-435.6\"/>\n<text text-anchor=\"middle\" x=\"290\" y=\"-414\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- encoder_block3&#45;&gt;self_attn3 -->\n<g id=\"edge9\" class=\"edge\">\n<title>encoder_block3-&gt;self_attn3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M113.3645,-491.586C151.396,-487.2079 214.1082,-478.7208 235,-468.4 250.7328,-460.6278 265.1674,-446.9258 275.2979,-435.7095\"/>\n</g>\n<!-- self_attn4 -->\n<g id=\"node10\" class=\"node\">\n<title>self_attn4</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"226.0192,-435.6 137.9808,-435.6 137.9808,-399.6 226.0192,-399.6 226.0192,-435.6\"/>\n<text text-anchor=\"middle\" x=\"182\" y=\"-414\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- encoder_block4&#45;&gt;self_attn4 -->\n<g id=\"edge10\" class=\"edge\">\n<title>encoder_block4-&gt;self_attn4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M99.457,-417.6C112.2469,-417.6 125.0368,-417.6 137.8266,-417.6\"/>\n</g>\n<!-- ln4 -->\n<g id=\"node26\" class=\"node\">\n<title>ln4</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"227.8302,-145.5005 138.1698,-145.5005 138.1698,-108.8995 227.8302,-108.8995 227.8302,-145.5005\"/>\n<text text-anchor=\"middle\" x=\"183\" y=\"-130.8\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Layer</text>\n<text text-anchor=\"middle\" x=\"183\" y=\"-116.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Normalization</text>\n</g>\n<!-- encoder_block4&#45;&gt;ln4 -->\n<g id=\"edge29\" class=\"edge\">\n<title>encoder_block4-&gt;ln4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M68.1765,-399.1951C67.1569,-357.306 69.5187,-252.9615 113,-181.6 121.8894,-167.0107 136.2303,-154.8457 149.6239,-145.7454\"/>\n</g>\n<!-- multihead1 -->\n<g id=\"node11\" class=\"node\">\n<title>multihead1</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"544.996,-363.1005 471.004,-363.1005 471.004,-326.4995 544.996,-326.4995 544.996,-363.1005\"/>\n<text text-anchor=\"middle\" x=\"508\" y=\"-348.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Multi-Head</text>\n<text text-anchor=\"middle\" x=\"508\" y=\"-334\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Attention</text>\n</g>\n<!-- self_attn1&#45;&gt;multihead1 -->\n<g id=\"edge11\" class=\"edge\">\n<title>self_attn1-&gt;multihead1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M507.2523,-399.2295C507.4039,-388.1936 507.5962,-374.1995 507.7477,-363.1646\"/>\n</g>\n<!-- multihead2 -->\n<g id=\"node12\" class=\"node\">\n<title>multihead2</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"435.996,-363.1005 362.004,-363.1005 362.004,-326.4995 435.996,-326.4995 435.996,-363.1005\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-348.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Multi-Head</text>\n<text text-anchor=\"middle\" x=\"399\" y=\"-334\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Attention</text>\n</g>\n<!-- self_attn2&#45;&gt;multihead2 -->\n<g id=\"edge12\" class=\"edge\">\n<title>self_attn2-&gt;multihead2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M398.2523,-399.2295C398.4039,-388.1936 398.5962,-374.1995 398.7477,-363.1646\"/>\n</g>\n<!-- multihead3 -->\n<g id=\"node13\" class=\"node\">\n<title>multihead3</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"327.996,-363.1005 254.004,-363.1005 254.004,-326.4995 327.996,-326.4995 327.996,-363.1005\"/>\n<text text-anchor=\"middle\" x=\"291\" y=\"-348.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Multi-Head</text>\n<text text-anchor=\"middle\" x=\"291\" y=\"-334\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Attention</text>\n</g>\n<!-- self_attn3&#45;&gt;multihead3 -->\n<g id=\"edge13\" class=\"edge\">\n<title>self_attn3-&gt;multihead3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M290.2523,-399.2295C290.4039,-388.1936 290.5962,-374.1995 290.7477,-363.1646\"/>\n</g>\n<!-- multihead4 -->\n<g id=\"node14\" class=\"node\">\n<title>multihead4</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"218.996,-363.1005 145.004,-363.1005 145.004,-326.4995 218.996,-326.4995 218.996,-363.1005\"/>\n<text text-anchor=\"middle\" x=\"182\" y=\"-348.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Multi-Head</text>\n<text text-anchor=\"middle\" x=\"182\" y=\"-334\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Attention</text>\n</g>\n<!-- self_attn4&#45;&gt;multihead4 -->\n<g id=\"edge14\" class=\"edge\">\n<title>self_attn4-&gt;multihead4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M182,-399.2295C182,-388.1936 182,-374.1995 182,-363.1646\"/>\n</g>\n<!-- ffn1 -->\n<g id=\"node15\" class=\"node\">\n<title>ffn1</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"555.8342,-290.3005 464.1658,-290.3005 464.1658,-253.6995 555.8342,-253.6995 555.8342,-290.3005\"/>\n<text text-anchor=\"middle\" x=\"510\" y=\"-275.6\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Feed-Forward</text>\n<text text-anchor=\"middle\" x=\"510\" y=\"-261.2\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Network</text>\n</g>\n<!-- multihead1&#45;&gt;ffn1 -->\n<g id=\"edge15\" class=\"edge\">\n<title>multihead1-&gt;ffn1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M508.5047,-326.4295C508.8079,-315.3936 509.1923,-301.3995 509.4955,-290.3646\"/>\n</g>\n<!-- ffn2 -->\n<g id=\"node16\" class=\"node\">\n<title>ffn2</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"446.8342,-290.3005 355.1658,-290.3005 355.1658,-253.6995 446.8342,-253.6995 446.8342,-290.3005\"/>\n<text text-anchor=\"middle\" x=\"401\" y=\"-275.6\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Feed-Forward</text>\n<text text-anchor=\"middle\" x=\"401\" y=\"-261.2\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Network</text>\n</g>\n<!-- multihead2&#45;&gt;ffn2 -->\n<g id=\"edge16\" class=\"edge\">\n<title>multihead2-&gt;ffn2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M399.5047,-326.4295C399.8079,-315.3936 400.1923,-301.3995 400.4955,-290.3646\"/>\n</g>\n<!-- ffn3 -->\n<g id=\"node17\" class=\"node\">\n<title>ffn3</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"337.8342,-290.3005 246.1658,-290.3005 246.1658,-253.6995 337.8342,-253.6995 337.8342,-290.3005\"/>\n<text text-anchor=\"middle\" x=\"292\" y=\"-275.6\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Feed-Forward</text>\n<text text-anchor=\"middle\" x=\"292\" y=\"-261.2\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Network</text>\n</g>\n<!-- multihead3&#45;&gt;ffn3 -->\n<g id=\"edge17\" class=\"edge\">\n<title>multihead3-&gt;ffn3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M291.2523,-326.4295C291.4039,-315.3936 291.5962,-301.3995 291.7477,-290.3646\"/>\n</g>\n<!-- ffn4 -->\n<g id=\"node18\" class=\"node\">\n<title>ffn4</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"228.8342,-290.3005 137.1658,-290.3005 137.1658,-253.6995 228.8342,-253.6995 228.8342,-290.3005\"/>\n<text text-anchor=\"middle\" x=\"183\" y=\"-275.6\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Feed-Forward</text>\n<text text-anchor=\"middle\" x=\"183\" y=\"-261.2\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Network</text>\n</g>\n<!-- multihead4&#45;&gt;ffn4 -->\n<g id=\"edge18\" class=\"edge\">\n<title>multihead4-&gt;ffn4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M182.2523,-326.4295C182.4039,-315.3936 182.5962,-301.3995 182.7477,-290.3646\"/>\n</g>\n<!-- residual1 -->\n<g id=\"node19\" class=\"node\">\n<title>residual1</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"541.1725,-217.6 478.8275,-217.6 478.8275,-181.6 541.1725,-181.6 541.1725,-217.6\"/>\n<text text-anchor=\"middle\" x=\"510\" y=\"-196\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Residual</text>\n</g>\n<!-- ffn1&#45;&gt;residual1 -->\n<g id=\"edge19\" class=\"edge\">\n<title>ffn1-&gt;residual1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M510,-253.3552C510,-242.4618 510,-228.77 510,-217.931\"/>\n</g>\n<!-- residual2 -->\n<g id=\"node20\" class=\"node\">\n<title>residual2</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"430.1725,-217.6 367.8275,-217.6 367.8275,-181.6 430.1725,-181.6 430.1725,-217.6\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-196\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Residual</text>\n</g>\n<!-- ffn2&#45;&gt;residual2 -->\n<g id=\"edge20\" class=\"edge\">\n<title>ffn2-&gt;residual2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M400.485,-253.3552C400.184,-242.4618 399.8058,-228.77 399.5064,-217.931\"/>\n</g>\n<!-- residual3 -->\n<g id=\"node21\" class=\"node\">\n<title>residual3</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"322.1725,-217.6 259.8275,-217.6 259.8275,-181.6 322.1725,-181.6 322.1725,-217.6\"/>\n<text text-anchor=\"middle\" x=\"291\" y=\"-196\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Residual</text>\n</g>\n<!-- ffn3&#45;&gt;residual3 -->\n<g id=\"edge21\" class=\"edge\">\n<title>ffn3-&gt;residual3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M291.7425,-253.3552C291.592,-242.4618 291.4029,-228.77 291.2532,-217.931\"/>\n</g>\n<!-- residual4 -->\n<g id=\"node22\" class=\"node\">\n<title>residual4</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"214.1725,-217.6 151.8275,-217.6 151.8275,-181.6 214.1725,-181.6 214.1725,-217.6\"/>\n<text text-anchor=\"middle\" x=\"183\" y=\"-196\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Residual</text>\n</g>\n<!-- ffn4&#45;&gt;residual4 -->\n<g id=\"edge22\" class=\"edge\">\n<title>ffn4-&gt;residual4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M183,-253.3552C183,-242.4618 183,-228.77 183,-217.931\"/>\n</g>\n<!-- ln1 -->\n<g id=\"node23\" class=\"node\">\n<title>ln1</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"554.8302,-145.5005 465.1698,-145.5005 465.1698,-108.8995 554.8302,-108.8995 554.8302,-145.5005\"/>\n<text text-anchor=\"middle\" x=\"510\" y=\"-130.8\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Layer</text>\n<text text-anchor=\"middle\" x=\"510\" y=\"-116.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Normalization</text>\n</g>\n<!-- residual1&#45;&gt;ln1 -->\n<g id=\"edge23\" class=\"edge\">\n<title>residual1-&gt;ln1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M510,-181.3304C510,-170.4389 510,-156.6503 510,-145.7156\"/>\n</g>\n<!-- ln2 -->\n<g id=\"node24\" class=\"node\">\n<title>ln2</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"442.8302,-145.5005 353.1698,-145.5005 353.1698,-108.8995 442.8302,-108.8995 442.8302,-145.5005\"/>\n<text text-anchor=\"middle\" x=\"398\" y=\"-130.8\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Layer</text>\n<text text-anchor=\"middle\" x=\"398\" y=\"-116.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Normalization</text>\n</g>\n<!-- residual2&#45;&gt;ln2 -->\n<g id=\"edge24\" class=\"edge\">\n<title>residual2-&gt;ln2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M398.7477,-181.3304C398.5972,-170.4389 398.4068,-156.6503 398.2557,-145.7156\"/>\n</g>\n<!-- ln3 -->\n<g id=\"node25\" class=\"node\">\n<title>ln3</title>\n<polygon fill=\"#ffffff\" stroke=\"#ffffff\" points=\"335.8302,-145.5005 246.1698,-145.5005 246.1698,-108.8995 335.8302,-108.8995 335.8302,-145.5005\"/>\n<text text-anchor=\"middle\" x=\"291\" y=\"-130.8\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Layer</text>\n<text text-anchor=\"middle\" x=\"291\" y=\"-116.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Normalization</text>\n</g>\n<!-- residual3&#45;&gt;ln3 -->\n<g id=\"edge25\" class=\"edge\">\n<title>residual3-&gt;ln3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M291,-181.3304C291,-170.4389 291,-156.6503 291,-145.7156\"/>\n</g>\n<!-- residual4&#45;&gt;ln4 -->\n<g id=\"edge26\" class=\"edge\">\n<title>residual4-&gt;ln4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M183,-181.3304C183,-170.4389 183,-156.6503 183,-145.7156\"/>\n</g>\n<!-- ln2&#45;&gt;encoder_block3 -->\n<g id=\"edge28\" class=\"edge\">\n<title>ln2-&gt;encoder_block3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M353.3332,-143.2628C350.5271,-144.099 347.7331,-144.8868 345,-145.6 256.7624,-168.6257 210.4239,-120.1991 143,-181.6 112.1792,-209.6676 118,-230.3141 118,-272 118,-344.8 118,-344.8 118,-344.8 118,-399.9128 130.4451,-418.0647 108,-468.4 106.7661,-471.1672 105.1159,-473.8249 103.2491,-476.3168\"/>\n</g>\n<!-- output -->\n<g id=\"node27\" class=\"node\">\n<title>output</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"329.1886,-52 250.8114,-52 250.8114,-16 329.1886,-16 329.1886,-52\"/>\n<text text-anchor=\"middle\" x=\"290\" y=\"-30.4\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#000000\">Output Text</text>\n</g>\n<!-- ln2&#45;&gt;output -->\n<g id=\"edge27\" class=\"edge\">\n<title>ln2-&gt;output</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M376.6627,-108.7867C357.6058,-92.3413 329.8558,-68.3941 310.9255,-52.0579\"/>\n</g>\n<!-- ln4&#45;&gt;output -->\n<g id=\"edge30\" class=\"edge\">\n<title>ln4-&gt;output</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M204.1398,-108.7867C223.0201,-92.3413 250.5133,-68.3941 269.2682,-52.0579\"/>\n</g>\n</g>\n</svg>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThis code snippet imports the necessary libraries.</div>","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import CTRLConfig, CTRLLMHeadModel, CTRLTokenizer, CTRLForSequenceClassification, AdamW","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:26:57.222912Z","iopub.execute_input":"2023-06-10T22:26:57.223165Z","iopub.status.idle":"2023-06-10T22:27:10.925031Z","shell.execute_reply.started":"2023-06-10T22:26:57.223143Z","shell.execute_reply":"2023-06-10T22:27:10.924111Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nDefining the path to train.csv and train folder</div>","metadata":{}},{"cell_type":"code","source":"train_csv_path = '/kaggle/input/landmark-recognition-2021/train.csv'\ntrain_folder_path = '/kaggle/input/landmark-recognition-2021/train/'\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:10.926956Z","iopub.execute_input":"2023-06-10T22:27:10.927285Z","iopub.status.idle":"2023-06-10T22:27:10.932110Z","shell.execute_reply.started":"2023-06-10T22:27:10.927253Z","shell.execute_reply":"2023-06-10T22:27:10.930778Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe code snippet reads a CSV file (`train_csv_path`) using pandas' `read_csv` function and stores the data in a DataFrame called `train_df`. The DataFrame represents a dataset containing information about images, including their IDs and corresponding image paths.<br>\n\nThe next line modifies the `image_path` column in the `train_df` DataFrame. It concatenates the `train_folder_path` (the base directory for the training images) with the subdirectories based on the ID of each image. The `train_df['id'].str[0]` extracts the first character of the ID, `train_df['id'].str[1]` extracts the second character, `train_df['id'].str[2]` extracts the third character, and `train_df['id']` represents the full ID. These components are concatenated using '/' as separators, and the '.jpg' extension is added at the end to form the complete image path for each image in the dataset.<br></div>\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path)\n# Modify image_path column to match the directory structure\ntrain_df['image_path'] = train_folder_path + train_df['id'].str[0] + '/' + train_df['id'].str[1] + '/' + train_df['id'].str[2] + '/' + train_df['id'] + '.jpg'\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:10.933590Z","iopub.execute_input":"2023-06-10T22:27:10.934240Z","iopub.status.idle":"2023-06-10T22:27:17.572987Z","shell.execute_reply.started":"2023-06-10T22:27:10.934209Z","shell.execute_reply":"2023-06-10T22:27:17.572050Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nPerforming sanity check of the dataframe's image_path column</div>","metadata":{}},{"cell_type":"code","source":"train_df['image_path'][0]","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:17.575755Z","iopub.execute_input":"2023-06-10T22:27:17.576120Z","iopub.status.idle":"2023-06-10T22:27:17.583141Z","shell.execute_reply.started":"2023-06-10T22:27:17.576088Z","shell.execute_reply":"2023-06-10T22:27:17.582104Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/landmark-recognition-2021/train/1/7/6/17660ef415d37059.jpg'"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nSplitting the modified dataframe into train and val data</div>","metadata":{}},{"cell_type":"code","source":"train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:17.584846Z","iopub.execute_input":"2023-06-10T22:27:17.585229Z","iopub.status.idle":"2023-06-10T22:27:18.074033Z","shell.execute_reply.started":"2023-06-10T22:27:17.585194Z","shell.execute_reply":"2023-06-10T22:27:18.073034Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe code snippet defines a series of image transformations using `torchvision.transforms.Compose`. These transformations are applied to each image in order to preprocess them before feeding them into a neural network model for training or inference.<br>\n\nThe transformations specified in `image_transforms` are as follows:<br>\n\n1. `transforms.Resize((224, 224))`: Resizes the input image to a fixed size of 224x224 pixels. This is a common size used in many computer vision models.<br>\n\n2. `transforms.ToTensor()`: Converts the image from PIL Image format to a PyTorch tensor. This allows the image to be processed by PyTorch and passed through neural networks.<br>\n\n3. `transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`: Normalizes the tensor image by subtracting the mean values `[0.485, 0.456, 0.406]` from each channel and dividing by the standard deviation values `[0.229, 0.224, 0.225]`. Normalization helps in standardizing the pixel values across images and improves model performance.<br></div>\n","metadata":{}},{"cell_type":"code","source":"image_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:18.075468Z","iopub.execute_input":"2023-06-10T22:27:18.076683Z","iopub.status.idle":"2023-06-10T22:27:18.082611Z","shell.execute_reply.started":"2023-06-10T22:27:18.076646Z","shell.execute_reply":"2023-06-10T22:27:18.081682Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe code snippet defines a custom dataset class called `LandmarkDataset`. This class inherits from `torch.utils.data.Dataset`, which is a PyTorch base class for creating custom datasets.<br>\n\nThe `LandmarkDataset` class has the following methods:<br>\n\n1. `__init__(self, data, transform=None)`: The initialization method takes two parameters: `data` and `transform`. The `data` parameter represents the dataset, typically a DataFrame containing information about the images. The `transform` parameter is an optional parameter that represents the image transformations to be applied to each image. These transformations are defined earlier using `transforms.Compose`.<br>\n\n2. `__len__(self)`: This method returns the length of the dataset, i.e., the total number of samples in the dataset.<br>\n\n3. `__getitem__(self, index)`: This method retrieves an item from the dataset at the specified `index`. It first retrieves the image path corresponding to the given index from the dataset. Then, it opens the image using `PIL.Image.open` and converts it to the RGB mode using `.convert('RGB')`. This ensures that the image has three channels (red, green, and blue) required by most deep learning models.<br>\n\n   If a transformation is specified (`self.transform is not None`), it applies the transformation to the image using `self.transform(image)`. The transformed image is then returned.<br>\n\n   Finally, the method returns the image as the output.<br></div>\n","metadata":{}},{"cell_type":"code","source":"class LandmarkDataset(torch.utils.data.Dataset):\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        image_path = self.data.iloc[index]['image_path']\n        image = Image.open(image_path).convert('RGB')\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:18.084300Z","iopub.execute_input":"2023-06-10T22:27:18.084687Z","iopub.status.idle":"2023-06-10T22:27:18.093095Z","shell.execute_reply.started":"2023-06-10T22:27:18.084628Z","shell.execute_reply":"2023-06-10T22:27:18.092191Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nCreating train and val dataset using the train, val splits of the data.</div>","metadata":{}},{"cell_type":"code","source":"# Create DataLoader objects for training and validation data\ntrain_dataset = LandmarkDataset(train_data, transform=image_transforms)\nval_dataset = LandmarkDataset(val_data, transform=image_transforms)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:18.094436Z","iopub.execute_input":"2023-06-10T22:27:18.095054Z","iopub.status.idle":"2023-06-10T22:27:18.103347Z","shell.execute_reply.started":"2023-06-10T22:27:18.095022Z","shell.execute_reply":"2023-06-10T22:27:18.102385Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nCreating dataloaders using the train and val dataset</div>","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:18.104885Z","iopub.execute_input":"2023-06-10T22:27:18.105252Z","iopub.status.idle":"2023-06-10T22:27:18.114351Z","shell.execute_reply.started":"2023-06-10T22:27:18.105223Z","shell.execute_reply":"2023-06-10T22:27:18.113459Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nAltering the configuration file of CTRL as per our need.</div>","metadata":{}},{"cell_type":"code","source":"config = CTRLConfig.from_pretrained('ctrl')\nconfig.num_labels = 1  # Number of output labels (landmark or not)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:18.118348Z","iopub.execute_input":"2023-06-10T22:27:18.118672Z","iopub.status.idle":"2023-06-10T22:27:19.289538Z","shell.execute_reply.started":"2023-06-10T22:27:18.118642Z","shell.execute_reply":"2023-06-10T22:27:19.288626Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/635 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8db21c802fe84acabd0fef15c2d6f902"}},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n\nThe code snippet defines a custom model called `CustomCTRLModel` that extends the `CTRLForSequenceClassification` class from the Transformers library.<br>\n\n1. `class CustomCTRLModel(CTRLForSequenceClassification)`: This line defines the `CustomCTRLModel` class that inherits from `CTRLForSequenceClassification`. This allows you to customize and extend the functionality of the base model.<br>\n\n2. `def __init__(self, config)`: This is the constructor method of the custom model. It takes a `config` object as an argument, which contains the configuration parameters for the model. Inside the constructor, the `super().__init__(config)` line calls the constructor of the base class to initialize the model using the provided configuration.<br>\n\n3. `self.image_embedding = nn.Linear(image_feature_size, config.hidden_size)`: This line creates a linear layer (`nn.Linear`) called `image_embedding`. It maps the `image_feature_size` to the `hidden_size` specified in the model's configuration.<br>\n\n4. `self.fusion = nn.Linear(config.hidden_size+1, config.hidden_size)`: This line creates another linear layer called `fusion`. It takes as input the concatenation of the `hidden_size` of the text outputs and the size of the image embeddings plus 1 (to account for the additional dimension introduced by `unsqueeze`). The output size is set to `hidden_size`.<br>\n\n5. `def forward(self, input_ids, inputs_embeds=None, image_embeds=None, **kwargs)`: This method defines the forward pass of the model. It takes input IDs, input embeddings, and image embeddings as arguments. The `**kwargs` parameter allows for additional keyword arguments that may be passed to the base model's forward method.<br>\n\n6. `text_outputs = super().forward(input_ids=input_ids, inputs_embeds=inputs_embeds, **kwargs)`: This line calls the `forward` method of the base class (`CTRLForSequenceClassification`) with the provided arguments. It computes the text outputs of the model.<br>\n\n7. `if image_embeds is not None:`: This conditional statement checks if image embeddings are provided as input.<br>\n\n8. `image_outputs = self.image_embedding(image_embeds)`: If image embeddings are provided, this line passes the image embeddings through the `image_embedding` linear layer to obtain `image_outputs`.<br>\n\n9. `combined_outputs = torch.cat((text_outputs[0], image_outputs.unsqueeze(0)), dim=1)`: This line concatenates the text outputs and the image outputs along the second dimension (`dim=1`). It uses `torch.cat` to concatenate the tensors. The image outputs are unsqueezed to add an additional dimension to match the shape of the text outputs.<br>\n\n10. `fused_outputs = self.fusion(combined_outputs)`: This line passes the concatenated outputs through the `fusion` linear layer to obtain the final fused outputs.<br>\n\n11. `return fused_outputs, text_outputs[1:]`: This line returns the fused outputs and the remaining text outputs (excluding the first element, which is the pooled output) as a tuple.<br>\n\n12. `else: return fused_outputs`: If image embeddings are not provided, this line simply returns the fused outputs.<br>\n\nThis custom model allows for the fusion of text and image features by concatenating their outputs and passing them through a linear layer for further processing.<br></div>","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimage_feature_size = 2048\nclass CustomCTRLModel(CTRLForSequenceClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        self.image_embedding = nn.Linear(image_feature_size, config.hidden_size)\n        self.fusion = nn.Linear(config.hidden_size+1, config.hidden_size)\n\n    def forward(self, input_ids, inputs_embeds=None, image_embeds=None, **kwargs):\n        text_outputs = super().forward(input_ids=input_ids, inputs_embeds=inputs_embeds, **kwargs)\n\n        if image_embeds is not None:\n            image_outputs = self.image_embedding(image_embeds)\n            #rint(text_outputs[0].shape, image_outputs.shape)\n            combined_outputs = torch.cat((text_outputs[0], image_outputs.unsqueeze(0)), dim=1)\n            fused_outputs = self.fusion(combined_outputs)\n            return fused_outputs, text_outputs[1:]\n        else:\n            return fused_outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:19.290879Z","iopub.execute_input":"2023-06-10T22:27:19.291764Z","iopub.status.idle":"2023-06-10T22:27:19.301074Z","shell.execute_reply.started":"2023-06-10T22:27:19.291732Z","shell.execute_reply":"2023-06-10T22:27:19.299872Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nInitialising the tokenizer of CTRL</div>","metadata":{}},{"cell_type":"code","source":"tokenizer = CTRLTokenizer.from_pretrained('ctrl')","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:19.302348Z","iopub.execute_input":"2023-06-10T22:27:19.302850Z","iopub.status.idle":"2023-06-10T22:27:26.497899Z","shell.execute_reply.started":"2023-06-10T22:27:19.302819Z","shell.execute_reply":"2023-06-10T22:27:26.496917Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/4.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"400be606d7f04e5a88c58e9c20449b3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/2.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70aa922b41cc49a7989cf0e883004b08"}},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe  code snippet demonstrates how to utilize multiple GPUs for training by using DataParallel in PyTorch.<br>\n\n1. `model = CustomCTRLModel(config=config)`: This line creates an instance of the `CustomCTRLModel` by passing the configuration object `config` to its constructor. This assumes that you have already defined the model architecture and imported the necessary modules.<br>\n\n2. `if torch.cuda.device_count() > 1:`: This conditional statement checks if there are multiple GPUs available for training. The `torch.cuda.device_count()` function returns the number of available GPUs.<br>\n\n3. `model = DataParallel(model)`: If multiple GPUs are available, this line wraps the model with `DataParallel`. The `DataParallel` class is responsible for distributing the input batches across multiple GPUs and aggregating the gradients during backpropagation.<br>\n\n4. `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`: This line selects the device for training. If a GPU is available, it sets the device to CUDA (`\"cuda\"`), otherwise, it sets it to CPU (`\"cpu\"`).<br>\n\n5. `model.to(device)`: This line moves the model to the selected device. By calling the `to` method on the model and passing the device as an argument, the model's parameters and buffers are transferred to the specified device.<br>\n\nAfter executing this code snippet, the `model` is ready to be trained using either a single GPU or multiple GPUs, depending on the availability in the kaggle kernel.<br></div>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DataParallel\n\n# Assuming you have already created your model\nmodel = CustomCTRLModel(config=config)\n\n# Check if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    # Create DataParallel model\n    model = DataParallel(model)\n\n# Move the model to the GPU(s)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:26.499313Z","iopub.execute_input":"2023-06-10T22:27:26.499887Z","iopub.status.idle":"2023-06-10T22:27:56.787085Z","shell.execute_reply.started":"2023-06-10T22:27:26.499853Z","shell.execute_reply":"2023-06-10T22:27:56.786155Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"CustomCTRLModel(\n  (transformer): CTRLModel(\n    (w): Embedding(246534, 1280)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-47): 48 x EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (ffn): Sequential(\n          (0): Linear(in_features=1280, out_features=8192, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8192, out_features=1280, bias=True)\n        )\n        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (layernorm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=1280, out_features=1, bias=False)\n  (image_embedding): Linear(in_features=2048, out_features=1280, bias=True)\n  (fusion): Linear(in_features=1281, out_features=1280, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nIt creates an instance of the AdamW optimizer by passing the model parameters (`model.parameters()`) and the learning rate (`lr=1e-5`) to its constructor. The `model.parameters()` function returns an iterator over all the trainable parameters of the model.<br>\n\nThe AdamW optimizer is a variant of the Adam optimizer that incorporates weight decay regularization. It is commonly used in deep learning for optimizing neural networks.</div>\n","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-5)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:56.790408Z","iopub.execute_input":"2023-06-10T22:27:56.791352Z","iopub.status.idle":"2023-06-10T22:27:56.813093Z","shell.execute_reply.started":"2023-06-10T22:27:56.791319Z","shell.execute_reply":"2023-06-10T22:27:56.812311Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nChecking the length of train_loader and val_loader to figure out the number of steps that will be needed to complete one epoch as per declared batch_size</div>","metadata":{}},{"cell_type":"code","source":"len(train_loader), len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:56.817037Z","iopub.execute_input":"2023-06-10T22:27:56.819177Z","iopub.status.idle":"2023-06-10T22:27:56.829024Z","shell.execute_reply.started":"2023-06-10T22:27:56.819145Z","shell.execute_reply":"2023-06-10T22:27:56.827895Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(1264376, 316094)"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe code snippet defines an image encoder using ResNet-50. The purpose of the image encoder is to extract image features from input images.\n1. `class ImageEncoder:`: This defines a custom class called `ImageEncoder` for the image encoder.<br>\n\n2. `def __init__(self):`: This is the constructor method of the `ImageEncoder` class. It initializes the ResNet-50 model (`self.resnet`) with pretrained weights and sets its fully connected layer (`self.resnet.fc`) to a `torch.nn.Identity()` layer, effectively removing the fully connected layer.<br>\n\n3. `self.transform = transforms.Compose([...])`: This defines a sequence of image transformations to be applied to the input images. It includes resizing the images to a fixed size of 224x224 pixels and normalizing the image pixels using mean and standard deviation values commonly used for pre-trained models.<br>\n\n4. `def __call__(self, images):`: This is the callable method of the `ImageEncoder` class. It takes input images as input and performs the image encoding process.<br>\n\n5. `images = self.transform(images)`: This applies the defined transformations to the input images.<br>\n\n6. `features = self.resnet(images)`: This passes the preprocessed images through the ResNet-50 model to obtain the image features. The output is a tensor of shape `(batch_size, num_features, 1, 1)`.<br>\n\n7. `return features.squeeze()`: This squeezes the tensor to remove the dimensions of size 1, resulting in a tensor of shape `(batch_size, num_features)`.<br>\n\nFinally, an instance of the `ImageEncoder` class is created as `image_encoder`, which can be used to extract image features by calling it with input images.<br></div>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\n\n# Define the image encoder using ResNet-50\nclass ImageEncoder:\n    def __init__(self):\n        self.resnet = models.resnet50(pretrained=True).to(device)\n        self.resnet.fc = torch.nn.Identity()  # Remove the fully connected layer\n\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __call__(self, images):\n        images = self.transform(images)\n        features = self.resnet(images)\n        return features.squeeze()\n\n# Create an instance of the image encoder\nimage_encoder = ImageEncoder()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:27:56.833493Z","iopub.execute_input":"2023-06-10T22:27:56.834616Z","iopub.status.idle":"2023-06-10T22:27:58.045214Z","shell.execute_reply.started":"2023-06-10T22:27:56.834586Z","shell.execute_reply":"2023-06-10T22:27:58.044244Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 272MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n\nThe code defines a training loop for a model using landmark images and text prompts. The steps that are executed in the code are -<br>\n\n1. `from tqdm import tqdm`: This imports the `tqdm` library, which provides a progress bar for iterations.<br>\n\n2. `num_epochs = 1`: This sets the number of training epochs.<br>\n\n3. `best_val_loss = float('inf')`: This initializes a variable to keep track of the best validation loss.<br>\n\n4. The code enters a loop that iterates over the specified number of epochs.<br>\n\n5. `model.train()`: This sets the model to training mode, enabling gradient computation and parameter updates.<br>\n\n6. `train_loss = 0.0`: This initializes the training loss to 0.<br>\n\n7. The loop iterates over the training data using `train_loader`, which presumably loads batches of images.<br>\n\n8. `images = images.to(device)`: This moves the input images to the appropriate device (e.g., GPU).<br>\n\n9. `input_ids = tokenizer.encode(...)`: This encodes a text prompt, such as \"Which landmark is depicted in this image?\", using a tokenizer. The resulting input_ids tensor is moved to the device.<br>\n\n10. `image_features = image_encoder(images)`: This extracts image features from the input images using the `image_encoder` object, which is an instance of the `ImageEncoder` class defined earlier.<br>\n\n11. `predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]`: This passes the input_ids and image features through the model to obtain landmark predictions. The predictions tensor is extracted for further processing.<br>\n\n12. `targets = torch.ones_like(predictions)`: This creates a tensor of the same shape as predictions, filled with ones. It assumes that all landmarks in the training data are present.<br>\n\n13. `loss = F.binary_cross_entropy_with_logits(predictions, targets)`: This computes the binary cross-entropy loss between the predictions and targets.<br>\n\n14. `optimizer.zero_grad()`: This zeroes the gradients of the model parameters.<br>\n\n15. `loss.backward()`: This performs backpropagation by computing gradients of the loss with respect to the model parameters.<br>\n\n16. `optimizer.step()`: This updates the model parameters based on the computed gradients.<br>\n\n17. The training loss is updated by adding the loss multiplied by the number of images in the batch.<br>\n\n18. After the inner training loop, the training loss is divided by the size of the training dataset to obtain the average training loss.<br>\n\n19. The code then enters the validation phase.<br>\n\n20. `model.eval()`: This sets the model to evaluation mode, disabling gradient computation and parameter updates.<br>\n\n21. `val_loss = 0.0`: This initializes the validation loss to 0.<br>\n\n22. The loop iterates over the validation data using `val_loader`, which presumably loads batches of validation images.<br>\n\n23. Similar to the training loop, the input images are moved to the appropriate device, and image features are extracted using the `image_encoder`.<br>\n\n24. The model is used to make predictions on the input_ids and image features, and the predictions tensor is extracted.<br>\n\n25. `targets = torch.zeros_like(predictions)`: This creates a tensor of the same shape as predictions, filled with zeros. It assumes that no landmarks are present in the validation data.<br>\n\n26. The validation loss is computed using binary cross-entropy with logits.<br>\n\n27. The validation loss is updated by adding the loss multiplied by the number of images in the batch.<br>\n\n28. After the inner validation loop, the validation loss is divided by the size of the validation dataset to obtain the average validation loss.<br>\n\n29. If the current validation loss is lower than the previous best validation loss, the model's state_dict is saved to a file called '\n\nbest_model.pth'. This allows later loading of the model with the best performance.<br>\n\n30. The training and validation loss for the current epoch are printed.<br>\n\n</div>","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn.functional as F\nnum_epochs = 1\nbest_val_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n\n    for step, images in tqdm(enumerate(train_loader)):\n        images = images.to(device)\n\n        # Generate landmark predictions using CTRL model\n        input_ids = tokenizer.encode(\"Which landmark is depicted in this image?\", add_special_tokens=True, return_tensors=\"pt\").to(device)\n        image_features = image_encoder(images)  # Extract image features using a pre-trained CNN\n        image_features = image_features.to(device)\n        input_ids = input_ids.to(device)\n        predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]\n\n        # Compute loss and perform backpropagation\n        targets = torch.ones_like(predictions)  # Assume all landmarks in training data\n        loss = F.binary_cross_entropy_with_logits(predictions, targets)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if step % 1 == 0:\n            print(\"Step-{}, Loss-{}\".format(step, loss.item()))\n            break\n\n        train_loss += loss.item() * images.size(0)\n\n    train_loss /= len(train_loader.dataset)\n    break\n    # Validation\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for images in val_loader:\n            images = images.to(device)\n\n            input_ids = tokenizer.encode(\"Which landmark is depicted in this image?\", add_special_tokens=True, return_tensors=\"pt\").to(device)\n            image_features = image_encoder(images)  # Extract image features using a pre-trained CNN\n            image_features = image_features.to(device)\n            input_ids = input_ids.to(device)\n            predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]\n\n            targets = torch.zeros_like(predictions)\n            loss = F.binary_cross_entropy_with_logits(predictions, targets)\n            val_loss += loss.item() * images.size(0)\n\n    val_loss /= len(val_loader.dataset)\n\n    # Save the model with the best validation loss\n    if val_loss < best_val_loss:\n        torch.save(model.module.state_dict(), 'best_model.pth')  # Save the module's state_dict for DataParallel model\n        best_val_loss = val_loss\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-06-10T22:29:12.585505Z","iopub.execute_input":"2023-06-10T22:29:12.586174Z","iopub.status.idle":"2023-06-10T22:29:13.095265Z","shell.execute_reply.started":"2023-06-10T22:29:12.586143Z","shell.execute_reply":"2023-06-10T22:29:13.093759Z"},"_kg_hide-output":true,"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(predictions, targets)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 15.90 GiB total capacity; 13.93 GiB already allocated; 683.75 MiB free; 14.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 15.90 GiB total capacity; 13.93 GiB already allocated; 683.75 MiB free; 14.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe code is used for generating predictions on the test data using the trained model. The execution of the code can be broken down into the following steps<br>\n\n1. `test_folder_path`: This variable specifies the path to the test folder containing the image files.<br>\n\n2. `test_files = os.listdir(test_folder_path)`: This retrieves the list of image files in the test folder.<br>\n\n3. `predictions_final = []`: This initializes an empty list to store the final predictions.<br>\n\n4. `model.eval()`: This sets the model to evaluation mode.<br>\n\n5. The code enters a loop that iterates over the files in the test folder using `os.walk(test_folder_path)`.<br>\n\n6. Within the loop, the code checks if the file has the '.jpg' extension to ensure it's an image file.<br>\n\n7. `image_path = os.path.join(root, file)`: This creates the full path to the image file.<br>\n\n8. `image = Image.open(image_path).convert('RGB')`: This opens the image file and converts it to RGB mode using the PIL library.<br>\n\n9. `image = image_transforms(image).unsqueeze(0).to(device)`: This applies the image transformations defined earlier (`image_transforms`) to preprocess the image for model input. It then unsqueezes the image tensor to add a batch dimension and moves it to the appropriate device.<br>\n\n10. `image_features = image_encoder(image)`: This extracts image features from the preprocessed image using the `image_encoder` object defined earlier.<br>\n\n11. `input_ids = tokenizer.encode(...)`: This encodes the text prompt, \"Which landmark is depicted in this image?\", using the tokenizer. The resulting input_ids tensor is moved to the device.<br>\n\n12. `batch_size = image.size(0)`: This retrieves the batch size, which is 1 in this case.<br>\n\n13. `input_ids = input_ids.expand(batch_size, -1)`: This expands the input_ids tensor to match the batch size, effectively duplicating the input_ids for each image in the batch.<br>\n\n14. `predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]`: This passes the input_ids and image features through the model to obtain landmark predictions. The predictions tensor is extracted for further processing.<br>\n\n15. `probs = torch.sigmoid(predictions)`: This applies the sigmoid function to the predictions to obtain probabilities.<br>\n\n16. `confidence_scores = probs.tolist()`: This converts the probability tensor to a Python list.<br>\n\n17. `image_id = os.path.splitext(file)[0]`: This retrieves the image ID by removing the file extension from the file name.<br>\n\n18. `prediction = f\"{image_id},\"`: This initializes the prediction string with the image ID.<br>\n\n19. If there are confidence scores available:<br>\n\n    a. `landmark_id = confidence_scores.index(max(confidence_scores))`: This finds the index of the maximum confidence score, representing the predicted landmark ID.<br>\n    \n    b. `confidence_score = max(confidence_scores)`: This retrieves the maximum confidence score.<br>\n    \n    c. `prediction += f\"{landmark_id} {confidence_score}\"`: This appends the predicted landmark ID and confidence score to the prediction string.<br>\n    \n20. `predictions_final.append(prediction)`: This adds the prediction string to the `predictions_final` list.<br>\n\nThe code iterates over all the image files in the test folder, generates predictions using the trained model, and stores the predictions in the `predictions_final` list.<br></div>","metadata":{}},{"cell_type":"code","source":"import os\n\ntest_folder_path = '/kaggle/input/landmark-recognition-2021/test/'\n\n# Get the list of image files in the test folder\ntest_files = os.listdir(test_folder_path)\n\npredictions_final = []\n\n#model.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\nwith torch.no_grad():\n    for root, dirs, files in tqdm(os.walk(test_folder_path)):\n        for file in files:\n            if file.endswith('.jpg'):\n                image_path = os.path.join(root, file)\n                image = Image.open(image_path).convert('RGB')\n                image = image_transforms(image).unsqueeze(0).to(device)\n                # Generate image features using the image encoder\n                image_features = image_encoder(image)\n                # Generate landmark predictions using CTRL model\n                input_ids = tokenizer.encode(\"Which landmark is depicted in this image?\", add_special_tokens=True, return_tensors=\"pt\").to(device)\n                batch_size = image.size(0)\n                input_ids = input_ids.expand(batch_size, -1)  # Expand input_ids to match the batch size\n\n                predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]\n                \n                probs = torch.sigmoid(predictions)\n                confidence_scores = probs.tolist()\n                # Add the prediction to the list\n                image_id = os.path.splitext(file)[0]\n                prediction = f\"{image_id},\"\n                if confidence_scores:\n                    landmark_id = confidence_scores.index(max(confidence_scores))\n                    confidence_score = max(confidence_scores)\n                    prediction += f\"{landmark_id} {confidence_score}\"\n                predictions_final.append(prediction)\n\n\n#intentional keyboard interrupt as the inference time is high!","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-06-10T22:29:14.716015Z","iopub.execute_input":"2023-06-10T22:29:14.716368Z","iopub.status.idle":"2023-06-10T22:29:34.611311Z","shell.execute_reply.started":"2023-06-10T22:29:14.716339Z","shell.execute_reply":"2023-06-10T22:29:34.609608Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"123it [00:19,  6.36it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Expand input_ids to match the batch size\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_features\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(predictions)\n\u001b[1;32m     30\u001b[0m confidence_scores \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39mtolist()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mCustomCTRLModel.forward\u001b[0;34m(self, input_ids, inputs_embeds, image_embeds, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, inputs_embeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, image_embeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m     text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         image_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_embedding(image_embeds)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py:759\u001b[0m, in \u001b[0;36mCTRLForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03m>>> loss.backward()  # doctest: +IGNORE_RESULT\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 759\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    774\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py:473\u001b[0m, in \u001b[0;36mCTRLModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    472\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 473\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m hidden_states, present \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py:188\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, mask, layer_past\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    186\u001b[0m ):\n\u001b[1;32m    187\u001b[0m     normed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(x)\n\u001b[0;32m--> 188\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    200\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py:155\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, v, k, q, mask, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     present \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[0;32m--> 155\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m scaled_attention \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    157\u001b[0m attn \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py:64\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(q, k, v, mask, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscaled_dot_product_attention\u001b[39m(q, k, v, mask, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# calculate attention\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     matmul_qk \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     dk \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m     scaled_attention_logits \u001b[38;5;241m=\u001b[39m matmul_qk \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(dk)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nSanity check of the predictions_final df,Desired output which is ready to be written to submission.csv</div>","metadata":{}},{"cell_type":"code","source":"predictions_final","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-06-10T22:29:56.956369Z","iopub.execute_input":"2023-06-10T22:29:56.957061Z","iopub.status.idle":"2023-06-10T22:29:56.972261Z","shell.execute_reply.started":"2023-06-10T22:29:56.957027Z","shell.execute_reply":"2023-06-10T22:29:56.971075Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['777f9efff0fc6b81,765 0.6193125247955322',\n '7774e44062fbd8bc,765 0.6198825836181641',\n '777173e839e6cfa7,765 0.6180853843688965',\n '7726658184c7e337,765 0.6152055263519287',\n '77be72c73bbf4f18,765 0.6229878067970276',\n '77bfa238b14dc4a7,765 0.616929829120636',\n '77f1028e6d388bd9,765 0.6155384182929993',\n '77f52f956df4b2a9,765 0.6228804588317871',\n '77510cd9d93add55,765 0.6240703463554382',\n '775d4591c8be421c,765 0.6158825755119324',\n '775ab99cf0595c31,765 0.6184576153755188',\n '77e1792a18b5aeae,765 0.6191000938415527',\n '77ed344adb36db5b,765 0.6154272556304932',\n '7705473b53b52915,765 0.6193345785140991',\n '7702b1ef3fa68ab5,765 0.6195984482765198',\n '77a501fd981d7603,765 0.6232295632362366',\n '77360d7b4e0f98a1,765 0.6219176650047302',\n '7718599448535b8b,765 0.6202540397644043',\n '77cc018e16004a62,765 0.620981752872467',\n '77c7a65e60a5605a,765 0.6231173872947693',\n '77cee1023af97e78,765 0.6201375722885132',\n '77ce9939b3d86473,765 0.6147096753120422',\n '774e570002863818,765 0.6152763962745667',\n '774789894a93a956,351 0.6160293817520142',\n '774f7cb4cf0d3b4f,765 0.6173343658447266',\n '779522659a4c78fc,765 0.6176795959472656',\n '779c172ffe03e26b,765 0.6218661665916443',\n '77d828196de9e77c,765 0.6160702109336853',\n '77d13fc049f2b10a,765 0.6127293109893799',\n '77dffb8103273edc,765 0.6213420033454895',\n '77da099fde98cbb7,765 0.6213910579681396',\n '727d8903c4f3d043,765 0.6171595454216003',\n '722ac0e4537483d6,765 0.6216793656349182',\n '722c061cc173a67c,765 0.6207787394523621',\n '7227ec11a97173be,765 0.6191481351852417',\n '72b0e5c2330e89f5,765 0.6210162043571472',\n '72b2226aa8a4a8a3,1198 0.6162593364715576',\n '72fbf82d5627f43a,765 0.6150548458099365',\n '72fcbeb351cab14b,765 0.6162693500518799',\n '72fd8e1377719dc6,765 0.6227283477783203',\n '72f83c10be6c49ba,765 0.6214370131492615',\n '72fbe3e921e097ac,765 0.6248546242713928',\n '72f6252008d2c2c2,765 0.6159253716468811',\n '72f6a7d79e1e5234,765 0.617377519607544',\n '72587447ff73ff9c,765 0.6226481199264526',\n '725a9cc011a4874a,765 0.6201494336128235',\n '72e7836cdbda16e8,765 0.6162976026535034',\n '72ea578921b3f3dd,765 0.6209622621536255',\n '72e3ffdade5c4409,765 0.6195480227470398',\n '7280411143ccaa14,765 0.6234040856361389',\n '728f6089fff434d6,765 0.6163396239280701',\n '728fea878ed130a0,765 0.6232848167419434',\n '720d0595458268f6,765 0.6191614866256714',\n '720db36a22f7b1fa,765 0.618464469909668',\n '72a1ad6acd85df32,765 0.6203708648681641',\n '72a4de4989377625,765 0.6208763122558594',\n '72a913f5f90a5766,765 0.6227323412895203',\n '72355f8512cd796f,765 0.6174321174621582',\n '72352a146563e9b2,765 0.6202152371406555',\n '72349fc5dbc30aa9,765 0.6167715191841125',\n '72126867e8c2955b,765 0.6177012920379639',\n '72c30cd2b86b9743,765 0.620804488658905',\n '72c8e98e0cf253ea,765 0.6213274598121643',\n '72ce63352c33bbf6,765 0.6168859601020813',\n '72c445c32b2f5f42,765 0.6179338693618774',\n '72c1654e2b6a60db,765 0.6188191175460815',\n '72c74d343af3436a,765 0.6177656650543213',\n '724798a1ee311a77,765 0.6172826290130615',\n '7262e47d330c773b,765 0.6207315325737',\n '72d3ff01f5a77c1c,765 0.6198772192001343',\n '72dd9e3d6db81462,765 0.6200145483016968',\n '7b7f326f7404ff72,765 0.6147934794425964',\n '7b756e588e7db527,765 0.6172568202018738',\n '7bbae78350f76821,765 0.6140414476394653',\n '7bbca22b743fb27a,765 0.6181344389915466',\n '7bbfae9f28b9d587,765 0.6181991696357727',\n '7bbd4bff6bef4fee,765 0.6193702220916748',\n '7bb18bdfa24ee3cf,765 0.6162904500961304',\n '7bf17db00570d6b8,765 0.6190495491027832',\n '7bf0c8b5166286ac,765 0.6149231195449829',\n '7bfeb69f97d65052,765 0.6196962594985962',\n '7b5f35d9981256f1,765 0.6208599805831909',\n '7b542a42d705e12d,765 0.6218576431274414',\n '7b5eb9b353495710,765 0.6150587797164917',\n '7b5eed005ec89b2a,765 0.614435613155365',\n '7b5563ca74b3f650,765 0.6208362579345703',\n '7be2dd532779bb79,765 0.6218681335449219',\n '7bef58e0e0c9b221,765 0.6182985901832581',\n '7be828c34512eb0d,765 0.619603157043457',\n '7b8a7e21f7845b05,765 0.620980978012085',\n '7b8d2607b1f99b69,765 0.6147815585136414',\n '7b82455b4b8349c1,765 0.617920458316803',\n '7b05410a37239602,765 0.6238833665847778',\n '7b0435637b61e68d,765 0.6154047250747681',\n '7b05c01f26c0675b,765 0.6192331910133362',\n '7b04659bc6a2f00f,765 0.6237324476242065',\n '7badc9ba50efe482,765 0.6195656657218933',\n '7b33ac3fd7799bc8,765 0.6219006776809692',\n '7b370437f376f23b,765 0.6201158761978149',\n '7b35ef8e4c7ed6de,765 0.6186511516571045',\n '7b10321a6248cb34,765 0.6213585734367371',\n '7bc70823a0ba1422,765 0.6231582164764404',\n '7bcc0b1be8fd1efa,765 0.6206735372543335',\n '7bcf5916a6cea9fd,765 0.6149045825004578',\n '7b995731462f1997,765 0.6185150146484375',\n '7b995d0da0a53eb0,765 0.6166743636131287',\n '7b90b9717bb92ea6,765 0.6135631203651428',\n '7b9fb1e81933c862,765 0.6195369958877563',\n '7b937e8951f10780,351 0.6127777099609375',\n '7b903a5ff9e5de2d,765 0.6165362596511841',\n '7b930e3bbaf5e39f,765 0.617117166519165',\n '7b6c1de0fbb36e8d,765 0.6143457293510437',\n '7b6ea461d8f51e14,765 0.6167893409729004',\n '7b6504322e8bc5fa,765 0.6269910335540771',\n '7b6e4c2b83f76fae,765 0.618840217590332',\n '7bd1d71d00e6e0ad,765 0.6178297400474548',\n '7bd844174af3d811,765 0.6210812926292419',\n '7bd1d33e12c229b1,765 0.6162609457969666',\n '7bdd3aef037a0919,765 0.6203587055206299',\n '7bddef68cf45e68e,765 0.62044757604599',\n '7bdf470b16265da9,765 0.621417224407196',\n '7f700c571efccda8,765 0.6187328696250916',\n '7f248f2bcb5c2a6a,765 0.6175244450569153',\n '7f2dffec99c618e8,765 0.6188188195228577',\n '7fb0a6daaa470e41,765 0.6192857623100281',\n '7fb054b76d508053,765 0.6188962459564209',\n '7fbb3f25a5eeff8f,765 0.6220998764038086',\n '7fb997133bd0626c,765 0.6182714700698853',\n '7fb0f88c39ca7030,765 0.6164987087249756',\n '7f52cf24020bc1e8,765 0.6215877532958984',\n '7f5b07ca3833ff1a,765 0.6201661229133606',\n '7f55b2017153f652,765 0.6134233474731445',\n '7f59ac7644aac013,765 0.6215949654579163',\n '7f5c33de78efe1d6,765 0.6130831241607666',\n '7f58c37739c23e1c,765 0.6211425065994263',\n '7fec495ccd58eb54,765 0.6174665093421936',\n '7fe688a471cbfe03,765 0.6217700839042664',\n '7fe7054b79866c9f,765 0.6173044443130493',\n '7fe1ab9f831a147e,765 0.6228775382041931',\n '7f859754cf55ef36,765 0.6207382082939148',\n '7f07d8da173735b6,765 0.6229767203330994',\n '7f014f54d3958088,765 0.6210314631462097',\n '7f00f9aa4dd64f54,765 0.6205806732177734',\n '7faaecdf80c8bebf,765 0.6156991720199585',\n '7fa25ac172b26771,765 0.6167673468589783',\n '7fa261ce1eb29637,765 0.6172716617584229',\n '7f3d8d3cb011056c,765 0.6170050501823425',\n '7f3ac58d033f240f,765 0.6175886392593384',\n '7f34f39d6194bff0,765 0.6152467727661133',\n '7f1e465f004324d7,765 0.6171106100082397',\n '7f1948289347a2f6,765 0.6172642707824707',\n '7f142ec5ec2cd2ff,765 0.6185718774795532',\n '7fccb93020d4ab7e,765 0.6187842488288879',\n '7fcb824f352e81e9,765 0.6220155954360962',\n '7fc5a88c618bd10d,765 0.6182399392127991',\n '7f48a391cb95afa6,765 0.6187793016433716',\n '7f4530773be993e4,765 0.6132346391677856',\n '7f4b4f4b0e0f12ca,765 0.6154506206512451',\n '7f4b7ae06d343125,765 0.6189332604408264',\n '7f4cab2ebb54a211,765 0.6176607608795166',\n '7f9d4a905c62aa8f,765 0.6153922080993652',\n '7f9fd45e6c8ad2ec,765 0.6130062937736511',\n '7f92a1faff66a405,765 0.615355372428894',\n '7f9514ac0a4d93d0,765 0.6198307871818542',\n '7f658f4f337702d0,765 0.6207913160324097',\n '7f6f4248511eab5c,765 0.6240361928939819',\n '7fd24a1be7ed090f,765 0.6131446957588196',\n '7fd64a5640006086,765 0.6198689937591553',\n '7fdf5b0d3f54f063,765 0.618117094039917',\n '7fd75cca3ef6e9b7,765 0.619997501373291',\n '7575d27768628a70,765 0.6172550320625305',\n '75788e295f380f7b,765 0.6213914155960083',\n '752f5fee89063caa,765 0.6153156757354736',\n '752e29ba0bad99a8,765 0.6206995844841003',\n '7527f1c360b79352,765 0.6135109066963196',\n '75b029fc325c7c62,765 0.6234186291694641',\n '75ba4f9b06596796,765 0.616831362247467',\n '75b6bfe71b5e5972,765 0.6153926253318787',\n '75f479a940e57be4,765 0.6202046871185303',\n '75ffd43a0feb3c87,765 0.6204688549041748',\n '75515c6bb2605617,765 0.6186237335205078',\n '755dfda49c6dc321,765 0.6174999475479126',\n '755f502fe0d305c4,765 0.6154148578643799',\n '75eeb9358a041329,765 0.6171600222587585',\n '75800e3a88861a0c,765 0.6198023557662964',\n '7584fd71148f621b,765 0.6168854236602783',\n '750fe8271ea634ae,765 0.6198761463165283',\n '750ff3a8f5332c57,765 0.6148591041564941',\n '750c98cac2e0a9a0,765 0.6206206679344177',\n '75a4aa6244ce2a96,765 0.6169970631599426',\n '75a1f02afb88123c,765 0.6175000071525574',\n '751f63d9f9d4521c,765 0.6165664196014404',\n '751d4de7d8362494,765 0.6150121092796326',\n '751ad3beff891d69,765 0.6183887124061584',\n '75c98ace1fe6eeea,765 0.6217247843742371',\n '75ce9d5fd0c0175c,765 0.6145139932632446',\n '75cac75b04ad35b5,765 0.61805659532547',\n '75c07a2081c368c7,765 0.6229207515716553',\n '75cddcf06619bd70,765 0.6179448366165161',\n '75c3295aa1e81c3d,765 0.619873583316803',\n '75ce706eca4bce61,765 0.6216116547584534',\n '75424b6d1b857e26,765 0.622248113155365',\n '7545d9e0dfe874de,765 0.6174771785736084',\n '75408d7f6f404895,765 0.6220240592956543',\n '754c447fe00114e8,765 0.6196451783180237',\n '75495acbd4515362,765 0.6191796660423279',\n '75414e6f3230ac7c,765 0.6225264072418213',\n '75479e4ba1f2d75b,765 0.6121750473976135',\n '759fa59e66bfd546,765 0.6183624267578125',\n '7598a475dccd564f,765 0.6167440414428711',\n '7590dc74f444885f,765 0.6199723482131958',\n '7591aee5504b25b2,765 0.6209378242492676',\n '756f64be0519a98f,765 0.6190808415412903',\n '75da057667774fba,765 0.6163782477378845',\n '75d640bd8bdb0535,765 0.6170421838760376',\n '75d52b8d6077e65b,765 0.6202254295349121',\n '75deae4b224e4b41,765 0.6195964813232422',\n '75d447e3b87d6e3b,765 0.6186097264289856',\n '7e751b0bcb3c9ceb,765 0.6198867559432983',\n '7e7c1845d2513c93,765 0.6149460673332214',\n '7e77ce1f29338f90,765 0.6203126907348633',\n '7e2c58dcd3f52d0f,765 0.6160790920257568',\n '7e2ab1f9f7232db8,765 0.6199748516082764',\n '7e2588bbcf1a9cb9,765 0.6181744933128357',\n '7e26bdb075eca439,765 0.6157824397087097',\n '7e2622dafa3864a3,1198 0.6160989999771118',\n '7e2a99054c526055,765 0.6187500357627869',\n '7eb1526cbf75894d,765 0.6195492744445801',\n '7ebba1340aafabd0,765 0.6127974390983582',\n '7ebedd7bdd145ce8,765 0.6169126033782959',\n '7eb08dc01fe4aee8,765 0.6197304129600525',\n '7eb06e73b3baeb3a,765 0.6220870614051819',\n '7ef9c6b9e9e6d8a3,765 0.615125298500061',\n '7eff2a8fda0e0f39,765 0.6183665990829468',\n '7ef8a3d047d8c6af,765 0.6225468516349792',\n '7ef985b82988d32d,765 0.6172675490379333',\n '7eff9f79718dbd1a,765 0.6210529208183289',\n '7e5b5718cc5c8681,765 0.6170253157615662',\n '7ee3c267415ca387,765 0.6222076416015625',\n '7ee74881beb9b761,765 0.621627926826477',\n '7e8a67659e844de1,765 0.6182966232299805',\n '7e003411ae672e4d,765 0.6156666278839111',\n '7e01990efcb3d845,765 0.6155907511711121',\n '7ea94434f6feea6a,765 0.6190070509910583',\n '7eabecb3e41bbf0c,765 0.6213112473487854',\n '7e375aeaa466e0db,765 0.618465006351471',\n '7e3e245ea8304ccb,765 0.6169652938842773',\n '7e32dd38786c35d3,765 0.6187227368354797',\n '7e3cc4ae85c60a0d,765 0.618432343006134',\n '7e39677976f6ed12,765 0.6185852885246277',\n '7e1392ced90bb08e,765 0.6120973229408264',\n '7e12bab9457f9600,765 0.6220014095306396',\n '7e416e9c7e4daaa5,765 0.6198275089263916',\n '7e43e4a5307f08dd,765 0.6180581450462341',\n '7e9ae85d59668bff,765 0.6167615652084351',\n '7e9e56268f041b21,765 0.6211821436882019',\n '7e980cf53023ce79,765 0.6194214820861816',\n '7e9507f3f5aac6c5,765 0.6184412240982056',\n '7e9cfa3412a370c1,765 0.6172714829444885',\n '7e6e52a7f3148171,765 0.6159582138061523',\n '7ed73ac1743f2a70,765 0.6176785230636597',\n '7ed0128b9349b350,765 0.6188568472862244',\n '7edf8d213b5df168,765 0.6143484711647034',\n '7ed31b6cc08f2b37,765 0.6192657351493835',\n '7edba78f0cd29bb2,765 0.6176425218582153',\n '7ed94331dd3dbcce,765 0.6165603399276733',\n '7870f8837232a965,765 0.6172264218330383',\n '787c0a772f5e9f61,765 0.6197999715805054',\n '787065d31cfc2851,765 0.6158580183982849',\n '7879d1c5c62f0b2f,765 0.61885666847229',\n '7822b6f82ecdab86,765 0.6108474731445312',\n '782bcadb414d9960,765 0.6223129630088806',\n '78bdcf3183425cd3,765 0.6219748854637146',\n '78b84335e8cb7d29,1198 0.6155634522438049',\n '78fa0955dfdd88d3,765 0.6237339973449707',\n '78fa0e4f06fb3633,765 0.6157341599464417',\n '7852efabeb32a1ed,765 0.6208546161651611',\n '7850a4289221ffde,765 0.6190792322158813',\n '785598b2e718e0ce,765 0.6199353933334351',\n '78e3e5b1af9aa454,765 0.6220573782920837',\n '78e2dc987950af1a,765 0.616371214389801',\n '788da20a2a9f17e9,765 0.6152622699737549',\n '7885e8fed5c45057,765 0.6165374517440796',\n '788d64cebfa42439,765 0.6178704500198364',\n '78837024f8a172a8,765 0.622219443321228',\n '788752e011dae89d,765 0.618419349193573',\n '788df211d5eacb5f,765 0.6230581402778625',\n '780c2be2c56f8efe,765 0.6164355874061584',\n '780d1df62ae77757,508 0.6139331459999084',\n '780cd9d9f6e57cff,765 0.6198140382766724',\n '78000f386f484539,765 0.6216439604759216',\n '78ae45524b47b5bc,765 0.6188268661499023',\n '783df238732ef4c9,765 0.6173089146614075',\n '78114e6b4199eb4b,765 0.6196876764297485',\n '784d5e23679e1e4e,765 0.6195110082626343',\n '784d59a81f6254da,765 0.6177523732185364',\n '78467ae9cfbea97c,765 0.6179403066635132',\n '784d4e8bab582f2d,765 0.617561399936676',\n '78989cfc92757c51,765 0.6217001080513',\n '7892fe0e61c03ad1,765 0.6192027926445007',\n '786121ce0adccf3b,765 0.6171969771385193',\n '786c55b65edd4d6c,765 0.6173161268234253',\n '78d0b5a41757f57f,765 0.6231951713562012',\n '707e8ad33a63a385,765 0.619914174079895',\n '70bf9e1c5fa9df22,765 0.6141245365142822',\n '70b980d7868483e9,765 0.6201789379119873',\n '70b9098cd6e7e39d,765 0.6205632090568542',\n '70f74b970879dbe8,765 0.619094729423523',\n '7052eb9ca90a8115,765 0.6188287138938904',\n '7054bfbf4efb0b20,765 0.6197640299797058',\n '7084fe8d54e5b1bf,765 0.6230559945106506',\n '70894ea22a1410c6,765 0.6191859245300293',\n '7080593d3f99522a,765 0.6169722676277161',\n '70805442291b5aa7,765 0.6187993288040161',\n '7085299ab64ff53b,765 0.6152200698852539',\n '70099f1c97c64509,765 0.6217799782752991',\n '700322c8aa189468,765 0.619473397731781',\n '70a3195c45fcbf1f,765 0.6161325573921204',\n '70a747f85da30f06,765 0.614741325378418',\n '70aada7cdd588b85,765 0.6179921627044678',\n '70a477a895db679f,765 0.6220710873603821',\n '7035041f92964d25,765 0.6203497052192688',\n '7033ffbaea8ed3ce,765 0.6130565404891968',\n '703fec951fbba070,765 0.6176208257675171',\n '703bcb9ef2479a97,765 0.6159830689430237',\n '7031fb5cd73db1f8,765 0.6177906394004822',\n '701541c18e268c40,765 0.6183792948722839',\n '7017234895c2e3fd,765 0.6146845817565918',\n '70c55291291b55a1,765 0.6218733787536621',\n '7042d6076cd18d31,765 0.6171102523803711',\n '704437761c727f99,765 0.6192898750305176']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}