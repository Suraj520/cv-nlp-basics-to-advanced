{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Architecture Overview\n\n> CTRL(Conditional Transformer Language Model)\n\nIt is a language model architecture designed to generate human like text based on a given control code or prompt. It combines the power of transformers with the ability to condition the model's output based on specific instructions.\n\nThe architecture of CTRL consists of various building blocks which are explained below\n\n1. Input Encoding : The input text is first tokenized. These tokens capture the meaning & structure of the text. Each token is then converted into a high dimensional vector representation.\n\n2. Transformer Encoder - The encoded input tokens are passed through multiple layers of transformer encoders. Each encoder layer consists of two sub layers - A multi headed self attn mechanism and a position wise feedfwd neural network. The self attn mechanism helps the model capture dependencies between diff words in the i/p whereas the feed forward network applies non linear transformations to each position in the sequence.\n\n3. Control code embedding : CTRL introduces an additional control code embedding to allow conditioning the model's output on specific instructions. The control code is appenedd to the input tokens and has its own learned embedding representations. This enables fine grained control over the generated text.\n\n4. Transformer Decoder : The output of the control code embedding is passed through a series of transformer decoder layers. Each decoder layer also consists of self attn and feed forward sub layers. The decoder layers refine the representation of the control code and generate the final output tokens.\n\n5. Output decoding - The decoder's o/p tokens are decoded to generate the final text. The decoding process involves converting the token representations back into text.\n\n\n> Architectural differences w.r.t ELECTRA, ALBERT, BERT, RoBERTa, GPT2, XLNet and T5\n\n- Controlled Generation - CTRL is specifically designed for controlled text generation where a control code or prompt guides the output. It allows users to specify the desired style, topic or other attributes of the generated text.\n\n- Bidirectionality - Unlike models like GPT2 and XLNet, which are unidirectional llms, CTRL is bidirectional i.e It can utilize both the left and right contexts of a token to generate its representations. This allows the model to have a better understanding of the dependencies within the text like Bidirectional LSTM does.\n\n- Control code conditioning - It introduces control code embedding which is a unique component in its architecture. It enables the model to condition its output based on the given control code making it highly flexible for various language generation tasks.\n\n- Diff pretraining objective- UNlike ELECTRA and BERT which employs MLM as their pretraining objective, CTRL employs a new objective called \"relevance ranking\" which involves compairing different parts of the input text to identify the most relevant content.\n\n- Domain adaptation - CTRL supports domain adaptation, allowing fine tuning of the model on specific domains or datasets. This improves the model's performance and adaptability for particular applications.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import CTRLConfig, CTRLLMHeadModel, CTRLTokenizer, CTRLForSequenceClassification, AdamW","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:15.093073Z","iopub.execute_input":"2023-05-25T23:31:15.093495Z","iopub.status.idle":"2023-05-25T23:31:27.978536Z","shell.execute_reply.started":"2023-05-25T23:31:15.093463Z","shell.execute_reply":"2023-05-25T23:31:27.977460Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This code snippet imports the necessary libraries for working with PyTorch, data manipulation with pandas, and natural language processing with the Transformers library.","metadata":{}},{"cell_type":"code","source":"train_csv_path = '/kaggle/input/landmark-recognition-2021/train.csv'\ntrain_folder_path = '/kaggle/input/landmark-recognition-2021/train/'\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:27.980644Z","iopub.execute_input":"2023-05-25T23:31:27.980984Z","iopub.status.idle":"2023-05-25T23:31:27.985587Z","shell.execute_reply.started":"2023-05-25T23:31:27.980956Z","shell.execute_reply":"2023-05-25T23:31:27.984645Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Defining the path to train.csv and train folder","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path)\n# Modify image_path column to match the directory structure\ntrain_df['image_path'] = train_folder_path + train_df['id'].str[0] + '/' + train_df['id'].str[1] + '/' + train_df['id'].str[2] + '/' + train_df['id'] + '.jpg'\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:27.987017Z","iopub.execute_input":"2023-05-25T23:31:27.987345Z","iopub.status.idle":"2023-05-25T23:31:36.195558Z","shell.execute_reply.started":"2023-05-25T23:31:27.987318Z","shell.execute_reply":"2023-05-25T23:31:36.194568Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The code snippet reads a CSV file (`train_csv_path`) using pandas' `read_csv` function and stores the data in a DataFrame called `train_df`. The DataFrame represents a dataset containing information about images, including their IDs and corresponding image paths.\n\nThe next line modifies the `image_path` column in the `train_df` DataFrame. It concatenates the `train_folder_path` (the base directory for the training images) with the subdirectories based on the ID of each image. The `train_df['id'].str[0]` extracts the first character of the ID, `train_df['id'].str[1]` extracts the second character, `train_df['id'].str[2]` extracts the third character, and `train_df['id']` represents the full ID. These components are concatenated using '/' as separators, and the '.jpg' extension is added at the end to form the complete image path for each image in the dataset.\n","metadata":{}},{"cell_type":"code","source":"train_df['image_path'][0]","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:36.198280Z","iopub.execute_input":"2023-05-25T23:31:36.198660Z","iopub.status.idle":"2023-05-25T23:31:36.207845Z","shell.execute_reply.started":"2023-05-25T23:31:36.198630Z","shell.execute_reply":"2023-05-25T23:31:36.206541Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/landmark-recognition-2021/train/1/7/6/17660ef415d37059.jpg'"},"metadata":{}}]},{"cell_type":"markdown","source":"Performing sanity check of the dataframe's image_path column","metadata":{}},{"cell_type":"code","source":"# Split the data into training and validation sets\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:36.209383Z","iopub.execute_input":"2023-05-25T23:31:36.209762Z","iopub.status.idle":"2023-05-25T23:31:36.943475Z","shell.execute_reply.started":"2023-05-25T23:31:36.209732Z","shell.execute_reply":"2023-05-25T23:31:36.942292Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Splitting the modified dataframe into train and val data","metadata":{}},{"cell_type":"code","source":"# Define the image transformations\nimage_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:36.945122Z","iopub.execute_input":"2023-05-25T23:31:36.945533Z","iopub.status.idle":"2023-05-25T23:31:36.952738Z","shell.execute_reply.started":"2023-05-25T23:31:36.945499Z","shell.execute_reply":"2023-05-25T23:31:36.951463Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The code snippet defines a series of image transformations using `torchvision.transforms.Compose`. These transformations are applied to each image in order to preprocess them before feeding them into a neural network model for training or inference.\n\nThe transformations specified in `image_transforms` are as follows:\n\n1. `transforms.Resize((224, 224))`: Resizes the input image to a fixed size of 224x224 pixels. This is a common size used in many computer vision models.\n\n2. `transforms.ToTensor()`: Converts the image from PIL Image format to a PyTorch tensor. This allows the image to be processed by PyTorch and passed through neural networks.\n\n3. `transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`: Normalizes the tensor image by subtracting the mean values `[0.485, 0.456, 0.406]` from each channel and dividing by the standard deviation values `[0.229, 0.224, 0.225]`. Normalization helps in standardizing the pixel values across images and improves model performance.\n","metadata":{}},{"cell_type":"code","source":"# Custom dataset class\nclass LandmarkDataset(torch.utils.data.Dataset):\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        image_path = self.data.iloc[index]['image_path']\n        image = Image.open(image_path).convert('RGB')\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:36.954223Z","iopub.execute_input":"2023-05-25T23:31:36.955331Z","iopub.status.idle":"2023-05-25T23:31:36.965372Z","shell.execute_reply.started":"2023-05-25T23:31:36.955293Z","shell.execute_reply":"2023-05-25T23:31:36.964171Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The code snippet defines a custom dataset class called `LandmarkDataset`. This class inherits from `torch.utils.data.Dataset`, which is a PyTorch base class for creating custom datasets.\n\nThe `LandmarkDataset` class has the following methods:\n\n1. `__init__(self, data, transform=None)`: The initialization method takes two parameters: `data` and `transform`. The `data` parameter represents the dataset, typically a DataFrame containing information about the images. The `transform` parameter is an optional parameter that represents the image transformations to be applied to each image. These transformations are defined earlier using `transforms.Compose`.\n\n2. `__len__(self)`: This method returns the length of the dataset, i.e., the total number of samples in the dataset.\n\n3. `__getitem__(self, index)`: This method retrieves an item from the dataset at the specified `index`. It first retrieves the image path corresponding to the given index from the dataset. Then, it opens the image using `PIL.Image.open` and converts it to the RGB mode using `.convert('RGB')`. This ensures that the image has three channels (red, green, and blue) required by most deep learning models.\n\n   If a transformation is specified (`self.transform is not None`), it applies the transformation to the image using `self.transform(image)`. The transformed image is then returned.\n\n   Finally, the method returns the image as the output.\n","metadata":{}},{"cell_type":"code","source":"# Create DataLoader objects for training and validation data\ntrain_dataset = LandmarkDataset(train_data, transform=image_transforms)\nval_dataset = LandmarkDataset(val_data, transform=image_transforms)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:36.967015Z","iopub.execute_input":"2023-05-25T23:31:36.967378Z","iopub.status.idle":"2023-05-25T23:31:36.981652Z","shell.execute_reply.started":"2023-05-25T23:31:36.967347Z","shell.execute_reply":"2023-05-25T23:31:36.980311Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Creating train and val dataset using the train, val splits of the data.","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:36.983551Z","iopub.execute_input":"2023-05-25T23:31:36.983971Z","iopub.status.idle":"2023-05-25T23:31:36.994695Z","shell.execute_reply.started":"2023-05-25T23:31:36.983939Z","shell.execute_reply":"2023-05-25T23:31:36.993309Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Creating dataloaders using the train and val dataset","metadata":{}},{"cell_type":"code","source":"config = CTRLConfig.from_pretrained('ctrl')\nconfig.num_labels = 1  # Number of output labels (landmark or not)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:36.999282Z","iopub.execute_input":"2023-05-25T23:31:36.999889Z","iopub.status.idle":"2023-05-25T23:31:37.160028Z","shell.execute_reply.started":"2023-05-25T23:31:36.999849Z","shell.execute_reply":"2023-05-25T23:31:37.158716Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/635 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7def5aea3866498d8217b074157261fd"}},"metadata":{}}]},{"cell_type":"markdown","source":"Altering the configuration file of CTRL as per our need.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimage_feature_size = 2048\nclass CustomCTRLModel(CTRLForSequenceClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        self.image_embedding = nn.Linear(image_feature_size, config.hidden_size)\n        self.fusion = nn.Linear(config.hidden_size+1, config.hidden_size)\n\n    def forward(self, input_ids, inputs_embeds=None, image_embeds=None, **kwargs):\n        text_outputs = super().forward(input_ids=input_ids, inputs_embeds=inputs_embeds, **kwargs)\n\n        if image_embeds is not None:\n            image_outputs = self.image_embedding(image_embeds)\n            #rint(text_outputs[0].shape, image_outputs.shape)\n            combined_outputs = torch.cat((text_outputs[0], image_outputs.unsqueeze(0)), dim=1)\n            fused_outputs = self.fusion(combined_outputs)\n            return fused_outputs, text_outputs[1:]\n        else:\n            return fused_outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:37.161797Z","iopub.execute_input":"2023-05-25T23:31:37.162282Z","iopub.status.idle":"2023-05-25T23:31:37.172313Z","shell.execute_reply.started":"2023-05-25T23:31:37.162229Z","shell.execute_reply":"2023-05-25T23:31:37.171141Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The code snippet defines a custom model called `CustomCTRLModel` that extends the `CTRLForSequenceClassification` class from the Transformers library.\n\n1. `class CustomCTRLModel(CTRLForSequenceClassification)`: This line defines the `CustomCTRLModel` class that inherits from `CTRLForSequenceClassification`. This allows you to customize and extend the functionality of the base model.\n\n2. `def __init__(self, config)`: This is the constructor method of the custom model. It takes a `config` object as an argument, which contains the configuration parameters for the model. Inside the constructor, the `super().__init__(config)` line calls the constructor of the base class to initialize the model using the provided configuration.\n\n3. `self.image_embedding = nn.Linear(image_feature_size, config.hidden_size)`: This line creates a linear layer (`nn.Linear`) called `image_embedding`. It maps the `image_feature_size` to the `hidden_size` specified in the model's configuration.\n\n4. `self.fusion = nn.Linear(config.hidden_size+1, config.hidden_size)`: This line creates another linear layer called `fusion`. It takes as input the concatenation of the `hidden_size` of the text outputs and the size of the image embeddings plus 1 (to account for the additional dimension introduced by `unsqueeze`). The output size is set to `hidden_size`.\n\n5. `def forward(self, input_ids, inputs_embeds=None, image_embeds=None, **kwargs)`: This method defines the forward pass of the model. It takes input IDs, input embeddings, and image embeddings as arguments. The `**kwargs` parameter allows for additional keyword arguments that may be passed to the base model's forward method.\n\n6. `text_outputs = super().forward(input_ids=input_ids, inputs_embeds=inputs_embeds, **kwargs)`: This line calls the `forward` method of the base class (`CTRLForSequenceClassification`) with the provided arguments. It computes the text outputs of the model.\n\n7. `if image_embeds is not None:`: This conditional statement checks if image embeddings are provided as input.\n\n8. `image_outputs = self.image_embedding(image_embeds)`: If image embeddings are provided, this line passes the image embeddings through the `image_embedding` linear layer to obtain `image_outputs`.\n\n9. `combined_outputs = torch.cat((text_outputs[0], image_outputs.unsqueeze(0)), dim=1)`: This line concatenates the text outputs and the image outputs along the second dimension (`dim=1`). It uses `torch.cat` to concatenate the tensors. The image outputs are unsqueezed to add an additional dimension to match the shape of the text outputs.\n\n10. `fused_outputs = self.fusion(combined_outputs)`: This line passes the concatenated outputs through the `fusion` linear layer to obtain the final fused outputs.\n\n11. `return fused_outputs, text_outputs[1:]`: This line returns the fused outputs and the remaining text outputs (excluding the first element, which is the pooled output) as a tuple.\n\n12. `else: return fused_outputs`: If image embeddings are not provided, this line simply returns the fused outputs.\n\nThis custom model allows for the fusion of text and image features by concatenating their outputs and passing them through a linear layer for further processing.","metadata":{}},{"cell_type":"code","source":"tokenizer = CTRLTokenizer.from_pretrained('ctrl')","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:37.174331Z","iopub.execute_input":"2023-05-25T23:31:37.174852Z","iopub.status.idle":"2023-05-25T23:31:38.192237Z","shell.execute_reply.started":"2023-05-25T23:31:37.174800Z","shell.execute_reply":"2023-05-25T23:31:38.191106Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/4.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c79079baafca41b2b6ae06e72220505f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/2.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c6e628d46eb451a99a324bc015ee6ab"}},"metadata":{}}]},{"cell_type":"markdown","source":"Initialising the tokenizer of CTRL","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DataParallel\n\n# Assuming you have already created your model\nmodel = CustomCTRLModel(config=config)\n\n# Check if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    # Create DataParallel model\n    model = DataParallel(model)\n\n# Move the model to the GPU(s)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:31:38.195494Z","iopub.execute_input":"2023-05-25T23:31:38.195957Z","iopub.status.idle":"2023-05-25T23:32:10.309233Z","shell.execute_reply.started":"2023-05-25T23:31:38.195916Z","shell.execute_reply":"2023-05-25T23:32:10.307842Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"CustomCTRLModel(\n  (transformer): CTRLModel(\n    (w): Embedding(246534, 1280)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-47): 48 x EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (ffn): Sequential(\n          (0): Linear(in_features=1280, out_features=8192, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8192, out_features=1280, bias=True)\n        )\n        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (layernorm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=1280, out_features=1, bias=False)\n  (image_embedding): Linear(in_features=2048, out_features=1280, bias=True)\n  (fusion): Linear(in_features=1281, out_features=1280, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"The provided code snippet demonstrates how to utilize multiple GPUs for training by using DataParallel in PyTorch.\n\n1. `model = CustomCTRLModel(config=config)`: This line creates an instance of the `CustomCTRLModel` by passing the configuration object `config` to its constructor. This assumes that you have already defined the model architecture and imported the necessary modules.\n\n2. `if torch.cuda.device_count() > 1:`: This conditional statement checks if there are multiple GPUs available for training. The `torch.cuda.device_count()` function returns the number of available GPUs.\n\n3. `model = DataParallel(model)`: If multiple GPUs are available, this line wraps the model with `DataParallel`. The `DataParallel` class is responsible for distributing the input batches across multiple GPUs and aggregating the gradients during backpropagation.\n\n4. `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`: This line selects the device for training. If a GPU is available, it sets the device to CUDA (`\"cuda\"`), otherwise, it sets it to CPU (`\"cpu\"`).\n\n5. `model.to(device)`: This line moves the model to the selected device. By calling the `to` method on the model and passing the device as an argument, the model's parameters and buffers are transferred to the specified device.\n\nAfter executing this code snippet, the `model` is ready to be trained using either a single GPU or multiple GPUs, depending on the availability in the kaggle kernel.","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-5)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:32:10.325720Z","iopub.execute_input":"2023-05-25T23:32:10.326082Z","iopub.status.idle":"2023-05-25T23:32:10.345924Z","shell.execute_reply.started":"2023-05-25T23:32:10.326052Z","shell.execute_reply":"2023-05-25T23:32:10.344815Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"`optimizer = AdamW(model.parameters(), lr=1e-5)`: This line creates an instance of the AdamW optimizer by passing the model parameters (`model.parameters()`) and the learning rate (`lr=1e-5`) to its constructor. The `model.parameters()` function returns an iterator over all the trainable parameters of the model.\n\nThe AdamW optimizer is a variant of the Adam optimizer that incorporates weight decay regularization. It is commonly used in deep learning for optimizing neural networks.\n","metadata":{}},{"cell_type":"code","source":"len(train_loader), len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:32:10.347072Z","iopub.execute_input":"2023-05-25T23:32:10.348100Z","iopub.status.idle":"2023-05-25T23:32:10.357811Z","shell.execute_reply.started":"2023-05-25T23:32:10.348065Z","shell.execute_reply":"2023-05-25T23:32:10.356688Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(1264376, 316094)"},"metadata":{}}]},{"cell_type":"markdown","source":"Checking the length of train_loader and val_loader to figure out the number of steps that will be needed to complete one epoch as per declared batch_size","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\n\n# Define the image encoder using ResNet-50\nclass ImageEncoder:\n    def __init__(self):\n        self.resnet = models.resnet50(pretrained=True).to(device)\n        self.resnet.fc = torch.nn.Identity()  # Remove the fully connected layer\n\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __call__(self, images):\n        images = self.transform(images)\n        features = self.resnet(images)\n        return features.squeeze()\n\n# Create an instance of the image encoder\nimage_encoder = ImageEncoder()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:32:10.359303Z","iopub.execute_input":"2023-05-25T23:32:10.359775Z","iopub.status.idle":"2023-05-25T23:32:11.004480Z","shell.execute_reply.started":"2023-05-25T23:32:10.359734Z","shell.execute_reply":"2023-05-25T23:32:11.003418Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code snippet defines an image encoder using ResNet-50. The purpose of the image encoder is to extract image features from input images.\n\n\n\n1. `class ImageEncoder:`: This defines a custom class called `ImageEncoder` for the image encoder.\n\n2. `def __init__(self):`: This is the constructor method of the `ImageEncoder` class. It initializes the ResNet-50 model (`self.resnet`) with pretrained weights and sets its fully connected layer (`self.resnet.fc`) to a `torch.nn.Identity()` layer, effectively removing the fully connected layer.\n\n3. `self.transform = transforms.Compose([...])`: This defines a sequence of image transformations to be applied to the input images. It includes resizing the images to a fixed size of 224x224 pixels and normalizing the image pixels using mean and standard deviation values commonly used for pre-trained models.\n\n4. `def __call__(self, images):`: This is the callable method of the `ImageEncoder` class. It takes input images as input and performs the image encoding process.\n\n5. `images = self.transform(images)`: This applies the defined transformations to the input images.\n\n6. `features = self.resnet(images)`: This passes the preprocessed images through the ResNet-50 model to obtain the image features. The output is a tensor of shape `(batch_size, num_features, 1, 1)`.\n\n7. `return features.squeeze()`: This squeezes the tensor to remove the dimensions of size 1, resulting in a tensor of shape `(batch_size, num_features)`.\n\nFinally, an instance of the `ImageEncoder` class is created as `image_encoder`, which can be used to extract image features by calling it with input images.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn.functional as F\nnum_epochs = 1\nbest_val_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n\n    for step, images in tqdm(enumerate(train_loader)):\n        images = images.to(device)\n\n        # Generate landmark predictions using CTRL model\n        input_ids = tokenizer.encode(\"Which landmark is depicted in this image?\", add_special_tokens=True, return_tensors=\"pt\").to(device)\n        image_features = image_encoder(images)  # Extract image features using a pre-trained CNN\n        image_features = image_features.to(device)\n        input_ids = input_ids.to(device)\n        predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]\n\n        # Compute loss and perform backpropagation\n        targets = torch.ones_like(predictions)  # Assume all landmarks in training data\n        loss = F.binary_cross_entropy_with_logits(predictions, targets)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if step % 1 == 0:\n            print(\"Step-{}, Loss-{}\".format(step, loss.item()))\n            break\n\n        train_loss += loss.item() * images.size(0)\n\n    train_loss /= len(train_loader.dataset)\n    break\n    # Validation\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for images in val_loader:\n            images = images.to(device)\n\n            input_ids = tokenizer.encode(\"Which landmark is depicted in this image?\", add_special_tokens=True, return_tensors=\"pt\").to(device)\n            image_features = image_encoder(images)  # Extract image features using a pre-trained CNN\n            image_features = image_features.to(device)\n            input_ids = input_ids.to(device)\n            predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]\n\n            targets = torch.zeros_like(predictions)\n            loss = F.binary_cross_entropy_with_logits(predictions, targets)\n            val_loss += loss.item() * images.size(0)\n\n    val_loss /= len(val_loader.dataset)\n\n    # Save the model with the best validation loss\n    if val_loss < best_val_loss:\n        torch.save(model.module.state_dict(), 'best_model.pth')  # Save the module's state_dict for DataParallel model\n        best_val_loss = val_loss\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:44:27.948861Z","iopub.execute_input":"2023-05-25T23:44:27.949248Z","iopub.status.idle":"2023-05-25T23:44:38.928220Z","shell.execute_reply.started":"2023-05-25T23:44:27.949218Z","shell.execute_reply":"2023-05-25T23:44:38.927023Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"0it [00:10, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Step-0, Loss-0.715984046459198\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code snippet provided is an example of a training loop for a model using landmark images and text prompts. The steps that are executed in the code are -\n\n1. `from tqdm import tqdm`: This imports the `tqdm` library, which provides a progress bar for iterations.\n\n2. `num_epochs = 1`: This sets the number of training epochs.\n\n3. `best_val_loss = float('inf')`: This initializes a variable to keep track of the best validation loss.\n\n4. The code enters a loop that iterates over the specified number of epochs.\n\n5. `model.train()`: This sets the model to training mode, enabling gradient computation and parameter updates.\n\n6. `train_loss = 0.0`: This initializes the training loss to 0.\n\n7. The loop iterates over the training data using `train_loader`, which presumably loads batches of images.\n\n8. `images = images.to(device)`: This moves the input images to the appropriate device (e.g., GPU).\n\n9. `input_ids = tokenizer.encode(...)`: This encodes a text prompt, such as \"Which landmark is depicted in this image?\", using a tokenizer. The resulting input_ids tensor is moved to the device.\n\n10. `image_features = image_encoder(images)`: This extracts image features from the input images using the `image_encoder` object, which is an instance of the `ImageEncoder` class defined earlier.\n\n11. `predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]`: This passes the input_ids and image features through the model to obtain landmark predictions. The predictions tensor is extracted for further processing.\n\n12. `targets = torch.ones_like(predictions)`: This creates a tensor of the same shape as predictions, filled with ones. It assumes that all landmarks in the training data are present.\n\n13. `loss = F.binary_cross_entropy_with_logits(predictions, targets)`: This computes the binary cross-entropy loss between the predictions and targets.\n\n14. `optimizer.zero_grad()`: This zeroes the gradients of the model parameters.\n\n15. `loss.backward()`: This performs backpropagation by computing gradients of the loss with respect to the model parameters.\n\n16. `optimizer.step()`: This updates the model parameters based on the computed gradients.\n\n17. The training loss is updated by adding the loss multiplied by the number of images in the batch.\n\n18. After the inner training loop, the training loss is divided by the size of the training dataset to obtain the average training loss.\n\n19. The code then enters the validation phase.\n\n20. `model.eval()`: This sets the model to evaluation mode, disabling gradient computation and parameter updates.\n\n21. `val_loss = 0.0`: This initializes the validation loss to 0.\n\n22. The loop iterates over the validation data using `val_loader`, which presumably loads batches of validation images.\n\n23. Similar to the training loop, the input images are moved to the appropriate device, and image features are extracted using the `image_encoder`.\n\n24. The model is used to make predictions on the input_ids and image features, and the predictions tensor is extracted.\n\n25. `targets = torch.zeros_like(predictions)`: This creates a tensor of the same shape as predictions, filled with zeros. It assumes that no landmarks are present in the validation data.\n\n26. The validation loss is computed using binary cross-entropy with logits.\n\n27. The validation loss is updated by adding the loss multiplied by the number of images in the batch.\n\n28. After the inner validation loop, the validation loss is divided by the size of the validation dataset to obtain the average validation loss.\n\n29. If the current validation loss is lower than the previous best validation loss, the model's state_dict is saved to a file called '\n\nbest_model.pth'. This allows later loading of the model with the best performance.\n\n30. The training and validation loss for the current epoch are printed.\n\nThe code snippet provided represents one epoch of training and validation. It may require multiple epochs to achieve good performance.","metadata":{}},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import os\n\ntest_folder_path = '/kaggle/input/landmark-recognition-2021/test/'\n\n# Get the list of image files in the test folder\ntest_files = os.listdir(test_folder_path)\n\npredictions_final = []\n\n#model.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\nwith torch.no_grad():\n    for root, dirs, files in tqdm(os.walk(test_folder_path)):\n        for file in files:\n            if file.endswith('.jpg'):\n                image_path = os.path.join(root, file)\n                image = Image.open(image_path).convert('RGB')\n                image = image_transforms(image).unsqueeze(0).to(device)\n                # Generate image features using the image encoder\n                image_features = image_encoder(image)\n                # Generate landmark predictions using CTRL model\n                input_ids = tokenizer.encode(\"Which landmark is depicted in this image?\", add_special_tokens=True, return_tensors=\"pt\").to(device)\n                batch_size = image.size(0)\n                input_ids = input_ids.expand(batch_size, -1)  # Expand input_ids to match the batch size\n\n                predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]\n                \n                probs = torch.sigmoid(predictions)\n                confidence_scores = probs.tolist()\n                # Add the prediction to the list\n                image_id = os.path.splitext(file)[0]\n                prediction = f\"{image_id},\"\n                if confidence_scores:\n                    landmark_id = confidence_scores.index(max(confidence_scores))\n                    confidence_score = max(confidence_scores)\n                    prediction += f\"{landmark_id} {confidence_score}\"\n                predictions_final.append(prediction)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:51:39.432023Z","iopub.execute_input":"2023-05-25T23:51:39.432507Z","iopub.status.idle":"2023-05-25T23:51:44.591301Z","shell.execute_reply.started":"2023-05-25T23:51:39.432471Z","shell.execute_reply":"2023-05-25T23:51:44.589229Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"5it [00:04,  1.08it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m image_transforms(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Generate image features using the image encoder\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate landmark predictions using CTRL model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich landmark is depicted in this image?\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n","Cell \u001b[0;32mIn[18], line 19\u001b[0m, in \u001b[0;36mImageEncoder.__call__\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[1;32m     18\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(images)\n\u001b[0;32m---> 19\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39msqueeze()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"The code snippet provided is for generating predictions on the test data using the trained model. The execution of the code can be broken down into the following steps\n\n1. `test_folder_path`: This variable specifies the path to the test folder containing the image files.\n\n2. `test_files = os.listdir(test_folder_path)`: This retrieves the list of image files in the test folder.\n\n3. `predictions_final = []`: This initializes an empty list to store the final predictions.\n\n4. `model.eval()`: This sets the model to evaluation mode.\n\n5. The code enters a loop that iterates over the files in the test folder using `os.walk(test_folder_path)`.\n\n6. Within the loop, the code checks if the file has the '.jpg' extension to ensure it's an image file.\n\n7. `image_path = os.path.join(root, file)`: This creates the full path to the image file.\n\n8. `image = Image.open(image_path).convert('RGB')`: This opens the image file and converts it to RGB mode using the PIL library.\n\n9. `image = image_transforms(image).unsqueeze(0).to(device)`: This applies the image transformations defined earlier (`image_transforms`) to preprocess the image for model input. It then unsqueezes the image tensor to add a batch dimension and moves it to the appropriate device.\n\n10. `image_features = image_encoder(image)`: This extracts image features from the preprocessed image using the `image_encoder` object defined earlier.\n\n11. `input_ids = tokenizer.encode(...)`: This encodes the text prompt, \"Which landmark is depicted in this image?\", using the tokenizer. The resulting input_ids tensor is moved to the device.\n\n12. `batch_size = image.size(0)`: This retrieves the batch size, which is 1 in this case.\n\n13. `input_ids = input_ids.expand(batch_size, -1)`: This expands the input_ids tensor to match the batch size, effectively duplicating the input_ids for each image in the batch.\n\n14. `predictions = model(input_ids=input_ids, image_embeds=image_features)[0][0]`: This passes the input_ids and image features through the model to obtain landmark predictions. The predictions tensor is extracted for further processing.\n\n15. `probs = torch.sigmoid(predictions)`: This applies the sigmoid function to the predictions to obtain probabilities.\n\n16. `confidence_scores = probs.tolist()`: This converts the probability tensor to a Python list.\n\n17. `image_id = os.path.splitext(file)[0]`: This retrieves the image ID by removing the file extension from the file name.\n\n18. `prediction = f\"{image_id},\"`: This initializes the prediction string with the image ID.\n\n19. If there are confidence scores available:\n\n    a. `landmark_id = confidence_scores.index(max(confidence_scores))`: This finds the index of the maximum confidence score, representing the predicted landmark ID.\n    \n    b. `confidence_score = max(confidence_scores)`: This retrieves the maximum confidence score.\n    \n    c. `prediction += f\"{landmark_id} {confidence_score}\"`: This appends the predicted landmark ID and confidence score to the prediction string.\n    \n20. `predictions_final.append(prediction)`: This adds the prediction string to the `predictions_final` list.\n\nThe code iterates over all the image files in the test folder, generates predictions using the trained model, and stores the predictions in the `predictions_final` list.","metadata":{}},{"cell_type":"code","source":"predictions_final","metadata":{"execution":{"iopub.status.busy":"2023-05-25T23:51:51.893678Z","iopub.execute_input":"2023-05-25T23:51:51.894466Z","iopub.status.idle":"2023-05-25T23:51:51.902175Z","shell.execute_reply.started":"2023-05-25T23:51:51.894428Z","shell.execute_reply":"2023-05-25T23:51:51.900910Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"['777f9efff0fc6b81,689 0.610384464263916',\n '7774e44062fbd8bc,689 0.6035844087600708',\n '777173e839e6cfa7,689 0.6068004369735718',\n '7726658184c7e337,689 0.6036490797996521',\n '77be72c73bbf4f18,867 0.6081327795982361']"},"metadata":{}}]},{"cell_type":"markdown","source":"Desired output which is ready to be written to submission.csv","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}