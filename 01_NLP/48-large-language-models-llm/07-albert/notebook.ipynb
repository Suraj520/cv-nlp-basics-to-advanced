{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Architectural Overview\n\n> ALBERT\n\nALBERT stands for \"A Lite BERT\", is a variant of the BERT(Bidirectional Encoder Representations from Transformers) model. It is designed to be more efficient and scalable while maintaining similar or even improved performance compared to BERT.\n\n> The architecture of ALBERT can be fragmented in the following steps\n\n1. Embeddings layer - Like other transformer based models, ALBERT starts with an embeddings layer. This layer maps input tokens to continuous vector representations called word embeddings. These embeddings capture the semantic meaning of the words and their contextual info.\n\n2. Transformer Encoder - It is the core building block of ALBERT. It consists of multiple stacked layers, and each layer has two sub layers; the self attention mechanism and a feed forward neural network.\n\n- Self attn mechanism - It allows each word in the input seq. to attend to other words in the same seq, capturing their relationships and dependencies. It calculates attention scores for each word and combines information from different positions.\n\n- Feed forward neural network - The feed forward neural network applies a non linear transformation to the outputs of the self attn mechanism, enhancing the representation of each word. It consists of two layers with a non linear activation function in between.\n\n3. Parameter sharing - ALBERT introduces parameter sharing. In BERT, all layers are unique which leads to a large number of parameters. In ALBERT, the layers are grouped and share parameters within each group. This reduces the model's overall size and makes it more memory efficient.\n\n4. Cross-Layer Parameter Sharing: ALBERT goes a step further by introducing cross layer parameter sharing. In addition to sharing params with a layer group, ALBERT shares parameters across different layers. This helps to further reduce the number of params and improves param efficiency.\n\n5. Sentence order prediction - ALBERT introduces a pre-training task called \"sentence order prediction\" In this task, input sequences are split into segments, and the model learns to predict the correct order of the segments. This additional objective helps ALBERT to better understand the relationships between sentences.\n\n> Architectural differences\n\n- ALBERT vs. BERT\nALBERT improves upon BERT by introducing parameter sharing, both within layer groups and across layers. This reduces the model size and improves efficiency without sacrificing performance.\n\n- ALBERT vs. ROBERTa:\nROBERTa is a variant of BERT that focuses on pre-training with larger batch sizes and more data. While both ALBERT and ROBERTa achieve similar performance, ALBERT is more param efficient due to its parameter sharing techniques.\n\n- ALBERT vs. XLNET:\nXLNet is a model that incorporates permutation based training, allowing it to capture dependencies between all positions in a sequence. ALBERT, on the other hand, uses a masked langugage modelling object and parameter sharing techniques. The main difference lies in the training objectives and modelling of depends.\n\n- ALBERT vs. T5:\nT5(Text to Text Transfer Transformer) is a versatile model that can be applied to various NLP tasks by casting them into a text to text format. ALBERT, like BERT, focuses on masked language modelling. T5 is more flexible in handling diff. tasks whiles ALBERT is specifally designed for language representation learning(i.e towards understanding the structure and meaning of language).","metadata":{}},{"cell_type":"code","source":"#importing modules\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import AlbertTokenizer, AlbertForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport csv\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:11.145105Z","iopub.execute_input":"2023-05-25T18:08:11.145456Z","iopub.status.idle":"2023-05-25T18:08:21.785677Z","shell.execute_reply.started":"2023-05-25T18:08:11.145430Z","shell.execute_reply":"2023-05-25T18:08:21.784630Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this snippet, the necessary modules and libraries are imported, including PyTorch, the ALBERT model and tokenizer from the transformers library, DataLoader for creating data loaders, Dataset for creating custom datasets, and pandas for reading and manipulating data.\n\n","metadata":{}},{"cell_type":"code","source":"# Load train.csv and test.csv using pandas\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:21.787520Z","iopub.execute_input":"2023-05-25T18:08:21.787852Z","iopub.status.idle":"2023-05-25T18:08:21.856706Z","shell.execute_reply.started":"2023-05-25T18:08:21.787827Z","shell.execute_reply":"2023-05-25T18:08:21.855819Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The training and testing data files, \"train.csv\" and \"test.csv,\" are loaded using pandas.\n\n","metadata":{}},{"cell_type":"code","source":"# Extract the text and target columns from the train and test data\ntrain_texts = train_df['text'].tolist()\ntrain_labels = train_df['target'].tolist()\ntest_texts = test_df['text'].tolist()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:21.859840Z","iopub.execute_input":"2023-05-25T18:08:21.861671Z","iopub.status.idle":"2023-05-25T18:08:21.868887Z","shell.execute_reply.started":"2023-05-25T18:08:21.861643Z","shell.execute_reply":"2023-05-25T18:08:21.868027Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The \"text\" and \"target\" columns are extracted from the training and testing data and stored as lists.\n\n","metadata":{}},{"cell_type":"code","source":"# Create train_data and test_data dictionaries\ntrain_data = [{'text': text, 'target': label} for text, label in zip(train_texts, train_labels)]\ntest_data = [{'text': text} for text in test_texts]","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:21.872807Z","iopub.execute_input":"2023-05-25T18:08:21.873102Z","iopub.status.idle":"2023-05-25T18:08:21.885416Z","shell.execute_reply.started":"2023-05-25T18:08:21.873076Z","shell.execute_reply":"2023-05-25T18:08:21.884577Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The training and testing data are transformed into dictionaries, where each dictionary entry contains the text and target (if available) for each data instance.\n\n","metadata":{}},{"cell_type":"code","source":"# Set up device (GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:21.888757Z","iopub.execute_input":"2023-05-25T18:08:21.889026Z","iopub.status.idle":"2023-05-25T18:08:21.918261Z","shell.execute_reply.started":"2023-05-25T18:08:21.888984Z","shell.execute_reply":"2023-05-25T18:08:21.917383Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"The code sets the device to use GPU if available; otherwise, it uses the CPU.\n\n","metadata":{}},{"cell_type":"code","source":"# Preprocess the data (assuming you have train_data and test_data)\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:21.921767Z","iopub.execute_input":"2023-05-25T18:08:21.922052Z","iopub.status.idle":"2023-05-25T18:08:22.728473Z","shell.execute_reply.started":"2023-05-25T18:08:21.922027Z","shell.execute_reply":"2023-05-25T18:08:22.727605Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa2b0c2609d4f579cec0e43f818e779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0dcedfc41c8463bb6b44f5726048fba"}},"metadata":{}}]},{"cell_type":"markdown","source":"The ALBERT tokenizer is instantiated using the \"albert-base-v2\" pre-trained model.\n\n","metadata":{}},{"cell_type":"code","source":"#custom dataset class\nclass DisasterTweetsDataset(Dataset):\n    def __init__(self, data, mode=\"train\"):\n        self.data = data\n        self.mode=mode\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        tweet = self.data[index]['text']\n        if self.mode == \"train\":\n            label = self.data[index]['target']\n            encoding = tokenizer.encode_plus(tweet, add_special_tokens=True, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n            input_ids = encoding['input_ids'].squeeze()\n            attention_mask = encoding['attention_mask'].squeeze()\n            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}\n        else:\n            encoding = tokenizer.encode_plus(tweet, add_special_tokens=True, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n            input_ids = encoding['input_ids'].squeeze()\n            attention_mask = encoding['attention_mask'].squeeze()\n            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:27:33.843307Z","iopub.execute_input":"2023-05-25T18:27:33.843734Z","iopub.status.idle":"2023-05-25T18:27:33.858203Z","shell.execute_reply.started":"2023-05-25T18:27:33.843701Z","shell.execute_reply":"2023-05-25T18:27:33.857260Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"A custom dataset class, DisasterTweetsDataset, is defined. It takes the data as input and implements the `__len__` and `__getitem__` methods required for a dataset class. The `__getitem__` method performs tokenization using the ALBERT tokenizer and returns the input IDs, attention mask, and label (if available) for a specific index.\n\n","metadata":{}},{"cell_type":"code","source":"train_dataset = DisasterTweetsDataset(train_data)\ntest_dataset = DisasterTweetsDataset(test_data,mode='test')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:27:35.719978Z","iopub.execute_input":"2023-05-25T18:27:35.720343Z","iopub.status.idle":"2023-05-25T18:27:35.725205Z","shell.execute_reply.started":"2023-05-25T18:27:35.720311Z","shell.execute_reply":"2023-05-25T18:27:35.724298Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=16*4, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=16*4, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:27:36.342330Z","iopub.execute_input":"2023-05-25T18:27:36.342691Z","iopub.status.idle":"2023-05-25T18:27:36.347862Z","shell.execute_reply.started":"2023-05-25T18:27:36.342663Z","shell.execute_reply":"2023-05-25T18:27:36.346711Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"The training and testing datasets are created using the custom dataset class, and dataloaders are initialized for both datasets. The train_dataloader shuffles the data during training, while the test_dataloader does not shuffle the data during testing.\n\n","metadata":{}},{"cell_type":"code","source":"# Define the ALBERT model\nmodel = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:22.765622Z","iopub.execute_input":"2023-05-25T18:08:22.765886Z","iopub.status.idle":"2023-05-25T18:08:28.746971Z","shell.execute_reply.started":"2023-05-25T18:08:22.765863Z","shell.execute_reply":"2023-05-25T18:08:28.746048Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"433bd8f7300b411c908b10ac61db1bc1"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias']\n- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The ALBERT model for sequence classification is instantiated using the \"albert-base-v2\" pre-trained model. The number of labels is set to 2 (binary classification), and the model is moved to the specified device.\n\n","metadata":{}},{"cell_type":"code","source":"# Define the optimizer and loss function\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:28.748579Z","iopub.execute_input":"2023-05-25T18:08:28.748937Z","iopub.status.idle":"2023-05-25T18:08:28.754150Z","shell.execute_reply.started":"2023-05-25T18:08:28.748903Z","shell.execute_reply":"2023-05-25T18:08:28.753055Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The optimizer (AdamW) is defined, which will update the model parameters during training. The learning rate is set to 2e-5. The loss function (CrossEntropyLoss) is also defined.\n\n","metadata":{}},{"cell_type":"code","source":"len(train_dataloader),len(test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:08:28.755637Z","iopub.execute_input":"2023-05-25T18:08:28.756362Z","iopub.status.idle":"2023-05-25T18:08:28.769135Z","shell.execute_reply.started":"2023-05-25T18:08:28.756330Z","shell.execute_reply":"2023-05-25T18:08:28.768122Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(119, 51)"},"metadata":{}}]},{"cell_type":"markdown","source":"Length of dataloader is checked to estimate the steps needed per epoch.","metadata":{}},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Training loop\nnum_epochs = 10\nbest_loss = float('inf')\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for step,batch in tqdm(enumerate(train_dataloader)):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    epoch_loss = running_loss / len(train_dataloader)\n    print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n\n    # Save the best model based on the lowest loss achieved during training\n    if epoch_loss < best_loss:\n        best_loss = epoch_loss\n        torch.save(model.state_dict(), 'best_model.pt')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:29:52.906253Z","iopub.execute_input":"2023-05-25T19:29:52.906635Z","iopub.status.idle":"2023-05-25T19:43:14.351062Z","shell.execute_reply.started":"2023-05-25T19:29:52.906607Z","shell.execute_reply":"2023-05-25T19:43:14.349604Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"119it [01:35,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 - Loss: 0.0304\n","output_type":"stream"},{"name":"stderr","text":"119it [01:35,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 - Loss: 0.0339\n","output_type":"stream"},{"name":"stderr","text":"119it [01:35,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 - Loss: 0.0148\n","output_type":"stream"},{"name":"stderr","text":"119it [01:35,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 - Loss: 0.0313\n","output_type":"stream"},{"name":"stderr","text":"119it [01:35,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 - Loss: 0.0472\n","output_type":"stream"},{"name":"stderr","text":"119it [01:35,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 - Loss: 0.0245\n","output_type":"stream"},{"name":"stderr","text":"119it [01:35,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 - Loss: 0.0163\n","output_type":"stream"},{"name":"stderr","text":"119it [01:35,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 - Loss: 0.0168\n","output_type":"stream"},{"name":"stderr","text":"44it [00:36,  1.22it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"The training loop iterates over the specified number of epochs. Within each epoch, the model is set to train mode, and the running loss is initialized. The loop then iterates through the training dataloader in batches. The input IDs, attention mask, and labels are moved to the specified device. The optimizer gradients are zeroed, and the model is called with the inputs and labels. The loss is calculated, and the gradients are backpropagated and updated using the optimizer. The running loss is updated. After each epoch, the epoch loss is calculated and printed.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"#loading the best trained model\nmodel.load_state_dict(torch.load('best_model.pt'))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:43:18.133052Z","iopub.execute_input":"2023-05-25T19:43:18.134052Z","iopub.status.idle":"2023-05-25T19:43:18.167535Z","shell.execute_reply.started":"2023-05-25T19:43:18.134006Z","shell.execute_reply":"2023-05-25T19:43:18.166474Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model and generate submission file\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        batch_predictions = torch.argmax(logits, dim=1)\n        predictions.extend(batch_predictions.cpu().tolist())\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:43:19.333943Z","iopub.execute_input":"2023-05-25T19:43:19.334318Z","iopub.status.idle":"2023-05-25T19:43:34.939021Z","shell.execute_reply.started":"2023-05-25T19:43:19.334289Z","shell.execute_reply":"2023-05-25T19:43:34.938048Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"The model is set to evaluation mode after loading the best checkpoint, and predictions are generated for the test dataset. The input IDs and attention mask are moved to the specified device, and the model is called with the inputs. The logits are obtained, and the predictions are extracted by taking the argmax along the second dimension. The predictions are extended to the predictions list.\n\n","metadata":{}},{"cell_type":"code","source":"# Generate submission.csv\nsubmission_file = 'submission.csv'\n\nwith open(submission_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['id', 'target'])\n\n    for i, prediction in enumerate(predictions):\n        id_value = test_df['id'].iloc[i]  \n        writer.writerow([id_value, prediction])\n\nprint(f'Submission file \"{submission_file}\" generated successfully.')","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:43:34.941053Z","iopub.execute_input":"2023-05-25T19:43:34.941475Z","iopub.status.idle":"2023-05-25T19:43:35.007402Z","shell.execute_reply.started":"2023-05-25T19:43:34.941443Z","shell.execute_reply":"2023-05-25T19:43:35.006350Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Submission file \"submission.csv\" generated successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A submission file named \"submission.csv\" is created. The file is opened in write mode, and a CSV writer is created. The header row is written, and the\n\n\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('submission.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:43:35.008739Z","iopub.execute_input":"2023-05-25T19:43:35.009167Z","iopub.status.idle":"2023-05-25T19:43:35.025139Z","shell.execute_reply.started":"2023-05-25T19:43:35.009134Z","shell.execute_reply":"2023-05-25T19:43:35.024272Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"         id  target\n0         0       1\n1         2       1\n2         3       1\n3         9       1\n4        11       1\n...     ...     ...\n3258  10861       1\n3259  10865       1\n3260  10868       1\n3261  10874       1\n3262  10875       1\n\n[3263 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}