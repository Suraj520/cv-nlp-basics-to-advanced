{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Architecture Overview\n\n> MarianMT\n\nIt is an architecture designed specifically for machine translation tasks. \n\nThe building blocks of the architecture are elaborated below.\n\n1. Encoder - The encoder block in MarianMT is responsible for processing the input source language and capturing its contextual information. It consists of multiple layers of self attn and feed fwd networks. The self attn mechanism allows the model to weigh the importance of diff words in the source sentence while capturing dependencies b/w them.\n\n2. DEcoder - The decoder block in it generates the translated target lang based on the encoded source language representation. It also consists of multiple layers of self attn and feed fwd networks. In addition to the self attn mechanism, the decoder also employs another attn mechanism called encoder-decoder attn. This allows the models to focus on relevant parts of the source sentence while generating the translation.\n\n3. Cross-Attention: The cross attention mechanism is a key component in MarianMT's architecture. It enables the decoder to attend to the encoded representations of the source sentence while generating the translation. By attending to diff parts of the source sentence, The model aligns to source and target language effectively.\n\n4. Positional Encoding : To capture the positional information of words in a sentence, both the encoder and decoder blocks in MarianMT use positional encoding. This allows the model to understand the order of words, which is crucial for translation tasks.\n\n> Comparison w.r.t to the transformer architectures.\n\n1. BERT - BERT is a pretrained model primarily used for tasks like natural language understanding and sentiment analysis whereas MarianMT is trained specifically for machine translation.\n\n2. GPT - GPT is unidirectional and generates text word by word whereas MarianMT is bidirectional and translates sentences from one language to another.\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import MarianMTModel, MarianTokenizer\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-05-27T12:55:55.884813Z","iopub.execute_input":"2023-05-27T12:55:55.885101Z","iopub.status.idle":"2023-05-27T12:56:10.167245Z","shell.execute_reply.started":"2023-05-27T12:55:55.885076Z","shell.execute_reply":"2023-05-27T12:56:10.166361Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code snippet you provided imports necessary libraries and modules, including `torch`, `torch.utils.data`, `transformers`, and `numpy`. It sets up the environment to work with PyTorch and the Hugging Face Transformers library.\n","metadata":{}},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, dataset_path):\n        self.hypotheses_cols_path = dataset_path + '/deen_nt2021_bleurt_0p2/hypotheses_cols.tsv'\n        self.hypotheses_rows_path = dataset_path + '/deen_nt2021_bleurt_0p2/hypotheses_rows.tsv'\n        self.scores_path = dataset_path + '/deen_nt2021_bleurt_0p2/scores.npy'\n        self.tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-de-en')\n        self.hypotheses_cols = []\n        self.hypotheses_rows = []\n        self.scores = []\n        self.load_data()\n\n    def load_data(self):\n        with open(self.hypotheses_cols_path, 'r', encoding='utf-8') as f:\n            self.hypotheses_cols = f.read().splitlines()\n        with open(self.hypotheses_rows_path, 'r', encoding='utf-8') as f:\n            self.hypotheses_rows = f.read().splitlines()\n        self.scores = np.load(self.scores_path)\n\n    def __len__(self):\n        return min(len(self.hypotheses_cols), len(self.hypotheses_rows), len(self.scores))\n\n    def __getitem__(self, idx):\n        source_text = self.hypotheses_cols[idx]\n        target_text = self.hypotheses_rows[idx]\n        score = self.scores[idx]\n        source_inputs = self.tokenizer.encode(source_text, padding='max_length', truncation=True, max_length=128,\n                                              return_tensors='pt')\n        target_inputs = self.tokenizer.encode(target_text, padding='max_length', truncation=True, max_length=128,\n                                              return_tensors='pt')\n        return {\n            'source_inputs': source_inputs.squeeze(),\n            'target_inputs': target_inputs.squeeze(),\n            'score': score\n        }\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T12:56:10.168920Z","iopub.execute_input":"2023-05-27T12:56:10.169248Z","iopub.status.idle":"2023-05-27T12:56:10.182408Z","shell.execute_reply.started":"2023-05-27T12:56:10.169207Z","shell.execute_reply":"2023-05-27T12:56:10.181528Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The code defines a custom PyTorch dataset called `TranslationDataset` for working with machine translation data. It's functionality can be broken down into following steps:\n\n- The `__init__` method initializes the dataset by setting the paths to the input files (hypotheses_cols.tsv, hypotheses_rows.tsv, and scores.npy) and the MarianTokenizer for the specific translation model.\n- The `load_data` method reads the contents of the input files into the corresponding variables (`hypotheses_cols`, `hypotheses_rows`, and `scores`).\n- The `__len__` method returns the length of the dataset, which is the minimum length among the three lists (`hypotheses_cols`, `hypotheses_rows`, and `scores`).\n- The `__getitem__` method is called when an item from the dataset is requested by index (`idx`). It retrieves the corresponding source text, target text, and score. Then, it encodes the source and target texts using the tokenizer, applying padding and truncation as necessary. Finally, it returns a dictionary containing the source inputs, target inputs, and score.\n","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    source_inputs = torch.stack([item['source_inputs'] for item in batch])\n    target_inputs = torch.stack([item['target_inputs'] for item in batch])\n    scores = torch.tensor([item['score'] for item in batch])\n    return {\n        'source_inputs': source_inputs,\n        'target_inputs': target_inputs,\n        'score': scores\n    }\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T12:56:10.183778Z","iopub.execute_input":"2023-05-27T12:56:10.184784Z","iopub.status.idle":"2023-05-27T12:56:10.192029Z","shell.execute_reply.started":"2023-05-27T12:56:10.184751Z","shell.execute_reply":"2023-05-27T12:56:10.190954Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The `collate_fn` function is a custom collate function used for data batching in the DataLoader. It takes a list of individual data samples (batch) and combines them into a single batch tensor for efficient processing. It's functionality can be broken down into following steos:\n\n- It retrieves the 'source_inputs', 'target_inputs', and 'score' from each item in the batch using list comprehensions.\n- It uses `torch.stack` to stack the 'source_inputs' and 'target_inputs' tensors along a new dimension, creating a batch tensor for both inputs.\n- It converts the list of scores to a tensor using `torch.tensor`.\n- Finally, it returns a dictionary containing the batched 'source_inputs', 'target_inputs', and 'score'.\n","metadata":{}},{"cell_type":"code","source":"dataset_path = '/kaggle/input/machine-translation-mbr-with-neural-metrics/de-en/newstest2021'  # Replace with the actual path to the dataset\ndataset = TranslationDataset(dataset_path)\ndataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T12:56:10.194981Z","iopub.execute_input":"2023-05-27T12:56:10.195456Z","iopub.status.idle":"2023-05-27T12:56:36.891209Z","shell.execute_reply.started":"2023-05-27T12:56:10.195427Z","shell.execute_reply":"2023-05-27T12:56:36.890203Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/source.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4ba45a22434e948cee71d2b8ecd0cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/target.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4347f2c7d3a04c28836d3cebe36b92fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"873609b441d14563a5616bcb9cb95933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9522ca304a7a4304b73aadc787225cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7947885617848198337e32fef977235"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this code snippet, the dataset is initialized using the `TranslationDataset` class, which takes the `dataset_path` as an argument.\n\nAfter initializing the dataset, a `DataLoader` is created using the `dataset`. The `batch_size` is set to 32, which means that the DataLoader will yield batches of 32 samples at a time. The `collate_fn` is passed as an argument to the `collate_fn` parameter, which will be used to collate the samples into batches. Additionally, `shuffle=True` is set to shuffle the samples during training.\n","metadata":{}},{"cell_type":"code","source":"for batch in dataloader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-05-27T12:56:36.892437Z","iopub.execute_input":"2023-05-27T12:56:36.892809Z","iopub.status.idle":"2023-05-27T12:56:46.730696Z","shell.execute_reply.started":"2023-05-27T12:56:36.892779Z","shell.execute_reply":"2023-05-27T12:56:46.729759Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'source_inputs': tensor([[  448,   173,  3034,  ..., 58100, 58100, 58100],\n        [  448,   309,  1908,  ..., 58100, 58100, 58100],\n        [  448,   309,  2284,  ..., 58100, 58100, 58100],\n        ...,\n        [  448,   129,  2206,  ..., 58100, 58100, 58100],\n        [  448,  6448,  8917,  ..., 58100, 58100, 58100],\n        [  448,   300,  2734,  ..., 58100, 58100, 58100]]), 'target_inputs': tensor([[  448,   173,  3034,  ..., 58100, 58100, 58100],\n        [  448,   309,  1908,  ..., 58100, 58100, 58100],\n        [  448,   309,  2284,  ..., 58100, 58100, 58100],\n        ...,\n        [  448,   129,  2206,  ..., 58100, 58100, 58100],\n        [  448,  6448,  8917,  ..., 58100, 58100, 58100],\n        [  448,   300,  2734,  ..., 58100, 58100, 58100]]), 'score': tensor([[[0.9688, 0.8281, 0.8750,  ..., 0.2637, 0.2480, 0.3594],\n         [0.8047, 0.9609, 0.8672,  ..., 0.2461, 0.2539, 0.3613],\n         [0.8672, 0.8906, 0.9688,  ..., 0.2637, 0.2490, 0.3574],\n         ...,\n         [0.2148, 0.2324, 0.2246,  ..., 0.9766, 0.2754, 0.2832],\n         [0.1816, 0.1699, 0.1641,  ..., 0.2412, 1.0234, 0.2812],\n         [0.3164, 0.3301, 0.3262,  ..., 0.2598, 0.3164, 0.9766]],\n\n        [[1.0156, 1.0234, 1.0156,  ..., 0.1299, 0.1172, 0.2871],\n         [1.0156, 1.0234, 1.0156,  ..., 0.1299, 0.1172, 0.2871],\n         [1.0156, 1.0234, 1.0156,  ..., 0.1299, 0.1172, 0.2871],\n         ...,\n         [0.1504, 0.1514, 0.1514,  ..., 0.9141, 0.1064, 0.1123],\n         [0.1641, 0.1631, 0.1631,  ..., 0.1045, 0.9062, 0.2080],\n         [0.2441, 0.2422, 0.2422,  ..., 0.0635, 0.0591, 1.0000]],\n\n        [[1.0078, 0.8125, 0.8047,  ..., 0.4512, 0.2754, 0.3750],\n         [0.7734, 1.0078, 0.8047,  ..., 0.4551, 0.2695, 0.3594],\n         [0.7969, 0.8125, 1.0078,  ..., 0.4551, 0.2441, 0.3594],\n         ...,\n         [0.4336, 0.4336, 0.4316,  ..., 0.9844, 0.2637, 0.3516],\n         [0.2852, 0.2539, 0.2441,  ..., 0.3398, 1.0000, 0.3555],\n         [0.3516, 0.3242, 0.3359,  ..., 0.3672, 0.3047, 1.0078]],\n\n        ...,\n\n        [[1.0000, 0.8672, 0.8672,  ..., 0.5195, 0.5430, 0.3906],\n         [0.8750, 1.0000, 0.8438,  ..., 0.5234, 0.5547, 0.3789],\n         [0.8594, 0.8359, 1.0078,  ..., 0.5234, 0.5391, 0.3828],\n         ...,\n         [0.6172, 0.6016, 0.5898,  ..., 0.9922, 0.4824, 0.3867],\n         [0.6797, 0.6875, 0.6797,  ..., 0.4902, 0.9766, 0.3672],\n         [0.4238, 0.4199, 0.4102,  ..., 0.3867, 0.3867, 1.0156]],\n\n        [[0.9922, 0.8203, 0.8359,  ..., 0.4023, 0.2461, 0.2539],\n         [0.8125, 0.9922, 0.8516,  ..., 0.4004, 0.2393, 0.2539],\n         [0.8438, 0.8594, 0.9922,  ..., 0.3906, 0.2422, 0.2432],\n         ...,\n         [0.3242, 0.3242, 0.3203,  ..., 0.9688, 0.2578, 0.3027],\n         [0.2461, 0.2383, 0.2432,  ..., 0.2852, 0.9297, 0.2480],\n         [0.1953, 0.1641, 0.1660,  ..., 0.2695, 0.1982, 0.9922]],\n\n        [[1.0078, 0.7578, 0.8516,  ..., 0.3242, 0.2471, 0.2773],\n         [0.6250, 1.0000, 0.6250,  ..., 0.2930, 0.1885, 0.2471],\n         [0.9062, 0.7891, 1.0078,  ..., 0.3320, 0.2471, 0.2812],\n         ...,\n         [0.3086, 0.2949, 0.2969,  ..., 0.9922, 0.1914, 0.2852],\n         [0.2598, 0.2119, 0.2471,  ..., 0.2734, 0.9688, 0.2402],\n         [0.2217, 0.2246, 0.2285,  ..., 0.2617, 0.1289, 0.9844]]],\n       dtype=torch.float16)}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/1405038457.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  scores = torch.tensor([item['score'] for item in batch])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Performing sanity check of the dataloader","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_name = 'Helsinki-NLP/opus-mt-de-en'\nmodel = MarianMTModel.from_pretrained(model_name).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T12:56:46.732259Z","iopub.execute_input":"2023-05-27T12:56:46.732926Z","iopub.status.idle":"2023-05-27T12:56:56.946188Z","shell.execute_reply.started":"2023-05-27T12:56:46.732885Z","shell.execute_reply":"2023-05-27T12:56:56.945192Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c163fca6e54a5ea55ba744f046109a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68ff50832bfe4d4ebc566e9ec5c94a87"}},"metadata":{}}]},{"cell_type":"markdown","source":"In this code, the device is set based on the availability of CUDA. If CUDA is available, the device is set to `'cuda'`, otherwise it is set to `'cpu'`.\n\nThe `model_name` variable is set to `'Helsinki-NLP/opus-mt-de-en'`, which is the pre-trained model name for the Marian machine translation model that translates German to English.\n\nThe `MarianMTModel` is then initialized using `from_pretrained()` with the `model_name` and moved to the specified device using `.to(device)`.\n\nAn optimizer is created using `torch.optim.Adam` and the parameters of the `model` are passed to it. The learning rate is set to `1e-4`.\n","metadata":{}},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"for epoch in range(1):\n    for step,batch in enumerate(dataloader):\n        source_inputs = batch['source_inputs'].to(device)\n        target_inputs = batch['target_inputs'].to(device)\n        scores = batch['score'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(source_inputs, decoder_input_ids=target_inputs, return_dict=True)\n        logits = outputs.logits.flatten()\n        \n        # Reshape scores to match the size of logits\n        scores = scores.view(-1)\n\n        # Resize logits to match the size of scores\n        logits = logits[:scores.size(0)]\n\n        # Convert logits and scores to Float dtype\n        logits = logits.float()\n        scores = scores.float()\n\n        loss = torch.nn.functional.mse_loss(logits, scores)\n        loss.backward()\n        optimizer.step()\n        \n        print(\"Step-{}, Loss-{}\".format(step,loss.item()))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T12:56:56.947722Z","iopub.execute_input":"2023-05-27T12:56:56.948106Z","iopub.status.idle":"2023-05-27T13:01:46.970597Z","shell.execute_reply.started":"2023-05-27T12:56:56.948068Z","shell.execute_reply":"2023-05-27T13:01:46.969559Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Step-0, Loss-5.724803924560547\nStep-1, Loss-4.6054253578186035\nStep-2, Loss-26.402923583984375\nStep-3, Loss-0.773452639579773\nStep-4, Loss-2.676593780517578\nStep-5, Loss-2.299866199493408\nStep-6, Loss-0.7236250638961792\nStep-7, Loss-0.5419917702674866\nStep-8, Loss-0.9050765037536621\nStep-9, Loss-0.765993058681488\nStep-10, Loss-0.4023821949958801\nStep-11, Loss-0.25546500086784363\nStep-12, Loss-0.3499484062194824\nStep-13, Loss-0.4514180123806\nStep-14, Loss-0.3846874535083771\nStep-15, Loss-0.22641226649284363\nStep-16, Loss-0.1575862020254135\nStep-17, Loss-0.21602994203567505\nStep-18, Loss-0.29133087396621704\nStep-19, Loss-0.2708299160003662\nStep-20, Loss-0.15109673142433167\nStep-21, Loss-0.09491079300642014\nStep-22, Loss-0.11346182227134705\nStep-23, Loss-0.18739387392997742\nStep-24, Loss-0.17765815556049347\nStep-25, Loss-0.11436118930578232\nStep-26, Loss-0.060721106827259064\nStep-27, Loss-0.07766149938106537\nStep-28, Loss-0.10315463691949844\nStep-29, Loss-0.10392075031995773\nStep-30, Loss-0.0815427228808403\nStep-31, Loss-0.051313187927007675\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# Define the German text\ngerman_text = \"Guten Tag!\"\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:01:46.972331Z","iopub.execute_input":"2023-05-27T13:01:46.972687Z","iopub.status.idle":"2023-05-27T13:01:46.979006Z","shell.execute_reply.started":"2023-05-27T13:01:46.972654Z","shell.execute_reply":"2023-05-27T13:01:46.978158Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer\nmodel_name = 'Helsinki-NLP/opus-mt-de-en'\n\ntokenizer = MarianTokenizer.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:01:46.980302Z","iopub.execute_input":"2023-05-27T13:01:46.981102Z","iopub.status.idle":"2023-05-27T13:01:47.273663Z","shell.execute_reply.started":"2023-05-27T13:01:46.981069Z","shell.execute_reply":"2023-05-27T13:01:47.272717Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Tokenize the German text\ninputs = tokenizer.encode(german_text, return_tensors='pt')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:01:47.276937Z","iopub.execute_input":"2023-05-27T13:01:47.277388Z","iopub.status.idle":"2023-05-27T13:01:47.281933Z","shell.execute_reply.started":"2023-05-27T13:01:47.277354Z","shell.execute_reply":"2023-05-27T13:01:47.280977Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"In the code snippet provided, the German text stored in the variable `german_text` is tokenized using the `tokenizer.encode()` method.\n\nThe `return_tensors='pt'` parameter specifies that the encoded tokens should be returned as PyTorch tensors. The resulting tokenized representation of the German text is stored in the variable `inputs`.","metadata":{}},{"cell_type":"code","source":"# Perform inference\noutputs = model.generate(inputs.to(model.device))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:01:47.283475Z","iopub.execute_input":"2023-05-27T13:01:47.284121Z","iopub.status.idle":"2023-05-27T13:01:47.378436Z","shell.execute_reply.started":"2023-05-27T13:01:47.284088Z","shell.execute_reply":"2023-05-27T13:01:47.376505Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the code snippet, the tokenized German text stored in the variable `inputs` is passed to the `model.generate()` method to perform inference. \n\nThe `generate()` method is used for sequence generation and takes the tokenized input as input. It generates the corresponding translated output sequence using the pre-trained translation model. \n\nThe resulting translated output sequence is stored in the variable `outputs`.","metadata":{}},{"cell_type":"code","source":"# Decode the English translation\nenglish_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:01:47.379719Z","iopub.execute_input":"2023-05-27T13:01:47.380081Z","iopub.status.idle":"2023-05-27T13:01:47.387972Z","shell.execute_reply.started":"2023-05-27T13:01:47.380048Z","shell.execute_reply":"2023-05-27T13:01:47.387060Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The translated output sequence stored in the variable `outputs` is decoded using the `tokenizer.decode()` method. The `decode()` method takes the tensor of token IDs (`outputs[0]`) and converts it back to text.\n\nThe `skip_special_tokens=True` argument is used to exclude any special tokens, such as padding or end-of-sequence tokens, from the decoded text. This ensures that only the meaningful translated text is extracted.\n\nThe resulting English translation is stored in the variable `english_translation`.","metadata":{}},{"cell_type":"code","source":"# Print the translated text\nprint(\"German Text: \", german_text)\nprint(\"English Translation: \", english_translation)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:01:47.389151Z","iopub.execute_input":"2023-05-27T13:01:47.389435Z","iopub.status.idle":"2023-05-27T13:01:47.397094Z","shell.execute_reply.started":"2023-05-27T13:01:47.389411Z","shell.execute_reply":"2023-05-27T13:01:47.396120Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"German Text:  Guten Tag!\nEnglish Translation:  Hello.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Displaying the translation output","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}