{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#035FCA; color:#19180F; font-size:40px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> MarianMT </div>\n<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> Architectural Overview.\n </div>\n<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nIt is an architecture designed specifically for machine translation tasks.<br>\nThe building blocks of the architecture are elaborated below.<br>\n\n1. Encoder - The encoder block in MarianMT is responsible for processing the input source language and capturing its contextual information. It consists of multiple layers of self attn and feed fwd networks. The self attn mechanism allows the model to weigh the importance of diff words in the source sentence while capturing dependencies b/w them.<br>\n\n2. Decoder - The decoder block in it generates the translated target lang based on the encoded source language representation. It also consists of multiple layers of self attn and feed fwd networks. In addition to the self attn mechanism, the decoder also employs another attn mechanism called encoder-decoder attn. This allows the models to focus on relevant parts of the source sentence while generating the translation.<br>\n\n3. Cross-Attention: The cross attention mechanism is a key component in MarianMT's architecture. It enables the decoder to attend to the encoded representations of the source sentence while generating the translation. By attending to diff parts of the source sentence, The model aligns to source and target language effectively.<br>\n\n4. Positional Encoding : To capture the positional information of words in a sentence, both the encoder and decoder blocks in MarianMT use positional encoding. This allows the model to understand the order of words, which is crucial for translation tasks.<br>\n\nComparison w.r.t to the transformer architectures.<br>\n\n1. BERT - BERT is a pretrained model primarily used for tasks like natural language understanding and sentiment analysis whereas MarianMT is trained specifically for machine translation.<br>\n\n2. GPT - GPT is unidirectional and generates text word by word whereas MarianMT is bidirectional and translates sentences from one language to another.<br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> Architecture Diagram.\n </div>\n <div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n    1. Input Layer: The architecture starts with the input layer, represented by the \"Input Layer\" cluster. It takes the source text as input.<br>\n    2. Encoder Layers: The \"Encoder Layers\" cluster represents the stack of encoder layers in MarianMT. Each encoder layer consists of several components:<br>\n   - Self-Attention: Each encoder layer has a self-attention mechanism, represented by the \"Self-Attention\" boxes. Self-attention helps the model focus on different parts of the input sequence while encoding the information.<br>\n   - Feed-Forward Network: The \"Feed-Forward Network\" boxes within each encoder layer represent the feed-forward neural network, which applies non-linear transformations to the input representations.\n<br>\n   - Add & Layer Norm: The \"Add & Layer Norm\" boxes show the addition and layer normalization steps in each encoder layer. These steps help in mitigating the vanishing gradient problem and stabilizing the learning process.<br>\n   The connections within the encoder layers show the flow of information from the input through each encoder layer. The output of one layer serves as the input to the next layer until all encoder layers have been processed.<br>\n    3. Decoder Layers: The \"Decoder Layers\" cluster represents the stack of decoder layers in MarianMT. Similar to the encoder layers, each decoder layer consists of several components:<br>\n   - Self-Attention: The \"Self-Attention\" boxes within each decoder layer represent self-attention applied to the decoder's input sequence. It helps the decoder focus on different parts of the target sequence while decoding.<br>\n   - Encoder-Decoder Attention: The \"Encoder-Decoder Attention\" boxes in each decoder layer represent attention applied between the decoder and the encoder. It allows the decoder to attend to relevant information from the encoder's output.<br>\n   - Feed-Forward Network: Similar to the encoder layers, the \"Feed-Forward Network\" boxes in each decoder layer apply non-linear transformations to the decoder's representations.<br>\n   - Add & Layer Norm: The \"Add & Layer Norm\" boxes depict the addition and layer normalization steps within each decoder layer.<br>\n   The connections within the decoder layers show the flow of information from one decoder layer to the next. Additionally, there are dotted lines connecting the last encoder layer to the first decoder layer, representing the skip connection that allows the decoder to access information from the encoder.<br>\n    4. Output Layer: The architecture concludes with the output layer, represented by the \"Output Layer\" cluster. It generates the target text based on the processed information from the decoder layers.<br>\nThe overall architecture of MarianMT involves the flow of information from the input layer through the encoder layers, followed by the decoder layers, and finally to the output layer. The self-attention mechanisms, feed-forward networks, skip connections, and layer normalization steps enable the model to effectively encode the source text and generate accurate translations in the target language.<br></div>","metadata":{}},{"cell_type":"code","source":"from IPython.display import SVG, display\n\n# Load the SVG file and display it\nsvg_file = '/kaggle/input/notebook-images/marian-mt.svg'\ndisplay(SVG(filename=svg_file))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:32:52.077149Z","iopub.execute_input":"2023-06-09T02:32:52.077517Z","iopub.status.idle":"2023-06-09T02:32:52.102781Z","shell.execute_reply.started":"2023-06-09T02:32:52.077473Z","shell.execute_reply":"2023-06-09T02:32:52.101963Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1382pt\" height=\"1238pt\" viewBox=\"0.00 0.00 1382.00 1238.40\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1234.4)\">\n<title>marianmt</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1234.4 1378,-1234.4 1378,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_input</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"299,-1145.6 299,-1222.4 399,-1222.4 399,-1145.6 299,-1145.6\"/>\n<text text-anchor=\"middle\" x=\"349\" y=\"-1205.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Layer</text>\n</g>\n<g id=\"clust2\" class=\"cluster\">\n<title>cluster_encoders</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-388.8 8,-1137.6 504,-1137.6 504,-388.8 8,-388.8\"/>\n<text text-anchor=\"middle\" x=\"256\" y=\"-1121\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder Layers</text>\n</g>\n<g id=\"clust3\" class=\"cluster\">\n<title>cluster_encoder1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"16,-761.6 16,-1104.8 180,-1104.8 180,-761.6 16,-761.6\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-1088.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder 1</text>\n</g>\n<g id=\"clust4\" class=\"cluster\">\n<title>cluster_encoder2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"16,-460.8 16,-753.6 180,-753.6 180,-460.8 16,-460.8\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-737\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder 2</text>\n</g>\n<g id=\"clust5\" class=\"cluster\">\n<title>cluster_encoderN</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"188,-879.2 188,-1104.8 496,-1104.8 496,-879.2 188,-879.2\"/>\n<text text-anchor=\"middle\" x=\"342\" y=\"-1088.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder N</text>\n</g>\n<g id=\"clust6\" class=\"cluster\">\n<title>cluster_decoders</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"512,-8 512,-871.2 1366,-871.2 1366,-8 512,-8\"/>\n<text text-anchor=\"middle\" x=\"939\" y=\"-854.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder Layers</text>\n</g>\n<g id=\"clust7\" class=\"cluster\">\n<title>cluster_decoder1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1170,-388.8 1170,-838.4 1358,-838.4 1358,-388.8 1170,-388.8\"/>\n<text text-anchor=\"middle\" x=\"1264\" y=\"-821.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder 1</text>\n</g>\n<g id=\"clust8\" class=\"cluster\">\n<title>cluster_decoder2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1050,-16 1050,-380.8 1358,-380.8 1358,-16 1050,-16\"/>\n<text text-anchor=\"middle\" x=\"1204\" y=\"-364.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder 2</text>\n</g>\n<g id=\"clust9\" class=\"cluster\">\n<title>cluster_decoderN</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"520,-604.8 520,-838.4 1162,-838.4 1162,-604.8 520,-604.8\"/>\n<text text-anchor=\"middle\" x=\"841\" y=\"-821.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Decoder N</text>\n</g>\n<g id=\"clust10\" class=\"cluster\">\n<title>cluster_output</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"539,-879.2 539,-956 637,-956 637,-879.2 539,-879.2\"/>\n<text text-anchor=\"middle\" x=\"588\" y=\"-939.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Output Layer</text>\n</g>\n<!-- input -->\n<g id=\"node1\" class=\"node\">\n<title>input</title>\n<text text-anchor=\"middle\" x=\"349\" y=\"-1167.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Source Text</text>\n</g>\n<!-- add_norm1_1 -->\n<g id=\"node4\" class=\"node\">\n<title>add_norm1_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"170.2976,-1072 43.7024,-1072 43.7024,-1036 170.2976,-1036 170.2976,-1072\"/>\n<text text-anchor=\"middle\" x=\"107\" y=\"-1049.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- input&#45;&gt;add_norm1_1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>input-&gt;add_norm1_1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M306.9382,-1165.9325C265.217,-1159.782 204.6196,-1149.2652 184,-1137.6 159.426,-1123.6976 138.2673,-1099.2193 124.3831,-1080.3306\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"127.1787,-1078.2225 118.5296,-1072.1034 121.475,-1082.2806 127.1787,-1078.2225\"/>\n</g>\n<!-- cluster_encoders -->\n<g id=\"node34\" class=\"node\">\n<title>cluster_encoders</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"588\" cy=\"-1054\" rx=\"75.8412\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"588\" y=\"-1049.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cluster_encoders</text>\n</g>\n<!-- input&#45;&gt;cluster_encoders -->\n<g id=\"edge25\" class=\"edge\">\n<title>input-&gt;cluster_encoders</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M391.1479,-1168.199C422.6603,-1164.2518 465.9878,-1155.7687 500,-1137.6 526.8351,-1123.2652 551.3911,-1098.5095 567.7035,-1079.6463\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"570.4045,-1081.8725 574.1725,-1071.9703 565.0518,-1077.3615 570.4045,-1081.8725\"/>\n</g>\n<!-- self_attention1 -->\n<g id=\"node2\" class=\"node\">\n<title>self_attention1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"150.0926,-1000 53.9074,-1000 53.9074,-964 150.0926,-964 150.0926,-1000\"/>\n<text text-anchor=\"middle\" x=\"102\" y=\"-977.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- add_norm1_2 -->\n<g id=\"node5\" class=\"node\">\n<title>add_norm1_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"164.2976,-923.2 37.7024,-923.2 37.7024,-887.2 164.2976,-887.2 164.2976,-923.2\"/>\n<text text-anchor=\"middle\" x=\"101\" y=\"-901\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attention1&#45;&gt;add_norm1_2 -->\n<g id=\"edge3\" class=\"edge\">\n<title>self_attention1-&gt;add_norm1_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M101.763,-963.7995C101.6447,-954.7132 101.4989,-943.5176 101.3666,-933.3549\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.8653,-933.2234 101.2353,-923.2699 97.8659,-933.3146 104.8653,-933.2234\"/>\n</g>\n<!-- feed_forward1 -->\n<g id=\"node3\" class=\"node\">\n<title>feed_forward1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"171.7766,-805.6 24.2234,-805.6 24.2234,-769.6 171.7766,-769.6 171.7766,-805.6\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-783.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Feed-Forward Network</text>\n</g>\n<!-- add_norm2_1 -->\n<g id=\"node8\" class=\"node\">\n<title>add_norm2_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"161.2976,-720.8 34.7024,-720.8 34.7024,-684.8 161.2976,-684.8 161.2976,-720.8\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-698.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- feed_forward1&#45;&gt;add_norm2_1 -->\n<g id=\"edge5\" class=\"edge\">\n<title>feed_forward1-&gt;add_norm2_1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98,-769.1997C98,-758.0112 98,-743.4983 98,-730.8954\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.5001,-730.8731 98,-720.8731 94.5001,-730.8731 101.5001,-730.8731\"/>\n</g>\n<!-- add_norm1_1&#45;&gt;self_attention1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>add_norm1_1-&gt;self_attention1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M105.7383,-1035.8314C105.2035,-1028.131 104.5677,-1018.9743 103.9734,-1010.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.4632,-1010.1467 103.2787,-1000.4133 100.48,-1010.6317 107.4632,-1010.1467\"/>\n</g>\n<!-- add_norm1_2&#45;&gt;feed_forward1 -->\n<g id=\"edge4\" class=\"edge\">\n<title>add_norm1_2-&gt;feed_forward1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M100.5378,-887.0827C100.0548,-868.1466 99.29,-838.1692 98.7262,-816.0666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"102.2216,-815.8391 98.4676,-805.9316 95.2239,-816.0177 102.2216,-815.8391\"/>\n</g>\n<!-- self_attention2 -->\n<g id=\"node6\" class=\"node\">\n<title>self_attention2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"146.0926,-648.8 49.9074,-648.8 49.9074,-612.8 146.0926,-612.8 146.0926,-648.8\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-626.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- add_norm2_2 -->\n<g id=\"node9\" class=\"node\">\n<title>add_norm2_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"161.2976,-576.8 34.7024,-576.8 34.7024,-540.8 161.2976,-540.8 161.2976,-576.8\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-554.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attention2&#45;&gt;add_norm2_2 -->\n<g id=\"edge7\" class=\"edge\">\n<title>self_attention2-&gt;add_norm2_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98,-612.6314C98,-604.931 98,-595.7743 98,-587.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.5001,-587.2132 98,-577.2133 94.5001,-587.2133 101.5001,-587.2132\"/>\n</g>\n<!-- feed_forward2 -->\n<g id=\"node7\" class=\"node\">\n<title>feed_forward2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"171.7766,-504.8 24.2234,-504.8 24.2234,-468.8 171.7766,-468.8 171.7766,-504.8\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-482.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Feed-Forward Network</text>\n</g>\n<!-- add_norm3_1 -->\n<g id=\"node14\" class=\"node\">\n<title>add_norm3_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"98\" cy=\"-414.8\" rx=\"64.8564\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"98\" y=\"-410.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">add_norm3_1</text>\n</g>\n<!-- feed_forward2&#45;&gt;add_norm3_1 -->\n<g id=\"edge9\" class=\"edge\">\n<title>feed_forward2-&gt;add_norm3_1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98,-468.6314C98,-460.931 98,-451.7743 98,-443.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.5001,-443.2132 98,-433.2133 94.5001,-443.2133 101.5001,-443.2132\"/>\n</g>\n<!-- add_norm2_1&#45;&gt;self_attention2 -->\n<g id=\"edge6\" class=\"edge\">\n<title>add_norm2_1-&gt;self_attention2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98,-684.6314C98,-676.931 98,-667.7743 98,-659.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.5001,-659.2132 98,-649.2133 94.5001,-659.2133 101.5001,-659.2132\"/>\n</g>\n<!-- add_norm2_2&#45;&gt;feed_forward2 -->\n<g id=\"edge8\" class=\"edge\">\n<title>add_norm2_2-&gt;feed_forward2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98,-540.6314C98,-532.931 98,-523.7743 98,-515.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.5001,-515.2132 98,-505.2133 94.5001,-515.2133 101.5001,-515.2132\"/>\n</g>\n<!-- self_attentionN -->\n<g id=\"node10\" class=\"node\">\n<title>self_attentionN</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"348.0926,-1000 251.9074,-1000 251.9074,-964 348.0926,-964 348.0926,-1000\"/>\n<text text-anchor=\"middle\" x=\"300\" y=\"-977.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- add_normN_2 -->\n<g id=\"node13\" class=\"node\">\n<title>add_normN_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"446.2976,-923.2 319.7024,-923.2 319.7024,-887.2 446.2976,-887.2 446.2976,-923.2\"/>\n<text text-anchor=\"middle\" x=\"383\" y=\"-901\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attentionN&#45;&gt;add_normN_2 -->\n<g id=\"edge11\" class=\"edge\">\n<title>self_attentionN-&gt;add_normN_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M319.6698,-963.7995C330.5233,-953.7568 344.1616,-941.1372 355.9995,-930.1836\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.5085,-932.6305 363.4713,-923.2699 353.7544,-927.4926 358.5085,-932.6305\"/>\n</g>\n<!-- feed_forwardN -->\n<g id=\"node11\" class=\"node\">\n<title>feed_forwardN</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"487.7766,-1072 340.2234,-1072 340.2234,-1036 487.7766,-1036 487.7766,-1072\"/>\n<text text-anchor=\"middle\" x=\"414\" y=\"-1049.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Feed-Forward Network</text>\n</g>\n<!-- add_normN_1 -->\n<g id=\"node12\" class=\"node\">\n<title>add_normN_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"322.2976,-1072 195.7024,-1072 195.7024,-1036 322.2976,-1036 322.2976,-1072\"/>\n<text text-anchor=\"middle\" x=\"259\" y=\"-1049.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- add_normN_1&#45;&gt;self_attentionN -->\n<g id=\"edge10\" class=\"edge\">\n<title>add_normN_1-&gt;self_attentionN</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M269.346,-1035.8314C273.9237,-1027.7925 279.4052,-1018.1666 284.4588,-1009.2918\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"287.6077,-1010.8351 289.5147,-1000.4133 281.5248,-1007.3712 287.6077,-1010.8351\"/>\n</g>\n<!-- add_norm1_dec_1 -->\n<g id=\"node18\" class=\"node\">\n<title>add_norm1_dec_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1315.2976,-805.6 1188.7024,-805.6 1188.7024,-769.6 1315.2976,-769.6 1315.2976,-805.6\"/>\n<text text-anchor=\"middle\" x=\"1252\" y=\"-783.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- add_normN_2&#45;&gt;add_norm1_dec_1 -->\n<g id=\"edge12\" class=\"edge\">\n<title>add_normN_2-&gt;add_norm1_dec_1</title>\n<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"1,5\" d=\"M446.3088,-892.0885C473.4845,-887.0545 505.6726,-881.8775 535,-879.2 569.9132,-876.0125 1133.6004,-884.5924 1166,-871.2 1194.2876,-859.5073 1218.4452,-833.6777 1233.8887,-813.802\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1236.76,-815.8069 1239.9604,-805.7069 1231.1602,-811.6066 1236.76,-815.8069\"/>\n</g>\n<!-- self_attention1_dec -->\n<g id=\"node15\" class=\"node\">\n<title>self_attention1_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1303.0926,-720.8 1206.9074,-720.8 1206.9074,-684.8 1303.0926,-684.8 1303.0926,-720.8\"/>\n<text text-anchor=\"middle\" x=\"1255\" y=\"-698.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- add_norm1_dec_2 -->\n<g id=\"node19\" class=\"node\">\n<title>add_norm1_dec_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1324.2976,-648.8 1197.7024,-648.8 1197.7024,-612.8 1324.2976,-612.8 1324.2976,-648.8\"/>\n<text text-anchor=\"middle\" x=\"1261\" y=\"-626.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attention1_dec&#45;&gt;add_norm1_dec_2 -->\n<g id=\"edge14\" class=\"edge\">\n<title>self_attention1_dec-&gt;add_norm1_dec_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1256.5141,-684.6314C1257.1558,-676.931 1257.9188,-667.7743 1258.6319,-659.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1262.1229,-659.4694 1259.4656,-649.2133 1255.1471,-658.888 1262.1229,-659.4694\"/>\n</g>\n<!-- encoder_attention1 -->\n<g id=\"node16\" class=\"node\">\n<title>encoder_attention1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1349.5976,-576.8 1178.4024,-576.8 1178.4024,-540.8 1349.5976,-540.8 1349.5976,-576.8\"/>\n<text text-anchor=\"middle\" x=\"1264\" y=\"-554.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder-Decoder Attention</text>\n</g>\n<!-- add_norm1_dec_3 -->\n<g id=\"node20\" class=\"node\">\n<title>add_norm1_dec_3</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1324.2976,-504.8 1197.7024,-504.8 1197.7024,-468.8 1324.2976,-468.8 1324.2976,-504.8\"/>\n<text text-anchor=\"middle\" x=\"1261\" y=\"-482.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- encoder_attention1&#45;&gt;add_norm1_dec_3 -->\n<g id=\"edge16\" class=\"edge\">\n<title>encoder_attention1-&gt;add_norm1_dec_3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1263.243,-540.6314C1262.9221,-532.931 1262.5406,-523.7743 1262.184,-515.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1265.6806,-515.0589 1261.7672,-505.2133 1258.6867,-515.3503 1265.6806,-515.0589\"/>\n</g>\n<!-- feed_forward1_dec -->\n<g id=\"node17\" class=\"node\">\n<title>feed_forward1_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1331.7766,-432.8 1184.2234,-432.8 1184.2234,-396.8 1331.7766,-396.8 1331.7766,-432.8\"/>\n<text text-anchor=\"middle\" x=\"1258\" y=\"-410.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Feed-Forward Network</text>\n</g>\n<!-- add_norm2_dec_1 -->\n<g id=\"node24\" class=\"node\">\n<title>add_norm2_dec_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1184.2976,-348 1057.7024,-348 1057.7024,-312 1184.2976,-312 1184.2976,-348\"/>\n<text text-anchor=\"middle\" x=\"1121\" y=\"-325.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- feed_forward1_dec&#45;&gt;add_norm2_dec_1 -->\n<g id=\"edge18\" class=\"edge\">\n<title>feed_forward1_dec-&gt;add_norm2_dec_1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1221.4271,-396.7516C1212.0257,-391.8193 1202.0214,-386.3087 1193,-380.8 1179.6334,-372.6381 1165.4431,-362.8801 1153.2255,-354.1078\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1155.266,-351.2641 1145.1167,-348.2239 1151.1549,-356.9297 1155.266,-351.2641\"/>\n</g>\n<!-- add_norm1_dec_1&#45;&gt;self_attention1_dec -->\n<g id=\"edge13\" class=\"edge\">\n<title>add_norm1_dec_1-&gt;self_attention1_dec</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1252.651,-769.1997C1253.0468,-758.0112 1253.5602,-743.4983 1254.0061,-730.8954\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1257.5048,-730.9906 1254.3606,-720.8731 1250.5092,-730.7431 1257.5048,-730.9906\"/>\n</g>\n<!-- add_norm1_dec_2&#45;&gt;encoder_attention1 -->\n<g id=\"edge15\" class=\"edge\">\n<title>add_norm1_dec_2-&gt;encoder_attention1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1261.757,-612.6314C1262.0779,-604.931 1262.4594,-595.7743 1262.816,-587.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1266.3133,-587.3503 1263.2328,-577.2133 1259.3194,-587.0589 1266.3133,-587.3503\"/>\n</g>\n<!-- add_norm1_dec_3&#45;&gt;feed_forward1_dec -->\n<g id=\"edge17\" class=\"edge\">\n<title>add_norm1_dec_3-&gt;feed_forward1_dec</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1260.243,-468.6314C1259.9221,-460.931 1259.5406,-451.7743 1259.184,-443.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1262.6806,-443.0589 1258.7672,-433.2133 1255.6867,-443.3503 1262.6806,-443.0589\"/>\n</g>\n<!-- self_attention2_dec -->\n<g id=\"node21\" class=\"node\">\n<title>self_attention2_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1175.0926,-276 1078.9074,-276 1078.9074,-240 1175.0926,-240 1175.0926,-276\"/>\n<text text-anchor=\"middle\" x=\"1127\" y=\"-253.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- add_norm2_dec_2 -->\n<g id=\"node25\" class=\"node\">\n<title>add_norm2_dec_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1201.2976,-204 1074.7024,-204 1074.7024,-168 1201.2976,-168 1201.2976,-204\"/>\n<text text-anchor=\"middle\" x=\"1138\" y=\"-181.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attention2_dec&#45;&gt;add_norm2_dec_2 -->\n<g id=\"edge20\" class=\"edge\">\n<title>self_attention2_dec-&gt;add_norm2_dec_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1129.7758,-239.8314C1130.9522,-232.131 1132.3511,-222.9743 1133.6586,-214.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1137.1364,-214.8272 1135.1869,-204.4133 1130.2167,-213.7699 1137.1364,-214.8272\"/>\n</g>\n<!-- encoder_attention2 -->\n<g id=\"node22\" class=\"node\">\n<title>encoder_attention2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1229.5976,-132 1058.4024,-132 1058.4024,-96 1229.5976,-96 1229.5976,-132\"/>\n<text text-anchor=\"middle\" x=\"1144\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder-Decoder Attention</text>\n</g>\n<!-- add_norm2_dec_3 -->\n<g id=\"node26\" class=\"node\">\n<title>add_norm2_dec_3</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1207.2976,-60 1080.7024,-60 1080.7024,-24 1207.2976,-24 1207.2976,-60\"/>\n<text text-anchor=\"middle\" x=\"1144\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- encoder_attention2&#45;&gt;add_norm2_dec_3 -->\n<g id=\"edge22\" class=\"edge\">\n<title>encoder_attention2-&gt;add_norm2_dec_3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1144,-95.8314C1144,-88.131 1144,-78.9743 1144,-70.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1147.5001,-70.4132 1144,-60.4133 1140.5001,-70.4133 1147.5001,-70.4132\"/>\n</g>\n<!-- feed_forward2_dec -->\n<g id=\"node23\" class=\"node\">\n<title>feed_forward2_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1349.7766,-348 1202.2234,-348 1202.2234,-312 1349.7766,-312 1349.7766,-348\"/>\n<text text-anchor=\"middle\" x=\"1276\" y=\"-325.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Feed-Forward Network</text>\n</g>\n<!-- add_norm2_dec_1&#45;&gt;self_attention2_dec -->\n<g id=\"edge19\" class=\"edge\">\n<title>add_norm2_dec_1-&gt;self_attention2_dec</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1122.5141,-311.8314C1123.1558,-304.131 1123.9188,-294.9743 1124.6319,-286.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1128.1229,-286.6694 1125.4656,-276.4133 1121.1471,-286.088 1128.1229,-286.6694\"/>\n</g>\n<!-- add_norm2_dec_2&#45;&gt;encoder_attention2 -->\n<g id=\"edge21\" class=\"edge\">\n<title>add_norm2_dec_2-&gt;encoder_attention2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1139.5141,-167.8314C1140.1558,-160.131 1140.9188,-150.9743 1141.6319,-142.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1145.1229,-142.6694 1142.4656,-132.4133 1138.1471,-142.088 1145.1229,-142.6694\"/>\n</g>\n<!-- self_attentionN_dec -->\n<g id=\"node27\" class=\"node\">\n<title>self_attentionN_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"784.0926,-720.8 687.9074,-720.8 687.9074,-684.8 784.0926,-684.8 784.0926,-720.8\"/>\n<text text-anchor=\"middle\" x=\"736\" y=\"-698.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Self-Attention</text>\n</g>\n<!-- add_normN_dec_2 -->\n<g id=\"node31\" class=\"node\">\n<title>add_normN_dec_2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"799.2976,-648.8 672.7024,-648.8 672.7024,-612.8 799.2976,-612.8 799.2976,-648.8\"/>\n<text text-anchor=\"middle\" x=\"736\" y=\"-626.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attentionN_dec&#45;&gt;add_normN_dec_2 -->\n<g id=\"edge24\" class=\"edge\">\n<title>self_attentionN_dec-&gt;add_normN_dec_2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M736,-684.6314C736,-676.931 736,-667.7743 736,-659.2166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"739.5001,-659.2132 736,-649.2133 732.5001,-659.2133 739.5001,-659.2132\"/>\n</g>\n<!-- encoder_attentionN -->\n<g id=\"node28\" class=\"node\">\n<title>encoder_attentionN</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1153.5976,-805.6 982.4024,-805.6 982.4024,-769.6 1153.5976,-769.6 1153.5976,-805.6\"/>\n<text text-anchor=\"middle\" x=\"1068\" y=\"-783.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Encoder-Decoder Attention</text>\n</g>\n<!-- feed_forwardN_dec -->\n<g id=\"node29\" class=\"node\">\n<title>feed_forwardN_dec</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"964.7766,-805.6 817.2234,-805.6 817.2234,-769.6 964.7766,-769.6 964.7766,-805.6\"/>\n<text text-anchor=\"middle\" x=\"891\" y=\"-783.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Feed-Forward Network</text>\n</g>\n<!-- add_normN_dec_1 -->\n<g id=\"node30\" class=\"node\">\n<title>add_normN_dec_1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"799.2976,-805.6 672.7024,-805.6 672.7024,-769.6 799.2976,-769.6 799.2976,-805.6\"/>\n<text text-anchor=\"middle\" x=\"736\" y=\"-783.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- add_normN_dec_1&#45;&gt;self_attentionN_dec -->\n<g id=\"edge23\" class=\"edge\">\n<title>add_normN_dec_1-&gt;self_attentionN_dec</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M736,-769.1997C736,-758.0112 736,-743.4983 736,-730.8954\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"739.5001,-730.8731 736,-720.8731 732.5001,-730.8731 739.5001,-730.8731\"/>\n</g>\n<!-- add_normN_dec_3 -->\n<g id=\"node32\" class=\"node\">\n<title>add_normN_dec_3</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"654.2976,-805.6 527.7024,-805.6 527.7024,-769.6 654.2976,-769.6 654.2976,-805.6\"/>\n<text text-anchor=\"middle\" x=\"591\" y=\"-783.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Add &amp; Layer Norm</text>\n</g>\n<!-- output -->\n<g id=\"node33\" class=\"node\">\n<title>output</title>\n<text text-anchor=\"middle\" x=\"588\" y=\"-901\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Target Text</text>\n</g>\n<!-- cluster_decoders -->\n<g id=\"node35\" class=\"node\">\n<title>cluster_decoders</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"588\" cy=\"-982\" rx=\"75.8412\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"588\" y=\"-977.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cluster_decoders</text>\n</g>\n<!-- cluster_encoders&#45;&gt;cluster_decoders -->\n<g id=\"edge26\" class=\"edge\">\n<title>cluster_encoders-&gt;cluster_decoders</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M588,-1035.8314C588,-1028.131 588,-1018.9743 588,-1010.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"591.5001,-1010.4132 588,-1000.4133 584.5001,-1010.4133 591.5001,-1010.4132\"/>\n</g>\n<!-- cluster_decoders&#45;&gt;output -->\n<g id=\"edge27\" class=\"edge\">\n<title>cluster_decoders-&gt;output</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M588,-963.7995C588,-954.7132 588,-943.5176 588,-933.3549\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"591.5001,-933.2698 588,-923.2699 584.5001,-933.2699 591.5001,-933.2698\"/>\n</g>\n</g>\n</svg>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nImporting modules </div>\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import MarianMTModel, MarianTokenizer\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:26:02.563195Z","iopub.execute_input":"2023-06-09T02:26:02.563446Z","iopub.status.idle":"2023-06-09T02:26:15.215080Z","shell.execute_reply.started":"2023-06-09T02:26:02.563421Z","shell.execute_reply":"2023-06-09T02:26:15.214178Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe code defines a custom PyTorch dataset called `TranslationDataset` for working with machine translation data. It's functionality can be broken down into following steps:<br>\n- The `__init__` method initializes the dataset by setting the paths to the input files (hypotheses_cols.tsv, hypotheses_rows.tsv, and scores.npy) and the MarianTokenizer for the specific translation model.<br>\n- The `load_data` method reads the contents of the input files into the corresponding variables (`hypotheses_cols`, `hypotheses_rows`, and `scores`).<br>\n- The `__len__` method returns the length of the dataset, which is the minimum length among the three lists (`hypotheses_cols`, `hypotheses_rows`, and `scores`).<br>\n- The `__getitem__` method is called when an item from the dataset is requested by index (`idx`). It retrieves the corresponding source text, target text, and score. Then, it encodes the source and target texts using the tokenizer, applying padding and truncation as necessary. Finally, it returns a dictionary containing the source inputs, target inputs, and score.<br>\n</div>","metadata":{}},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, dataset_path):\n        self.hypotheses_cols_path = dataset_path + '/deen_nt2021_bleurt_0p2/hypotheses_cols.tsv'\n        self.hypotheses_rows_path = dataset_path + '/deen_nt2021_bleurt_0p2/hypotheses_rows.tsv'\n        self.scores_path = dataset_path + '/deen_nt2021_bleurt_0p2/scores.npy'\n        self.tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-de-en')\n        self.hypotheses_cols = []\n        self.hypotheses_rows = []\n        self.scores = []\n        self.load_data()\n\n    def load_data(self):\n        with open(self.hypotheses_cols_path, 'r', encoding='utf-8') as f:\n            self.hypotheses_cols = f.read().splitlines()\n        with open(self.hypotheses_rows_path, 'r', encoding='utf-8') as f:\n            self.hypotheses_rows = f.read().splitlines()\n        self.scores = np.load(self.scores_path)\n\n    def __len__(self):\n        return min(len(self.hypotheses_cols), len(self.hypotheses_rows), len(self.scores))\n\n    def __getitem__(self, idx):\n        source_text = self.hypotheses_cols[idx]\n        target_text = self.hypotheses_rows[idx]\n        score = self.scores[idx]\n        source_inputs = self.tokenizer.encode(source_text, padding='max_length', truncation=True, max_length=128,\n                                              return_tensors='pt')\n        target_inputs = self.tokenizer.encode(target_text, padding='max_length', truncation=True, max_length=128,\n                                              return_tensors='pt')\n        return {\n            'source_inputs': source_inputs.squeeze(),\n            'target_inputs': target_inputs.squeeze(),\n            'score': score\n        }\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:26:15.216814Z","iopub.execute_input":"2023-06-09T02:26:15.217162Z","iopub.status.idle":"2023-06-09T02:26:15.227824Z","shell.execute_reply.started":"2023-06-09T02:26:15.217130Z","shell.execute_reply":"2023-06-09T02:26:15.226954Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe `collate_fn` function is a custom collate function used for data batching in the DataLoader. It takes a list of individual data samples (batch) and combines them into a single batch tensor for efficient processing. It's functionality can be broken down into following steos:<br>\n- It retrieves the 'source_inputs', 'target_inputs', and 'score' from each item in the batch using list comprehensions.<br>\n- It uses `torch.stack` to stack the 'source_inputs' and 'target_inputs' tensors along a new dimension, creating a batch tensor for both inputs.<br>\n- It converts the list of scores to a tensor using `torch.tensor`.<br>\n- Finally, it returns a dictionary containing the batched 'source_inputs', 'target_inputs', and 'score'.<br>\n</div>","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    source_inputs = torch.stack([item['source_inputs'] for item in batch])\n    target_inputs = torch.stack([item['target_inputs'] for item in batch])\n    scores = torch.tensor([item['score'] for item in batch])\n    return {\n        'source_inputs': source_inputs,\n        'target_inputs': target_inputs,\n        'score': scores\n    }\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:26:15.229162Z","iopub.execute_input":"2023-06-09T02:26:15.229712Z","iopub.status.idle":"2023-06-09T02:26:15.238666Z","shell.execute_reply.started":"2023-06-09T02:26:15.229680Z","shell.execute_reply":"2023-06-09T02:26:15.237631Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nIn this code snippet, the dataset is initialized using the `TranslationDataset` class, which takes the `dataset_path` as an argument.<br>\nAfter initializing the dataset, a `DataLoader` is created using the `dataset`. The `batch_size` is set to 32, which means that the DataLoader will yield batches of 32 samples at a time. The `collate_fn` is passed as an argument to the `collate_fn` parameter, which will be used to collate the samples into batches. Additionally, `shuffle=True` is set to shuffle the samples during training.\n</div>","metadata":{}},{"cell_type":"code","source":"dataset_path = '/kaggle/input/machine-translation-mbr-with-neural-metrics/de-en/newstest2021'  # Replace with the actual path to the dataset\ndataset = TranslationDataset(dataset_path)\ndataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:26:15.241317Z","iopub.execute_input":"2023-06-09T02:26:15.241637Z","iopub.status.idle":"2023-06-09T02:26:40.467933Z","shell.execute_reply.started":"2023-06-09T02:26:15.241606Z","shell.execute_reply":"2023-06-09T02:26:40.466915Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/source.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c3fbbd2d58248fe96fad12ef7abc106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/target.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"593199a5eb834b9cbd6c1c8baa2c1292"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11d7ceb2637445d7984f1724dea69bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690203aba25544a88536ceb3382aeec7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07698ae62eaf4958aad5d107ad8fa24f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nPerforming sanity check of the dataloader</div>","metadata":{}},{"cell_type":"code","source":"for batch in dataloader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:26:40.469429Z","iopub.execute_input":"2023-06-09T02:26:40.469757Z","iopub.status.idle":"2023-06-09T02:26:50.372512Z","shell.execute_reply.started":"2023-06-09T02:26:40.469725Z","shell.execute_reply":"2023-06-09T02:26:50.371576Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'source_inputs': tensor([[  448,   976,  1009,  ..., 58100, 58100, 58100],\n        [  448,   129,  1741,  ..., 58100, 58100, 58100],\n        [  448,   184,  3034,  ..., 58100, 58100, 58100],\n        ...,\n        [  448, 20649,  8917,  ..., 58100, 58100, 58100],\n        [  448,   300,  2348,  ..., 58100, 58100, 58100],\n        [  448,   184,  4484,  ..., 58100, 58100, 58100]]), 'target_inputs': tensor([[  448,   976,  1009,  ..., 58100, 58100, 58100],\n        [  448,   129,  1741,  ..., 58100, 58100, 58100],\n        [  448,   184,  3034,  ..., 58100, 58100, 58100],\n        ...,\n        [  448, 20649,  8917,  ..., 58100, 58100, 58100],\n        [  448,   300,  2348,  ..., 58100, 58100, 58100],\n        [  448,   184,  4484,  ..., 58100, 58100, 58100]]), 'score': tensor([[[0.9766, 0.7578, 0.7422,  ..., 0.3594, 0.2500, 0.3477],\n         [0.7578, 0.9922, 0.8672,  ..., 0.3711, 0.2363, 0.3242],\n         [0.7812, 0.8672, 0.9766,  ..., 0.4160, 0.2402, 0.3164],\n         ...,\n         [0.2305, 0.1807, 0.1670,  ..., 1.0078, 0.3320, 0.4473],\n         [0.1934, 0.2012, 0.1992,  ..., 0.3672, 0.9531, 0.2969],\n         [0.2539, 0.2021, 0.1982,  ..., 0.3633, 0.2871, 0.9922]],\n\n        [[1.0078, 0.8750, 0.6719,  ..., 0.4043, 0.3828, 0.2559],\n         [0.8438, 1.0078, 0.6875,  ..., 0.3965, 0.3613, 0.2852],\n         [0.7891, 0.8281, 0.9922,  ..., 0.4043, 0.3457, 0.2773],\n         ...,\n         [0.3379, 0.3320, 0.3594,  ..., 0.9922, 0.3320, 0.2363],\n         [0.3359, 0.3281, 0.3359,  ..., 0.2773, 0.9453, 0.3906],\n         [0.2617, 0.2539, 0.2617,  ..., 0.2344, 0.3984, 1.0000]],\n\n        [[0.9766, 0.8281, 0.8281,  ..., 0.2031, 0.2070, 0.3359],\n         [0.8281, 0.9688, 0.8438,  ..., 0.2207, 0.2275, 0.3203],\n         [0.7969, 0.8125, 0.9766,  ..., 0.2178, 0.2207, 0.3516],\n         ...,\n         [0.2129, 0.2207, 0.2441,  ..., 0.9531, 0.1514, 0.2734],\n         [0.2500, 0.2617, 0.2578,  ..., 0.1436, 0.9453, 0.2969],\n         [0.3398, 0.3438, 0.3633,  ..., 0.2188, 0.2080, 0.9922]],\n\n        ...,\n\n        [[0.9844, 0.7891, 0.7891,  ..., 0.2598, 0.2910, 0.2793],\n         [0.7656, 0.9844, 0.9844,  ..., 0.2949, 0.3105, 0.3125],\n         [0.7656, 0.9844, 0.9844,  ..., 0.2949, 0.3105, 0.3125],\n         ...,\n         [0.2891, 0.3359, 0.3359,  ..., 0.9922, 0.2285, 0.3242],\n         [0.2148, 0.2578, 0.2578,  ..., 0.2129, 0.9766, 0.2695],\n         [0.2578, 0.3047, 0.3047,  ..., 0.3281, 0.2695, 0.9844]],\n\n        [[0.9922, 0.8828, 0.9219,  ..., 0.3398, 0.1377, 0.3359],\n         [0.8828, 0.9922, 0.8438,  ..., 0.3164, 0.1338, 0.3262],\n         [0.8984, 0.8516, 1.0000,  ..., 0.3457, 0.1250, 0.3359],\n         ...,\n         [0.3281, 0.2930, 0.3340,  ..., 0.9531, 0.1270, 0.3750],\n         [0.2314, 0.2100, 0.2227,  ..., 0.2500, 0.9531, 0.3184],\n         [0.2695, 0.2539, 0.2656,  ..., 0.2324, 0.1689, 0.9766]],\n\n        [[0.9922, 0.8828, 0.8828,  ..., 0.3789, 0.3828, 0.2617],\n         [0.8828, 1.0078, 1.0078,  ..., 0.4199, 0.4180, 0.2480],\n         [0.8828, 1.0078, 1.0078,  ..., 0.4199, 0.4180, 0.2480],\n         ...,\n         [0.4492, 0.4746, 0.4746,  ..., 1.0000, 0.2910, 0.2852],\n         [0.5977, 0.6250, 0.6250,  ..., 0.3418, 0.9844, 0.2227],\n         [0.2246, 0.2217, 0.2217,  ..., 0.2129, 0.1943, 0.9688]]],\n       dtype=torch.float16)}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_29/1405038457.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  scores = torch.tensor([item['score'] for item in batch])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nIn this code, the device is set based on the availability of CUDA. If CUDA is available, the device is set to `'cuda'`, otherwise it is set to `'cpu'`.<br>\nThe `model_name` variable is set to `'Helsinki-NLP/opus-mt-de-en'`, which is the pre-trained model name for the Marian machine translation model that translates German to English.<br>\nThe `MarianMTModel` is then initialized using `from_pretrained()` with the `model_name` and moved to the specified device using `.to(device)`.<br>\nAn optimizer is created using `torch.optim.Adam` and the parameters of the `model` are passed to it. The learning rate is set to `1e-4`.<br></div>\n","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_name = 'Helsinki-NLP/opus-mt-de-en'\nmodel = MarianMTModel.from_pretrained(model_name).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:26:50.374100Z","iopub.execute_input":"2023-06-09T02:26:50.374768Z","iopub.status.idle":"2023-06-09T02:27:18.964488Z","shell.execute_reply.started":"2023-06-09T02:26:50.374732Z","shell.execute_reply":"2023-06-09T02:27:18.963446Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d87b1b44e244318950898c103db9b4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45f26da81ae45a8b42d5abd8c25ec53"}},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nTraining loop<br></div>\n","metadata":{}},{"cell_type":"code","source":"for epoch in range(1):\n    for step,batch in enumerate(dataloader):\n        source_inputs = batch['source_inputs'].to(device)\n        target_inputs = batch['target_inputs'].to(device)\n        scores = batch['score'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(source_inputs, decoder_input_ids=target_inputs, return_dict=True)\n        logits = outputs.logits.flatten()\n        \n        # Reshape scores to match the size of logits\n        scores = scores.view(-1)\n\n        # Resize logits to match the size of scores\n        logits = logits[:scores.size(0)]\n\n        # Convert logits and scores to Float dtype\n        logits = logits.float()\n        scores = scores.float()\n\n        loss = torch.nn.functional.mse_loss(logits, scores)\n        loss.backward()\n        optimizer.step()\n        \n        print(\"Step-{}, Loss-{}\".format(step,loss.item()))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:27:18.965979Z","iopub.execute_input":"2023-06-09T02:27:18.966324Z","iopub.status.idle":"2023-06-09T02:32:23.931405Z","shell.execute_reply.started":"2023-06-09T02:27:18.966292Z","shell.execute_reply":"2023-06-09T02:32:23.929903Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Step-0, Loss-6.140812873840332\nStep-1, Loss-5.103466033935547\nStep-2, Loss-33.85565185546875\nStep-3, Loss-1.0194576978683472\nStep-4, Loss-2.71262526512146\nStep-5, Loss-2.3830552101135254\nStep-6, Loss-0.8063452243804932\nStep-7, Loss-0.594483494758606\nStep-8, Loss-1.063205361366272\nStep-9, Loss-0.8688474893569946\nStep-10, Loss-0.39699050784111023\nStep-11, Loss-0.2795497477054596\nStep-12, Loss-0.39580032229423523\nStep-13, Loss-0.5460092425346375\nStep-14, Loss-0.3480374217033386\nStep-15, Loss-0.23657439649105072\nStep-16, Loss-0.18055318295955658\nStep-17, Loss-0.25841689109802246\nStep-18, Loss-0.3393004536628723\nStep-19, Loss-0.2732982337474823\nStep-20, Loss-0.14043833315372467\nStep-21, Loss-0.11748728901147842\nStep-22, Loss-0.14804702997207642\nStep-23, Loss-0.19636274874210358\nStep-24, Loss-0.14661014080047607\nStep-25, Loss-0.07689355313777924\nStep-26, Loss-0.07264295965433121\nStep-27, Loss-0.0739104300737381\nStep-28, Loss-0.11785602569580078\nStep-29, Loss-0.10988137125968933\nStep-30, Loss-0.07705274969339371\nStep-31, Loss-0.051662344485521317\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nInference loop</div>\n","metadata":{}},{"cell_type":"code","source":"# Define the German text\ngerman_text = \"Guten Tag!\"\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:32:23.933097Z","iopub.execute_input":"2023-06-09T02:32:23.933437Z","iopub.status.idle":"2023-06-09T02:32:23.938446Z","shell.execute_reply.started":"2023-06-09T02:32:23.933405Z","shell.execute_reply":"2023-06-09T02:32:23.937410Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer\nmodel_name = 'Helsinki-NLP/opus-mt-de-en'\n\ntokenizer = MarianTokenizer.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:32:23.940092Z","iopub.execute_input":"2023-06-09T02:32:23.940414Z","iopub.status.idle":"2023-06-09T02:32:24.371534Z","shell.execute_reply.started":"2023-06-09T02:32:23.940385Z","shell.execute_reply":"2023-06-09T02:32:24.370536Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nIn the code snippet provided, the German text stored in the variable `german_text` is tokenized using the `tokenizer.encode()` method.<br>\nThe `return_tensors='pt'` parameter specifies that the encoded tokens should be returned as PyTorch tensors. The resulting tokenized representation of the German text is stored in the variable `inputs`.<br></div>","metadata":{}},{"cell_type":"code","source":"# Tokenize the German text\ninputs = tokenizer.encode(german_text, return_tensors='pt')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:32:24.374883Z","iopub.execute_input":"2023-06-09T02:32:24.375346Z","iopub.status.idle":"2023-06-09T02:32:24.380367Z","shell.execute_reply.started":"2023-06-09T02:32:24.375319Z","shell.execute_reply":"2023-06-09T02:32:24.379301Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nIn the code snippet, the tokenized German text stored in the variable `inputs` is passed to the `model.generate()` method to perform inference. <br>\nThe `generate()` method is used for sequence generation and takes the tokenized input as input. It generates the corresponding translated output sequence using the pre-trained translation model. <br>\nThe resulting translated output sequence is stored in the variable `outputs`.<br></div>","metadata":{}},{"cell_type":"code","source":"# Perform inference\noutputs = model.generate(inputs.to(model.device))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:32:24.382102Z","iopub.execute_input":"2023-06-09T02:32:24.382484Z","iopub.status.idle":"2023-06-09T02:32:24.470310Z","shell.execute_reply.started":"2023-06-09T02:32:24.382442Z","shell.execute_reply":"2023-06-09T02:32:24.469307Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe translated output sequence stored in the variable `outputs` is decoded using the `tokenizer.decode()` method. The `decode()` method takes the tensor of token IDs (`outputs[0]`) and converts it back to text.<br>\nThe `skip_special_tokens=True` argument is used to exclude any special tokens, such as padding or end-of-sequence tokens, from the decoded text. This ensures that only the meaningful translated text is extracted.<br>\nThe resulting English translation is stored in the variable `english_translation`.<br></div>","metadata":{}},{"cell_type":"code","source":"# Decode the English translation\nenglish_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:32:24.471762Z","iopub.execute_input":"2023-06-09T02:32:24.472170Z","iopub.status.idle":"2023-06-09T02:32:24.477761Z","shell.execute_reply.started":"2023-06-09T02:32:24.472132Z","shell.execute_reply":"2023-06-09T02:32:24.476745Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nDisplaying the translation output</div>","metadata":{}},{"cell_type":"code","source":"# Print the translated text\nprint(\"German Text: \", german_text)\nprint(\"English Translation: \", english_translation)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T02:32:24.478863Z","iopub.execute_input":"2023-06-09T02:32:24.479973Z","iopub.status.idle":"2023-06-09T02:32:24.487234Z","shell.execute_reply.started":"2023-06-09T02:32:24.479942Z","shell.execute_reply":"2023-06-09T02:32:24.486078Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"German Text:  Guten Tag!\nEnglish Translation:  Hello!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}