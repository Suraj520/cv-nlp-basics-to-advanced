{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#035FCA; color:#19180F; font-size:40px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> Roberta Base </div>\n\n<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">üîßWorking of the architecture‚öôÔ∏è\n </div>\n<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n- Suppose we have the following sentence: \"The quick brown fox jumps over the lazy dog\".<br>\n- To process this sentence using Robert's base, we first need to build it into subword order using the BPE token. The tokenizer breaks the sentence into the following subwords:<br>\n[\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]<br>\n- Each subword is assigned a unique numeric ID called input_id. The input_id for this statement is:<br>\n[0, 2327, 1669, 1239, 1029, 1231, 1103, 0, 10747, 995, 2]<br>\n- Then pad the input_ids with zeros to ensure that all sequences are the same length. In this case, the maximum sequence length is 10, so the input_ids are padded with the following lengths:\n[0, 2327, 1669, 1239, 1029, 1231, 1103, 0, 10747, 995]<br>\n- In addition to input_ids, Robert's base generates an attention mask, which is a binary mask indicating which tokens in the input sequence the model should focus on. The attention mask is generated by setting the value to 1 for all unpadded markers and 0 for all padded markers. In this case, the attention mask would be:\n[1, 1, 1, 1, 1, 1, 1, 0, 1, 1]<br>\n- Finally, Robert's base also generates a tensor of token_type_ids, which is used to distinguish between different segments of the input sequence. In this case, there is only one segment, so all token_type_ids are set to 0.<br>\n<br>\n- Once the input_id, attention mask, and token_type_ids are generated, they are fed into Robert's base model for processing. During the training, the model is optimized to reduce the loss function that measures the difference between the expected output and the real output. The model is trained in the large text data body and has learned to recognize text patterns, there are signs of its task, such as prediction of the next word in a sentence or classification of the text fragment.<br></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> How does Roberta fit in for the task ? ü§î</div><br>\n<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\n- In order to predict the readability level in CommonLit Readability Prize Challenge texts with Roberta base methodology, we must first tokenize them into sequences of subwords. Hugging Face library provides us with a tokenizer that accomplishes this by converting texts into sequences of input_ids, attention_ids, and token_type_ids. \n<br>\n- Input ids numerically represent subwords within texts while attention ids give us information on which ones are particularly important for our model's output generation. \n<br>\n- Token type ids enable our models to distinguish between different parts or segments within an inputted text (such as distinction between questions and answers). \n<br>\n- With our segmented data good-to-go thanks to Hugging Face's library support , we can now feed it all through Roberta base model itself! Having been trained extensively on various forms/styles/texts prior . It is capable now if recognizing patterns directly relevant/indicative towards that very elusive 'readability level' all those participating in The CommonLit Readability Prize seek after! The complexity of sentence structures and advanced vocabulary in an input text can increase the predicted readability level according to some models. \n<br>\n- Simpler texts using less sophisticated language are more likely to be assigned lower predicted levels by these models.\n<br></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#568FD1; color:#19180F; font-size:30px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\"> üè¢ Architecture Diagram.\n </div>","metadata":{}},{"cell_type":"code","source":"from IPython.display import SVG, display\n\n# Load the SVG file and display it\nsvg_file = '/kaggle/input/notebook-images/roberta-base.svg'\ndisplay(SVG(filename=svg_file))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T00:08:19.687060Z","iopub.execute_input":"2023-06-09T00:08:19.688303Z","iopub.status.idle":"2023-06-09T00:08:19.715518Z","shell.execute_reply.started":"2023-06-09T00:08:19.688258Z","shell.execute_reply":"2023-06-09T00:08:19.713492Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg height=\"902pt\" viewBox=\"0.00 0.00 989.00 901.60\" width=\"989pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 897.6)\">\n<title>roberta_base</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-897.6 985,-897.6 985,4 -4,4\" stroke=\"transparent\"/>\n<g class=\"cluster\" id=\"clust1\">\n<title>cluster_input</title>\n<polygon fill=\"none\" points=\"857,-808.8 857,-885.6 947,-885.6 947,-808.8 857,-808.8\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"902\" y=\"-869\">Input Layer</text>\n</g>\n<g class=\"cluster\" id=\"clust2\">\n<title>cluster_embedding</title>\n<polygon fill=\"none\" points=\"512,-614.4 512,-800.8 973,-800.8 973,-614.4 512,-614.4\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"742.5\" y=\"-784.2\">Embedding Layer</text>\n</g>\n<g class=\"cluster\" id=\"clust3\">\n<title>cluster_encoders</title>\n<polygon fill=\"none\" points=\"8,-8 8,-724 504,-724 504,-8 8,-8\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256\" y=\"-707.4\">Encoder Layers</text>\n</g>\n<g class=\"cluster\" id=\"clust4\">\n<title>cluster_encoder1</title>\n<polygon fill=\"none\" points=\"332,-380.8 332,-691.2 496,-691.2 496,-380.8 332,-380.8\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-674.6\">Encoder 1</text>\n</g>\n<g class=\"cluster\" id=\"clust5\">\n<title>cluster_encoder2</title>\n<polygon fill=\"none\" points=\"332,-80 332,-372.8 496,-372.8 496,-80 332,-80\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-356.2\">Encoder 2</text>\n</g>\n<g class=\"cluster\" id=\"clust6\">\n<title>cluster_encoderN</title>\n<polygon fill=\"none\" points=\"16,-465.6 16,-691.2 324,-691.2 324,-465.6 16,-465.6\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170\" y=\"-674.6\">Encoder N</text>\n</g>\n<g class=\"cluster\" id=\"clust7\">\n<title>cluster_pooler</title>\n<polygon fill=\"none\" points=\"847,-465.6 847,-542.4 935,-542.4 935,-465.6 847,-465.6\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891\" y=\"-525.8\">Pooler Layer</text>\n</g>\n<g class=\"cluster\" id=\"clust8\">\n<title>cluster_output</title>\n<polygon fill=\"none\" points=\"846,-380.8 846,-457.6 937,-457.6 937,-380.8 846,-380.8\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891.5\" y=\"-441\">Output Layer</text>\n</g>\n<!-- input -->\n<g class=\"node\" id=\"node1\">\n<title>input</title>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"902\" y=\"-830.6\">Input Text</text>\n</g>\n<!-- embedding -->\n<g class=\"node\" id=\"node2\">\n<title>embedding</title>\n<polygon fill=\"none\" points=\"964.7605,-768 839.2395,-768 839.2395,-732 964.7605,-732 964.7605,-768\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"902\" y=\"-745.8\">Token Embeddings</text>\n</g>\n<!-- input&#45;&gt;embedding -->\n<g class=\"edge\" id=\"edge12\">\n<title>input-&gt;embedding</title>\n<path d=\"M902,-816.3997C902,-805.2112 902,-790.6983 902,-778.0954\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"905.5001,-778.0731 902,-768.0731 898.5001,-778.0731 905.5001,-778.0731\" stroke=\"#000000\"/>\n</g>\n<!-- embeddings -->\n<g class=\"node\" id=\"node5\">\n<title>embeddings</title>\n<polygon fill=\"none\" points=\"964.8255,-658.4 817.1745,-658.4 817.1745,-622.4 964.8255,-622.4 964.8255,-658.4\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891\" y=\"-636.2\">Combined Embeddings</text>\n</g>\n<!-- embedding&#45;&gt;embeddings -->\n<g class=\"edge\" id=\"edge13\">\n<title>embedding-&gt;embeddings</title>\n<path d=\"M900.1836,-731.9018C898.4559,-714.6883 895.8309,-688.5333 893.8292,-668.5888\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"897.3004,-668.1256 892.8191,-658.5251 890.3353,-668.8247 897.3004,-668.1256\" stroke=\"#000000\"/>\n</g>\n<!-- add_norm1_1 -->\n<g class=\"node\" id=\"node8\">\n<title>add_norm1_1</title>\n<polygon fill=\"none\" points=\"486.2976,-658.4 359.7024,-658.4 359.7024,-622.4 486.2976,-622.4 486.2976,-658.4\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"423\" y=\"-636.2\">Add &amp; Layer Norm</text>\n</g>\n<!-- embedding&#45;&gt;add_norm1_1 -->\n<g class=\"edge\" id=\"edge1\">\n<title>embedding-&gt;add_norm1_1</title>\n<path d=\"M839.043,-733.2623C836.3349,-732.7884 833.6456,-732.3635 831,-732 795.4342,-727.1139 541.053,-738.0108 508,-724 479.9863,-712.1253 456.1178,-686.3228 440.8708,-666.5031\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"443.6496,-664.3744 434.8773,-658.4332 438.03,-668.5481 443.6496,-664.3744\" stroke=\"#000000\"/>\n</g>\n<!-- positional -->\n<g class=\"node\" id=\"node3\">\n<title>positional</title>\n<polygon fill=\"none\" points=\"821.9981,-768 676.0019,-768 676.0019,-732 821.9981,-732 821.9981,-768\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-745.8\">Positional Embeddings</text>\n</g>\n<!-- segment -->\n<g class=\"node\" id=\"node4\">\n<title>segment</title>\n<polygon fill=\"none\" points=\"658.4869,-768 519.5131,-768 519.5131,-732 658.4869,-732 658.4869,-768\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589\" y=\"-745.8\">Segment Embeddings</text>\n</g>\n<!-- cluster_encoders -->\n<g class=\"node\" id=\"node21\">\n<title>cluster_encoders</title>\n<ellipse cx=\"891\" cy=\"-568.4\" fill=\"none\" rx=\"75.8412\" ry=\"18\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891\" y=\"-564.2\">cluster_encoders</text>\n</g>\n<!-- embeddings&#45;&gt;cluster_encoders -->\n<g class=\"edge\" id=\"edge14\">\n<title>embeddings-&gt;cluster_encoders</title>\n<path d=\"M891,-622.2314C891,-614.531 891,-605.3743 891,-596.8166\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"894.5001,-596.8132 891,-586.8133 887.5001,-596.8133 894.5001,-596.8132\" stroke=\"#000000\"/>\n</g>\n<!-- self_attention1 -->\n<g class=\"node\" id=\"node6\">\n<title>self_attention1</title>\n<polygon fill=\"none\" points=\"466.0926,-586.4 369.9074,-586.4 369.9074,-550.4 466.0926,-550.4 466.0926,-586.4\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418\" y=\"-564.2\">Self-Attention</text>\n</g>\n<!-- add_norm1_2 -->\n<g class=\"node\" id=\"node9\">\n<title>add_norm1_2</title>\n<polygon fill=\"none\" points=\"480.2976,-509.6 353.7024,-509.6 353.7024,-473.6 480.2976,-473.6 480.2976,-509.6\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"417\" y=\"-487.4\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attention1&#45;&gt;add_norm1_2 -->\n<g class=\"edge\" id=\"edge3\">\n<title>self_attention1-&gt;add_norm1_2</title>\n<path d=\"M417.763,-550.1995C417.6447,-541.1132 417.4989,-529.9176 417.3666,-519.7549\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"420.8653,-519.6234 417.2353,-509.6699 413.8659,-519.7146 420.8653,-519.6234\" stroke=\"#000000\"/>\n</g>\n<!-- feed_forward1 -->\n<g class=\"node\" id=\"node7\">\n<title>feed_forward1</title>\n<polygon fill=\"none\" points=\"487.7766,-424.8 340.2234,-424.8 340.2234,-388.8 487.7766,-388.8 487.7766,-424.8\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-402.6\">Feed-Forward Network</text>\n</g>\n<!-- add_norm2_1 -->\n<g class=\"node\" id=\"node12\">\n<title>add_norm2_1</title>\n<polygon fill=\"none\" points=\"477.2976,-340 350.7024,-340 350.7024,-304 477.2976,-304 477.2976,-340\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-317.8\">Add &amp; Layer Norm</text>\n</g>\n<!-- feed_forward1&#45;&gt;add_norm2_1 -->\n<g class=\"edge\" id=\"edge5\">\n<title>feed_forward1-&gt;add_norm2_1</title>\n<path d=\"M414,-388.3997C414,-377.2112 414,-362.6983 414,-350.0954\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-350.0731 414,-340.0731 410.5001,-350.0731 417.5001,-350.0731\" stroke=\"#000000\"/>\n</g>\n<!-- add_norm1_1&#45;&gt;self_attention1 -->\n<g class=\"edge\" id=\"edge2\">\n<title>add_norm1_1-&gt;self_attention1</title>\n<path d=\"M421.7383,-622.2314C421.2035,-614.531 420.5677,-605.3743 419.9734,-596.8166\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"423.4632,-596.5467 419.2787,-586.8133 416.48,-597.0317 423.4632,-596.5467\" stroke=\"#000000\"/>\n</g>\n<!-- add_norm1_2&#45;&gt;feed_forward1 -->\n<g class=\"edge\" id=\"edge4\">\n<title>add_norm1_2-&gt;feed_forward1</title>\n<path d=\"M416.349,-473.1997C415.9532,-462.0112 415.4398,-447.4983 414.9939,-434.8954\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"418.4908,-434.7431 414.6394,-424.8731 411.4952,-434.9906 418.4908,-434.7431\" stroke=\"#000000\"/>\n</g>\n<!-- self_attention2 -->\n<g class=\"node\" id=\"node10\">\n<title>self_attention2</title>\n<polygon fill=\"none\" points=\"462.0926,-268 365.9074,-268 365.9074,-232 462.0926,-232 462.0926,-268\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-245.8\">Self-Attention</text>\n</g>\n<!-- add_norm2_2 -->\n<g class=\"node\" id=\"node13\">\n<title>add_norm2_2</title>\n<polygon fill=\"none\" points=\"477.2976,-196 350.7024,-196 350.7024,-160 477.2976,-160 477.2976,-196\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-173.8\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attention2&#45;&gt;add_norm2_2 -->\n<g class=\"edge\" id=\"edge7\">\n<title>self_attention2-&gt;add_norm2_2</title>\n<path d=\"M414,-231.8314C414,-224.131 414,-214.9743 414,-206.4166\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-206.4132 414,-196.4133 410.5001,-206.4133 417.5001,-206.4132\" stroke=\"#000000\"/>\n</g>\n<!-- feed_forward2 -->\n<g class=\"node\" id=\"node11\">\n<title>feed_forward2</title>\n<polygon fill=\"none\" points=\"487.7766,-124 340.2234,-124 340.2234,-88 487.7766,-88 487.7766,-124\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-101.8\">Feed-Forward Network</text>\n</g>\n<!-- add_norm3_1 -->\n<g class=\"node\" id=\"node18\">\n<title>add_norm3_1</title>\n<ellipse cx=\"414\" cy=\"-34\" fill=\"none\" rx=\"64.8564\" ry=\"18\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-29.8\">add_norm3_1</text>\n</g>\n<!-- feed_forward2&#45;&gt;add_norm3_1 -->\n<g class=\"edge\" id=\"edge9\">\n<title>feed_forward2-&gt;add_norm3_1</title>\n<path d=\"M414,-87.8314C414,-80.131 414,-70.9743 414,-62.4166\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-62.4132 414,-52.4133 410.5001,-62.4133 417.5001,-62.4132\" stroke=\"#000000\"/>\n</g>\n<!-- add_norm2_1&#45;&gt;self_attention2 -->\n<g class=\"edge\" id=\"edge6\">\n<title>add_norm2_1-&gt;self_attention2</title>\n<path d=\"M414,-303.8314C414,-296.131 414,-286.9743 414,-278.4166\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-278.4132 414,-268.4133 410.5001,-278.4133 417.5001,-278.4132\" stroke=\"#000000\"/>\n</g>\n<!-- add_norm2_2&#45;&gt;feed_forward2 -->\n<g class=\"edge\" id=\"edge8\">\n<title>add_norm2_2-&gt;feed_forward2</title>\n<path d=\"M414,-159.8314C414,-152.131 414,-142.9743 414,-134.4166\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-134.4132 414,-124.4133 410.5001,-134.4133 417.5001,-134.4132\" stroke=\"#000000\"/>\n</g>\n<!-- self_attentionN -->\n<g class=\"node\" id=\"node14\">\n<title>self_attentionN</title>\n<polygon fill=\"none\" points=\"135.0926,-586.4 38.9074,-586.4 38.9074,-550.4 135.0926,-550.4 135.0926,-586.4\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-564.2\">Self-Attention</text>\n</g>\n<!-- add_normN_2 -->\n<g class=\"node\" id=\"node17\">\n<title>add_normN_2</title>\n<polygon fill=\"none\" points=\"150.2976,-509.6 23.7024,-509.6 23.7024,-473.6 150.2976,-473.6 150.2976,-509.6\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-487.4\">Add &amp; Layer Norm</text>\n</g>\n<!-- self_attentionN&#45;&gt;add_normN_2 -->\n<g class=\"edge\" id=\"edge11\">\n<title>self_attentionN-&gt;add_normN_2</title>\n<path d=\"M87,-550.1995C87,-541.1132 87,-529.9176 87,-519.7549\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"90.5001,-519.6698 87,-509.6699 83.5001,-519.6699 90.5001,-519.6698\" stroke=\"#000000\"/>\n</g>\n<!-- feed_forwardN -->\n<g class=\"node\" id=\"node15\">\n<title>feed_forwardN</title>\n<polygon fill=\"none\" points=\"315.7766,-658.4 168.2234,-658.4 168.2234,-622.4 315.7766,-622.4 315.7766,-658.4\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242\" y=\"-636.2\">Feed-Forward Network</text>\n</g>\n<!-- add_normN_1 -->\n<g class=\"node\" id=\"node16\">\n<title>add_normN_1</title>\n<polygon fill=\"none\" points=\"150.2976,-658.4 23.7024,-658.4 23.7024,-622.4 150.2976,-622.4 150.2976,-658.4\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-636.2\">Add &amp; Layer Norm</text>\n</g>\n<!-- add_normN_1&#45;&gt;self_attentionN -->\n<g class=\"edge\" id=\"edge10\">\n<title>add_normN_1-&gt;self_attentionN</title>\n<path d=\"M87,-622.2314C87,-614.531 87,-605.3743 87,-596.8166\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"90.5001,-596.8132 87,-586.8133 83.5001,-596.8133 90.5001,-596.8132\" stroke=\"#000000\"/>\n</g>\n<!-- pooler -->\n<g class=\"node\" id=\"node19\">\n<title>pooler</title>\n<polygon fill=\"none\" points=\"920.5654,-509.6 861.4346,-509.6 861.4346,-473.6 920.5654,-473.6 920.5654,-509.6\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891\" y=\"-487.4\">Pooling</text>\n</g>\n<!-- output -->\n<g class=\"node\" id=\"node20\">\n<title>output</title>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891\" y=\"-402.6\">Output</text>\n</g>\n<!-- pooler&#45;&gt;output -->\n<g class=\"edge\" id=\"edge16\">\n<title>pooler-&gt;output</title>\n<path d=\"M891,-473.1997C891,-462.0112 891,-447.4983 891,-434.8954\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"894.5001,-434.8731 891,-424.8731 887.5001,-434.8731 894.5001,-434.8731\" stroke=\"#000000\"/>\n</g>\n<!-- cluster_encoders&#45;&gt;pooler -->\n<g class=\"edge\" id=\"edge15\">\n<title>cluster_encoders-&gt;pooler</title>\n<path d=\"M891,-550.1995C891,-541.1132 891,-529.9176 891,-519.7549\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"894.5001,-519.6698 891,-509.6699 887.5001,-519.6699 894.5001,-519.6698\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">Installing transformers</div>","metadata":{}},{"cell_type":"code","source":"!pip install transformers --quiet","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:39.643496Z","iopub.execute_input":"2023-06-08T23:48:39.644153Z","iopub.status.idle":"2023-06-08T23:48:54.111095Z","shell.execute_reply.started":"2023-06-08T23:48:39.644114Z","shell.execute_reply":"2023-06-08T23:48:54.109706Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">Setting device for torch</div>","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:54.114565Z","iopub.execute_input":"2023-06-08T23:48:54.115266Z","iopub.status.idle":"2023-06-08T23:48:55.468059Z","shell.execute_reply.started":"2023-06-08T23:48:54.115217Z","shell.execute_reply":"2023-06-08T23:48:55.466951Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">Importing modules</div>","metadata":{}},{"cell_type":"code","source":"#importing modules\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:55.469687Z","iopub.execute_input":"2023-06-08T23:48:55.470266Z","iopub.status.idle":"2023-06-08T23:48:58.087393Z","shell.execute_reply.started":"2023-06-08T23:48:55.470234Z","shell.execute_reply":"2023-06-08T23:48:58.086175Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">Reading the dataframe</div>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:58.090599Z","iopub.execute_input":"2023-06-08T23:48:58.091093Z","iopub.status.idle":"2023-06-08T23:48:58.195086Z","shell.execute_reply.started":"2023-06-08T23:48:58.091061Z","shell.execute_reply":"2023-06-08T23:48:58.193881Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:58.196858Z","iopub.execute_input":"2023-06-08T23:48:58.197239Z","iopub.status.idle":"2023-06-08T23:48:58.220670Z","shell.execute_reply.started":"2023-06-08T23:48:58.197200Z","shell.execute_reply":"2023-06-08T23:48:58.219505Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"          id url_legal license  \\\n0  c12129c31       NaN     NaN   \n1  85aa80a4c       NaN     NaN   \n2  b69ac6792       NaN     NaN   \n3  dd1000b26       NaN     NaN   \n4  37c1b32fb       NaN     NaN   \n\n                                             excerpt    target  standard_error  \n0  When the young people returned to the ballroom... -0.340259        0.464009  \n1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n3  And outside before the palace a great garden w... -1.054013        0.450007  \n4  Once upon a time there were Three Bears who li...  0.247197        0.510845  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url_legal</th>\n      <th>license</th>\n      <th>excerpt</th>\n      <th>target</th>\n      <th>standard_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c12129c31</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>When the young people returned to the ballroom...</td>\n      <td>-0.340259</td>\n      <td>0.464009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85aa80a4c</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n      <td>-0.315372</td>\n      <td>0.480805</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b69ac6792</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>As Roger had predicted, the snow departed as q...</td>\n      <td>-0.580118</td>\n      <td>0.476676</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dd1000b26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>And outside before the palace a great garden w...</td>\n      <td>-1.054013</td>\n      <td>0.450007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37c1b32fb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Once upon a time there were Three Bears who li...</td>\n      <td>0.247197</td>\n      <td>0.510845</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe following dataset class takes in four parameters: texts, targets, tokenizer, and max_length.<br>\nThe texts parameter represents the text data that is being passed to the dataset.<br>\nThe targets parameter represents the target values for the text data. The tokenizer parameter is the tokenizer used to tokenize the text data. The max_length parameter represents the maximum length of the input text.<br>\nThe len method returns the length of the texts data. The getitem method takes an index as input and returns a dictionary that contains the input_ids, attention_mask, and target values for that particular index.<br></div>","metadata":{}},{"cell_type":"code","source":"#creating dataset class\nclass CommonLitDataset(Dataset):\n    def __init__(self, texts, targets, tokenizer, max_length):\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts.iloc[idx]\n        target = self.targets.iloc[idx]\n        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n        return {'input_ids': encoding['input_ids'].squeeze(), 'attention_mask': encoding['attention_mask'].squeeze(), 'target': torch.tensor(target, dtype=torch.float)}\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:58.222223Z","iopub.execute_input":"2023-06-08T23:48:58.223148Z","iopub.status.idle":"2023-06-08T23:48:58.233184Z","shell.execute_reply.started":"2023-06-08T23:48:58.223103Z","shell.execute_reply":"2023-06-08T23:48:58.231979Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe CommonLitModel class inherits from the nn.Module class, which is the base class for all PyTorch neural network modules. The __init__ method initializes the model and its components. The AutoModel method in the transformer library is used to load a pre-trained model, and the nn.Linear method is used to create a linear layer that maps a hidden model dimension to a single output value.<br>\n\nThe Forward method determines the forward transition of the model. It requires two inputs - input_ids and attention_mask. The input_ids are in the tokenized input model, and the attention mask is a binary mask that indicates which tokens are padding tokens and which are actual tokens. output is the output of the pre-trained model, and pooler_output is the output of the last layer of the model, which is used as the input of the linear layer. The output of the linear layer is then returned as the final output of the model.<br></div>","metadata":{}},{"cell_type":"code","source":"#creating model class\nclass CommonLitModel(nn.Module):\n    def __init__(self, model_name):\n        super(CommonLitModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.linear = nn.Linear(self.model.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        pooler_output = outputs['pooler_output']\n        return self.linear(pooler_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:58.234932Z","iopub.execute_input":"2023-06-08T23:48:58.235714Z","iopub.status.idle":"2023-06-08T23:48:58.245419Z","shell.execute_reply.started":"2023-06-08T23:48:58.235673Z","shell.execute_reply":"2023-06-08T23:48:58.244106Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe following function train accepts four input arguments: model, data loader, optimizer, criterion, and entity.<br>\n<br>\nThe model refers to the neural network model that will be trained. Dataloader is an iterable object for loading training data.<br>\nOptimizers are optimization algorithms used to update model weights during training. <br>\n\nThe criterion is a loss function that measures the difference between predicted output and actual output.<br>\n\nThe device indicates whether to use the CPU or GPU for calculations. <br>\n\nThe function first puts the model in training mode using model.train(), Then do the following for each data packet in the data loader:<br>\n\n- The optimizer resets the model parameter gradients to zero using optimizer.zero_grad().<br>\n- Use input_ids.to(device), target.to(device), and attention_mask.to(device) to extract the input data, target label, and attention mask from the batch and move it to the specified device.<br>\n- Pass input data and an attention mask to the model to generate the expected output using model (input_ids, attention mask). Calculate the difference between the predicted output and the target label using the criterion loss function to obtain the loss value.<br>\n- Use loss.backward() to calculate the gradient of the model parameters with respect to the loss value. The optimizer is used to update the model parameter using optimizer.step ().<br>\n- The loss of the current batch is added to the value of the operating loss. After all batches have been processed in the data loader, the function returns the average loss value across all lots. <br> </div>","metadata":{}},{"cell_type":"code","source":"#training loop\ndef train(model, dataloader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    for batch in dataloader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        target = batch['target'].to(device)\n        output = model(input_ids, attention_mask)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    return running_loss / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:58.247250Z","iopub.execute_input":"2023-06-08T23:48:58.247751Z","iopub.status.idle":"2023-06-08T23:48:58.258304Z","shell.execute_reply.started":"2023-06-08T23:48:58.247713Z","shell.execute_reply":"2023-06-08T23:48:58.257021Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe evaluation function has four input parameters:<br>\n<br>\n- Model: A trained deep learning model to evaluate<br>\n- Dataloader: An instance of the DataLoader class that iterates over the validation dataset<br>\n- Criterion: A loss function used to calculate model performance<br>\n- Device: The device on which the model is evaluated (e.g. CPU or GPU)<br>\n<br>\nWorking of the code <br>\n<br>\nUse the model.eval() method to put the model into evaluation mode. Then use a for loop to loop over each batch of data in the validation dataset. For each batch, extract the input ID, attention mask, and target value from the batch and move them to the specified unit using the to() method. The model's pass method is called with the input id and attention mask as arguments, and the resulting output is stored in the output variable. Calculates the loss between the model output and the target value using the specified loss function and adds the loss value to the run_loss variable.<br>\n<br>\nFinally, the average loss across all batches of the validation data set is calculated by dividing run_loss by the length of the data loader and is returned as the output of the function.<br></div>","metadata":{}},{"cell_type":"code","source":"#validation loop\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            target = batch['target'].to(device)\n            output = model(input_ids, attention_mask)\n            loss = criterion(output, target)\n            running_loss += loss.item()\n    return running_loss / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:58.259975Z","iopub.execute_input":"2023-06-08T23:48:58.260510Z","iopub.status.idle":"2023-06-08T23:48:58.273545Z","shell.execute_reply.started":"2023-06-08T23:48:58.260471Z","shell.execute_reply":"2023-06-08T23:48:58.272245Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nFinally, The model is trained after splitting the data into train,val split</div>","metadata":{}},{"cell_type":"code","source":"train_texts, val_texts, train_targets, val_targets = train_test_split(df['excerpt'], df['target'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:58.278614Z","iopub.execute_input":"2023-06-08T23:48:58.279388Z","iopub.status.idle":"2023-06-08T23:48:58.290407Z","shell.execute_reply.started":"2023-06-08T23:48:58.279356Z","shell.execute_reply":"2023-06-08T23:48:58.289320Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nDataset for train, val split is created in this snippet.</div>","metadata":{}},{"cell_type":"code","source":"#initialize the tokenizer and create datasets\nmodel_name = 'roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmax_length = 256\n\ntrain_dataset = CommonLitDataset(train_texts, train_targets, tokenizer, max_length)\nval_dataset = CommonLitDataset(val_texts, val_targets, tokenizer, max_length)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:48:58.291812Z","iopub.execute_input":"2023-06-08T23:48:58.292640Z","iopub.status.idle":"2023-06-08T23:49:00.188728Z","shell.execute_reply.started":"2023-06-08T23:48:58.292597Z","shell.execute_reply":"2023-06-08T23:49:00.187572Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55719f1980fe4131a1eaff9b54cd2f97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d67cada0cc2049fe9be5e8ddf20f9380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac95b896a0047ab92aab0249f76ed80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b6622fe7016499bb862ec34695754aa"}},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nDataloaders are created in this snippet.</div>","metadata":{}},{"cell_type":"code","source":"#create dataloaders\nbatch_size = 32\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:49:00.190320Z","iopub.execute_input":"2023-06-08T23:49:00.190734Z","iopub.status.idle":"2023-06-08T23:49:00.198367Z","shell.execute_reply.started":"2023-06-08T23:49:00.190692Z","shell.execute_reply":"2023-06-08T23:49:00.196466Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nSanity check of dataloader are done in this snippet</div>","metadata":{}},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:49:00.200401Z","iopub.execute_input":"2023-06-08T23:49:00.200821Z","iopub.status.idle":"2023-06-08T23:49:00.364714Z","shell.execute_reply.started":"2023-06-08T23:49:00.200778Z","shell.execute_reply":"2023-06-08T23:49:00.363512Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[    0, 11478,   415,  ...,     1,     1,     1],\n        [    0,   133, 16192,  ...,     1,     1,     1],\n        [    0,   113,  7608,  ...,     1,     1,     1],\n        ...,\n        [    0,  3762,     9,  ...,     1,     1,     1],\n        [    0,   170,    56,  ...,     1,     1,     1],\n        [    0, 36176,     4,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'target': tensor([ 0.8109, -1.9890, -0.4486, -2.3668, -0.6239, -1.3741, -0.6689, -0.6543,\n        -1.4036, -0.7628,  0.0197, -1.1634,  1.0784, -0.9335, -2.4172, -0.0505,\n        -1.6221, -0.8274, -0.1407,  1.0203, -1.5631, -2.0578, -1.7984, -1.4224,\n        -1.9779,  0.1239, -2.6793, -2.1231,  0.0987, -2.4935, -2.2186, -0.9216])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nModel is moved to device after initialisation along with defining the optimizer and loss function.</div>","metadata":{}},{"cell_type":"code","source":"model = CommonLitModel(model_name).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\ncriterion = nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:49:00.366375Z","iopub.execute_input":"2023-06-08T23:49:00.367734Z","iopub.status.idle":"2023-06-08T23:49:10.364259Z","shell.execute_reply.started":"2023-06-08T23:49:00.367683Z","shell.execute_reply":"2023-06-08T23:49:10.362889Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6de83c1e2f34ee4aaf141e9ff2ba946"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nModel is trained for 10 epochs.</div>","metadata":{}},{"cell_type":"code","source":"#Training the model\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n    val_loss = evaluate(model, val_dataloader, criterion, device)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T23:49:10.366820Z","iopub.execute_input":"2023-06-08T23:49:10.367263Z","iopub.status.idle":"2023-06-09T00:07:59.574843Z","shell.execute_reply.started":"2023-06-08T23:49:10.367221Z","shell.execute_reply":"2023-06-09T00:07:59.573510Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Train Loss: 1.2169, Validation Loss: 1.0498\nEpoch 2/10, Train Loss: 1.0925, Validation Loss: 1.0466\nEpoch 3/10, Train Loss: 1.0819, Validation Loss: 1.0484\nEpoch 4/10, Train Loss: 1.0824, Validation Loss: 1.0488\nEpoch 5/10, Train Loss: 1.0828, Validation Loss: 1.0594\nEpoch 6/10, Train Loss: 1.0824, Validation Loss: 1.0529\nEpoch 7/10, Train Loss: 1.0842, Validation Loss: 1.0573\nEpoch 8/10, Train Loss: 1.0785, Validation Loss: 1.0501\nEpoch 9/10, Train Loss: 1.0813, Validation Loss: 1.0470\nEpoch 10/10, Train Loss: 1.0817, Validation Loss: 1.0581\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe model is saved in this snippet</div>","metadata":{}},{"cell_type":"code","source":"#save the model\ntorch.save(model.state_dict(), 'commonlit_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T00:07:59.576544Z","iopub.execute_input":"2023-06-09T00:07:59.577043Z","iopub.status.idle":"2023-06-09T00:08:00.370522Z","shell.execute_reply.started":"2023-06-09T00:07:59.577000Z","shell.execute_reply":"2023-06-09T00:08:00.369304Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nModel is loaded in this snippet</div>","metadata":{}},{"cell_type":"code","source":"#load the model for evaluation\nmodel.load_state_dict(torch.load('commonlit_model.pth'))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T00:08:00.372417Z","iopub.execute_input":"2023-06-09T00:08:00.372846Z","iopub.status.idle":"2023-06-09T00:08:00.726292Z","shell.execute_reply.started":"2023-06-09T00:08:00.372803Z","shell.execute_reply":"2023-06-09T00:08:00.725198Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nValidation loss is calculated in this snippet</div>","metadata":{}},{"cell_type":"code","source":"val_loss = evaluate(model, val_dataloader, criterion, device)\nprint(f\"Validation Loss: {val_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T00:08:00.728297Z","iopub.execute_input":"2023-06-09T00:08:00.729519Z","iopub.status.idle":"2023-06-09T00:08:10.144954Z","shell.execute_reply.started":"2023-06-09T00:08:00.729473Z","shell.execute_reply":"2023-06-09T00:08:10.136674Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Validation Loss: 1.0581\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nThe get_preditions() function takes three parameters: a model, a data loader, and an entity.<br>\n<br>\n- Model is a pre-trained model that will be used to make predictions.<br>\n- Dataloader is a validation data loader that contains batches of validation datasets.<br>\n- Device specifies the device (GPU or CPU) on which to perform the calculation. In this function, model.eval() puts the model into evaluation mode to disable training-specific operations such as dropout. <br>\n- The prediction list is initialized to hold the predicted values. Using torch.no_grad (): The context manager is used to exclude the gradient calculation, making the inference process faster and more efficient.<br>\n- This feature is then repeated through the data loading program and the input_ids and attention_mask are obtained from each batch of the dataloader. \n- These input values, along with the attention mask, are then fed to the model to produce output values.<br>\n- The predictions generated by the model are then transformed into a list and expanded to the pre-initialized list of predictions. Finally, the function returns a list of predictions.<br>\n</div>","metadata":{}},{"cell_type":"code","source":"def get_predictions(model, dataloader, device):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            output = model(input_ids, attention_mask)\n            predictions.extend(output.squeeze().tolist())\n    return predictions\n\nval_predictions = get_predictions(model, val_dataloader, device)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T00:08:10.155512Z","iopub.execute_input":"2023-06-09T00:08:10.156528Z","iopub.status.idle":"2023-06-09T00:08:19.673551Z","shell.execute_reply.started":"2023-06-09T00:08:10.156471Z","shell.execute_reply":"2023-06-09T00:08:19.672339Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#A7C6F5; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 5px solid #19180F;\">\nMean square error between validation targets and predictions are calculated in this snippet.</div>","metadata":{}},{"cell_type":"code","source":"mse = mean_squared_error(val_targets, val_predictions)\nprint(f\"Mean Squared Error: {mse:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T00:08:19.675145Z","iopub.execute_input":"2023-06-09T00:08:19.676176Z","iopub.status.idle":"2023-06-09T00:08:19.685225Z","shell.execute_reply.started":"2023-06-09T00:08:19.676130Z","shell.execute_reply":"2023-06-09T00:08:19.683758Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Mean Squared Error: 1.0285\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}