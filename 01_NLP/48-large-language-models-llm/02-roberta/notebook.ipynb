{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## About\n",
        "\n",
        "> RoBERTA\n",
        "\n",
        "- It is also known as Robustly Optimized BERT Approach.\n",
        "- It is a language model based on the BERT(Bidirectional Encoder Representations from Transformers) architecture.\n",
        "- The core components of a block diagram of RoBERTA consists of Input Text, Tokenizer, Embedding Layer, multiple transformer encoders, pooling layer and the output.\n",
        "- Input text is basically the raw text that needs to processed and can be a sentence, paragraph or an entire document.\n",
        "- Tokenizer is the block where the input text is being fed and it splits it into individual tokens. The tokens can be words, subwords or even chars. Tokenization as a process helps in breaking down the text into meaningful units for further processing.\n",
        "- Embeddings layer is the block which converts the tokenized text into word embeddings. Embeddings are vector representations of words or tokens which aim at capturing the semantic and contextual information. These embeddings are learned during the training proces and are used to represent the input tokens.\n",
        "- Transformer Encoder is being utilized by ROBERTA over a stack. Each transformer encoder consists of multiple attention layers and feed forward neural networks. These encoders process the embedding in a self attention mechanism where each token attends to other tokens in the input sequence thereby capturing the dependencies and relationships between them. These stacked transformer encoders capture hierarhical information in the input text.\n",
        "- The output of the transformer encoder's last layer is being passed through the pooling layer. The pooling layer aggregates the information from all the tokens into a fixed length representation such as mean or max of them. This pooled representation captures the semantic meaning of the input text.\n",
        "- Finally, The pooled representation is passed through a linear layer or other classif/regression layers to produce the desired output."
      ],
      "metadata": {
        "id": "7IdCI-mC8IhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1EOwBwS5QMR",
        "outputId": "87e52bca-ed62-479e-a4dd-8e461c2f7828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"suraj520\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"3f7b336dcca01a32b5c1c511af889e78\""
      ],
      "metadata": {
        "id": "OpfQhJwbBPi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c sentiment-analysis-on-movie-reviews\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyMcE_g4Awsy",
        "outputId": "61bf1bbb-1757-4416-afb0-7f45c26d0988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sentiment-analysis-on-movie-reviews.zip to /content\n",
            "\r  0% 0.00/1.90M [00:00<?, ?B/s]\n",
            "\r100% 1.90M/1.90M [00:00<00:00, 171MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip sentiment-analysis-on-movie-reviews.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2koI98uAx5d",
        "outputId": "70cfc99b-75b2-4f48-b931-4acbcceb1013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sentiment-analysis-on-movie-reviews.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: test.tsv.zip            \n",
            "  inflating: train.tsv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1MqfJypBhht",
        "outputId": "cb7c4c0a-4791-49ec-8b11-0e52839f3e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "_1gUp5KWBb1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.tsv.zip\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"test.tsv.zip\", sep=\"\\t\")\n",
        "\n",
        "# Split into input text and corresponding labels\n",
        "train_text = train_df[\"Phrase\"].tolist()\n",
        "train_labels = train_df[\"Sentiment\"].tolist()\n",
        "test_text = test_df[\"Phrase\"].tolist()\n"
      ],
      "metadata": {
        "id": "63i_Zvz0Be9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n"
      ],
      "metadata": {
        "id": "jXLUWdgFBpyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input text\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    train_text,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Jz-Qub4mBu3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the input dataset\n",
        "class MovieReviewsDataset(Dataset):\n",
        "    def __init__(self, tokens, labels):\n",
        "        self.input_ids = tokens[\"input_ids\"]\n",
        "        self.attention_mask = tokens[\"attention_mask\"]\n",
        "        self.labels = labels\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[index],\n",
        "            \"attention_mask\": self.attention_mask[index],\n",
        "            \"labels\": self.labels[index]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Whq__yZpBw0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MovieReviewsDataset(train_tokens, train_labels)"
      ],
      "metadata": {
        "id": "dq2zEe9BB0f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=5)\n",
        "model.to(device)\n",
        "\n",
        "# Prepare the dataloader\n",
        "batch_size = 32*4\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG3uG3D6B2PO",
        "outputId": "6595e83f-3dec-4588-9ea8-b796654234ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etDyyQQNB8KB",
        "outputId": "66ace797-75bb-4eb0-9b87-5928996b7a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "from tqdm import tqdm\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        if step%100==0:\n",
        "          print(\"Step-{}, Loss-{}\".format(step,loss.item()))\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_460-rRB-ft",
        "outputId": "02c90232-aea0-4eb0-8892-8ba27d029748"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step-0, Loss-1.5351877212524414\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [02:26,  1.46s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step-100, Loss-0.9756748080253601\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "200it [04:51,  1.46s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step-200, Loss-0.8548558950424194\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "300it [07:17,  1.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step-300, Loss-0.6825312972068787\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "400it [09:43,  1.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step-400, Loss-0.8613194823265076\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "500it [12:08,  1.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step-500, Loss-0.630751371383667\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "600it [14:34,  1.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step-600, Loss-0.7979530096054077\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "700it [16:59,  1.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step-700, Loss-0.7271313667297363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "800it [19:25,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-800, Loss-0.6418065428733826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "900it [21:51,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-900, Loss-0.8061614036560059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1000it [24:16,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1000, Loss-0.7546658515930176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1100it [26:41,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1100, Loss-0.881874144077301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1200it [29:07,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1200, Loss-0.8997511863708496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1220it [29:35,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Loss: 0.8083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-0, Loss-0.7351599335670471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [02:25,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-100, Loss-0.735181450843811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [04:51,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-200, Loss-0.7025976777076721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "300it [07:16,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-300, Loss-0.7172818779945374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400it [09:42,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-400, Loss-0.7566203474998474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "500it [12:07,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-500, Loss-0.6831374168395996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "600it [14:33,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-600, Loss-0.6197997331619263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "700it [16:58,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-700, Loss-0.6673067808151245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "800it [19:24,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-800, Loss-0.5941049456596375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "900it [21:49,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-900, Loss-0.733845591545105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1000it [24:15,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1000, Loss-0.6613959074020386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1100it [26:40,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1100, Loss-0.7154840230941772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1200it [29:06,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1200, Loss-0.7117800116539001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1220it [29:34,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Average Loss: 0.6983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-0, Loss-0.5501241087913513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [02:25,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-100, Loss-0.6431103944778442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [04:51,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-200, Loss-0.6930710077285767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "300it [07:16,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-300, Loss-0.6709518432617188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400it [09:42,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-400, Loss-0.6845566034317017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "500it [12:07,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-500, Loss-0.7604778409004211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "600it [14:33,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-600, Loss-0.5870668292045593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "700it [16:58,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-700, Loss-0.6723702549934387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "800it [19:23,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-800, Loss-0.6351805329322815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "900it [21:49,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-900, Loss-0.6650876998901367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "928it [22:30,  1.45s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the test text\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    test_text,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "omonaQheCCUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test dataset\n",
        "test_dataset = MovieReviewsDataset(test_tokens, [0]*len(test_text))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "IsZ_n0quCYLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        predicted_labels = torch.argmax(probabilities, dim=1)\n",
        "        predictions.extend(predicted_labels.cpu().numpy().tolist())\n"
      ],
      "metadata": {
        "id": "Cq3DE723CZcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to sentiment labels\n",
        "sentiment_labels = [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]\n",
        "sentiment_predictions = [sentiment_labels[prediction] for prediction in predictions]\n"
      ],
      "metadata": {
        "id": "DpZlr5CMCf8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9J8HsZ5CiNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}