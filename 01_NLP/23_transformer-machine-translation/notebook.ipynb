{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_p5VPHEvF_d",
        "outputId": "8f3889b7-2cd6-4f7c-e07f-6a7c6ecefe3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "OYwAz_8vvhe3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#neccessary imports \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import spacy\n",
        "from torch import Tensor\n",
        "!pip install einops --quiet\n",
        "from einops import rearrange\n",
        "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
        "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "!python -m spacy download en_core_web_sm --quiet\n",
        "!python -m spacy download de_core_news_sm --quiet"
      ],
      "metadata": {
        "id": "j9dCZXbsB3l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de75f58d-0b3a-4c8e-8806-573ce8df498f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-14 22:15:58.564164: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 22:15:58.564276: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 22:15:58.564295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-02-14 22:16:19.018838: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 22:16:19.019007: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 22:16:19.019031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/deu.txt',delimiter='\\t',header=None)\n",
        "df.columns = ['English','German','Source']\n"
      ],
      "metadata": {
        "id": "iJrcCreN5LF7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "DbvLt5JWvmIN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the text\n",
        "#turning unicode string to plain ASCII\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "#clean text by converting to lower case, removing non -letter characters\n",
        "def clean_text(text):\n",
        "    text = unicodeToAscii(text.lower().strip())\n",
        "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "    text = re.sub(\"[.!?]\", '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "#applying the clean_text method to df\n",
        "df[\"English\"] = df[\"English\"].apply(clean_text)\n",
        "df[\"German\"] = df[\"German\"].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "FNjMMOrq0cxA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_token = 1\n",
        "end_token =2\n",
        "oov_token=3\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, language):\n",
        "        self.language = language\n",
        "        self.stoi = {}#string2index\n",
        "        self.stoc = {} # string2count\n",
        "        self.itos = {0:\"<PAD>\",start_token:\"<START>\",end_token:\"<END>\"}#index2sting #additional- add ,oov_token:\"<OOV>\"\n",
        "        for k,v in self.itos.items():\n",
        "            self.stoi[v]=k\n",
        "        self.num_words =31\n",
        "        if self.language == \"English\":\n",
        "            self.tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "        elif self.language == \"German\":\n",
        "            self.tokenizer = spacy.load(\"de_core_news_sm\")\n",
        "        else:\n",
        "            print(\"Invalid language\")\n",
        "\n",
        "    def tokenize_sent(self,sentence):\n",
        "        return [token.text.lower() for token in self.tokenizer.tokenizer(sentence)]\n",
        "    \n",
        "    def add_word(self,word):\n",
        "        if word not in self.stoi:\n",
        "            self.stoi[word]=self.num_words\n",
        "            self.stoc[word]=1\n",
        "            self.itos[self.num_words]=word\n",
        "            self.num_words+=1\n",
        "        else:\n",
        "            self.stoc[word]+=1\n",
        "\n",
        "    def build_vocab(self,sentence):\n",
        "        for word in self.tokenize_sent(sentence):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def process_sentence(self,sentence):\n",
        "        \n",
        "        return [self.stoi[token] for token in self.tokenize_sent(sentence)]\n"
      ],
      "metadata": {
        "id": "2boXL8orvulp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building vocab for english and german\n",
        "eng_vocab = Vocabulary(\"English\")\n",
        "ger_vocab = Vocabulary(\"German\")\n",
        "\n",
        "for english_sentence,german_sentence in zip(df[\"English\"].values.tolist(),df[\"German\"].values.tolist()):\n",
        "    eng_vocab.build_vocab(english_sentence)\n",
        "    ger_vocab.build_vocab(german_sentence)"
      ],
      "metadata": {
        "id": "NZkjW3I0v4RB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of English vocab - {}\".format(len(eng_vocab.stoi)))\n",
        "print(\"Length of German vocab - {}\".format(len(ger_vocab.stoi)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqQSqsPJ0oYu",
        "outputId": "d6724659-ae56-4fd7-8ce1-79c9cf772850"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of English vocab - 15607\n",
            "Length of German vocab - 34126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self,dataframe,eng_vocab,ger_vocab):\n",
        "        super(TranslationDataset,self).__init__()\n",
        "        self.dataframe = dataframe\n",
        "        self.eng_vocab = eng_vocab\n",
        "        self.ger_vocab = ger_vocab\n",
        "        self.english = self.dataframe['English'].values.tolist()\n",
        "        self.german = self.dataframe['German'].values.tolist()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def process_sent(self,sent,vocab):\n",
        "        #starting each sentence with start_token and ending with end_token\n",
        "        processed_sent = [vocab.stoi[\"<START>\"]]\n",
        "        processed_sent.extend(vocab.process_sentence(sent))\n",
        "        processed_sent.append(vocab.stoi[\"<END>\"])\n",
        "        return processed_sent\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        processed_eng_sent = self.process_sent(self.english[index],self.eng_vocab)\n",
        "        processed_ger_sent = self.process_sent(self.german[index],self.ger_vocab)\n",
        "\n",
        "        \n",
        "        item = {'input': torch.tensor(processed_eng_sent), 'output':torch.tensor(processed_ger_sent)}\n",
        "        return item"
      ],
      "metadata": {
        "id": "zP4dFSCXwAnh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TranslationDataset(df,eng_vocab,ger_vocab)\n"
      ],
      "metadata": {
        "id": "HXnkAXajwJtD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check of train_dataset\n",
        "for i in range(len(train_dataset)):\n",
        "  train_dataset.__getitem__(i)\n",
        "print(\"No Errors found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTTax2570yyP",
        "outputId": "2ea2d3d3-5d57-4aac-83be-ee46808dba5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Errors found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#collate_function\n",
        "class Collater(object):\n",
        "    def __init__(self, pad_index):\n",
        "        self.pad_index = pad_index\n",
        "\n",
        "    def __call__(self, batch):\n",
        "\n",
        "        input = [item['input'] for item in batch]\n",
        "        output = [item['output'] for item in batch]\n",
        "        input = pad_sequence(input, batch_first=False, padding_value=self.pad_index)\n",
        "        output = pad_sequence(output, batch_first=False, padding_value=self.pad_index)\n",
        "        item = {'input':input, 'output':output}\n",
        "        return item\n",
        "\n"
      ],
      "metadata": {
        "id": "IXA8O0g-wLVC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dataloaders\n",
        "batch_size=2\n",
        "pad_idx = eng_vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size, num_workers=1, shuffle=False,pin_memory=True, collate_fn=Collater(pad_idx))"
      ],
      "metadata": {
        "id": "VJZtsLAcwPgD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "    \n",
        "    print(\"Input batch details \",batch['input'],batch['input'].shape)\n",
        "\n",
        "    print(\"_______________________________\")\n",
        "    print(\"Output batch details\", batch['output'],batch['output'].shape)\n",
        "    print(\"_______________________________\")\n",
        "\n",
        "    if i==10:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKUnFOev02yn",
        "outputId": "c94ed5ea-60d9-4300-ddf0-313b6885a7ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch details  tensor([[ 1,  1],\n",
            "        [31, 32],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [31, 32],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [32, 33],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [33, 35],\n",
            "        [34,  2],\n",
            "        [ 2,  0]]) torch.Size([4, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [33, 34],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [35, 36],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [34, 35],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [37, 38],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [36, 36],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [39, 40],\n",
            "        [ 2, 41],\n",
            "        [ 0,  2]]) torch.Size([4, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [37, 38],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [42, 43],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [38, 39],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [43, 44],\n",
            "        [ 2, 45],\n",
            "        [ 0,  2]]) torch.Size([4, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [31, 41],\n",
            "        [40,  2],\n",
            "        [ 2,  0]]) torch.Size([4, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [46, 32],\n",
            "        [47,  2],\n",
            "        [ 2,  0]]) torch.Size([4, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [42, 42],\n",
            "        [ 2,  2]]) torch.Size([3, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [48, 50],\n",
            "        [49,  2],\n",
            "        [ 2,  0]]) torch.Size([4, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [43, 43],\n",
            "        [44, 44],\n",
            "        [ 2,  2]]) torch.Size([4, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [51, 51],\n",
            "        [52, 54],\n",
            "        [53, 53],\n",
            "        [ 2, 55],\n",
            "        [ 0,  2]]) torch.Size([6, 2])\n",
            "_______________________________\n",
            "Input batch details  tensor([[ 1,  1],\n",
            "        [43, 43],\n",
            "        [45, 46],\n",
            "        [ 2,  2]]) torch.Size([4, 2])\n",
            "_______________________________\n",
            "Output batch details tensor([[ 1,  1],\n",
            "        [51, 51],\n",
            "        [56, 57],\n",
            "        [ 2,  2]]) torch.Size([4, 2])\n",
            "_______________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(eng_vocab.stoi)\n",
        "OUTPUT_DIM = len(ger_vocab.stoi)\n",
        "dim_model = 256\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.english_embedding = nn.Embedding(INPUT_DIM, dim_model)\n",
        "        self.german_embedding = nn.Embedding(OUTPUT_DIM, dim_model)\n",
        "        self.transformer = nn.Transformer(d_model=dim_model, \n",
        "            num_encoder_layers=2, num_decoder_layers=2, \n",
        "            dropout=0.5, dim_feedforward=2048)\n",
        "        self.fc1 = nn.Linear(dim_model, OUTPUT_DIM)\n",
        "    \n",
        "    def forward(self, inputs, targets):\n",
        "        x = self.english_embedding(inputs)\n",
        "        y = self.german_embedding(targets)\n",
        "        tgt_mask = torch.triu(torch.ones(targets.size(0), targets.size(0)), diagonal=1).bool().to(device)\n",
        "        out = self.transformer(x, y, tgt_mask=tgt_mask)\n",
        "        out = self.fc1(out.permute(1, 0, 2)) # (batch, sequence, feature)\n",
        "        return out.permute(1, 0, 2).reshape(-1, OUTPUT_DIM) # (sequence, batch, feature)\n",
        "\n",
        "model = Net().to(device)"
      ],
      "metadata": {
        "id": "5Q7WSzw-wTys"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "step=0\n",
        "writer = SummaryWriter(f'runs/loss_plot')\n",
        "#Init checkpoint\n",
        "checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
        "#Training\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_loader):   \n",
        "            input_seq = batch['input'].to(device)\n",
        "            output_seq = batch['output'].to(device)\n",
        "            pred = model(input_seq.to(device), output_seq[:-1,].to(device))\n",
        "            loss = criterion(pred, output_seq[1:,].view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if step%100==0:\n",
        "              print(\"Epoch- {}, Step-{}, Loss- {}\".format(epoch,step, loss))\n",
        "              torch.save(checkpoint,\"checkpoint.pth.tar\")\n",
        "            writer.add_scalar(\"Train loss\",loss, global_step=step)\n",
        "            step+=1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "87MCwqAWwla0",
        "outputId": "7b20bbdc-8f10-4c06-bb68-0974de2d2f86"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch- 0, Step-0, Loss- 10.543071746826172\n",
            "Epoch- 0, Step-100, Loss- 10.763671875\n",
            "Epoch- 0, Step-200, Loss- 9.161202430725098\n",
            "Epoch- 0, Step-300, Loss- 8.945785522460938\n",
            "Epoch- 0, Step-400, Loss- 12.551543235778809\n",
            "Epoch- 0, Step-500, Loss- 9.107523918151855\n",
            "Epoch- 0, Step-600, Loss- 7.778702259063721\n",
            "Epoch- 0, Step-700, Loss- 12.402889251708984\n",
            "Epoch- 0, Step-800, Loss- 10.04754638671875\n",
            "Epoch- 0, Step-900, Loss- 16.693405151367188\n",
            "Epoch- 0, Step-1000, Loss- 16.928977966308594\n",
            "Epoch- 0, Step-1100, Loss- 15.601746559143066\n",
            "Epoch- 0, Step-1200, Loss- 9.093459129333496\n",
            "Epoch- 0, Step-1300, Loss- 28.988901138305664\n",
            "Epoch- 0, Step-1400, Loss- 24.852602005004883\n",
            "Epoch- 0, Step-1500, Loss- 25.00566864013672\n",
            "Epoch- 0, Step-1600, Loss- 28.026077270507812\n",
            "Epoch- 0, Step-1700, Loss- 16.028865814208984\n",
            "Epoch- 0, Step-1800, Loss- 23.67810821533203\n",
            "Epoch- 0, Step-1900, Loss- 22.488731384277344\n",
            "Epoch- 0, Step-2000, Loss- 25.10645294189453\n",
            "Epoch- 0, Step-2100, Loss- 31.892921447753906\n",
            "Epoch- 0, Step-2200, Loss- 22.596874237060547\n",
            "Epoch- 0, Step-2300, Loss- 26.966867446899414\n",
            "Epoch- 0, Step-2400, Loss- 30.61367416381836\n",
            "Epoch- 0, Step-2500, Loss- 37.71463394165039\n",
            "Epoch- 0, Step-2600, Loss- 45.81415557861328\n",
            "Epoch- 0, Step-2700, Loss- 35.5299072265625\n",
            "Epoch- 0, Step-2800, Loss- 32.96751022338867\n",
            "Epoch- 0, Step-2900, Loss- 40.45465087890625\n",
            "Epoch- 0, Step-3000, Loss- 42.96432113647461\n",
            "Epoch- 0, Step-3100, Loss- 61.00843048095703\n",
            "Epoch- 0, Step-3200, Loss- 62.496681213378906\n",
            "Epoch- 0, Step-3300, Loss- 57.12750244140625\n",
            "Epoch- 0, Step-3400, Loss- 62.30040740966797\n",
            "Epoch- 0, Step-3500, Loss- 48.348262786865234\n",
            "Epoch- 0, Step-3600, Loss- 44.34752655029297\n",
            "Epoch- 0, Step-3700, Loss- 60.870445251464844\n",
            "Epoch- 0, Step-3800, Loss- 59.382137298583984\n",
            "Epoch- 0, Step-3900, Loss- 49.523109436035156\n",
            "Epoch- 0, Step-4000, Loss- 36.367591857910156\n",
            "Epoch- 0, Step-4100, Loss- 43.283878326416016\n",
            "Epoch- 0, Step-4200, Loss- 35.964271545410156\n",
            "Epoch- 0, Step-4300, Loss- 53.77587890625\n",
            "Epoch- 0, Step-4400, Loss- 33.42169952392578\n",
            "Epoch- 0, Step-4500, Loss- 48.24179458618164\n",
            "Epoch- 0, Step-4600, Loss- 46.21739196777344\n",
            "Epoch- 0, Step-4700, Loss- 52.732521057128906\n",
            "Epoch- 0, Step-4800, Loss- 48.78438949584961\n",
            "Epoch- 0, Step-4900, Loss- 64.00752258300781\n",
            "Epoch- 0, Step-5000, Loss- 56.762176513671875\n",
            "Epoch- 0, Step-5100, Loss- 80.17391204833984\n",
            "Epoch- 0, Step-5200, Loss- 62.02912139892578\n",
            "Epoch- 0, Step-5300, Loss- 57.6076774597168\n",
            "Epoch- 0, Step-5400, Loss- 63.0947380065918\n",
            "Epoch- 0, Step-5500, Loss- 60.99443435668945\n",
            "Epoch- 0, Step-5600, Loss- 48.732059478759766\n",
            "Epoch- 0, Step-5700, Loss- 76.36506652832031\n",
            "Epoch- 0, Step-5800, Loss- 62.2834587097168\n",
            "Epoch- 0, Step-5900, Loss- 58.34428787231445\n",
            "Epoch- 0, Step-6000, Loss- 81.71694946289062\n",
            "Epoch- 0, Step-6100, Loss- 57.55149459838867\n",
            "Epoch- 0, Step-6200, Loss- 71.26457214355469\n",
            "Epoch- 0, Step-6300, Loss- 79.80485534667969\n",
            "Epoch- 0, Step-6400, Loss- 83.06727600097656\n",
            "Epoch- 0, Step-6500, Loss- 89.56159973144531\n",
            "Epoch- 0, Step-6600, Loss- 86.64046478271484\n",
            "Epoch- 0, Step-6700, Loss- 75.13646697998047\n",
            "Epoch- 0, Step-6800, Loss- 40.94123840332031\n",
            "Epoch- 0, Step-6900, Loss- 63.4133415222168\n",
            "Epoch- 0, Step-7000, Loss- 113.22128295898438\n",
            "Epoch- 0, Step-7100, Loss- 93.2341537475586\n",
            "Epoch- 0, Step-7200, Loss- 87.1982421875\n",
            "Epoch- 0, Step-7300, Loss- 66.96517944335938\n",
            "Epoch- 0, Step-7400, Loss- 88.23018646240234\n",
            "Epoch- 0, Step-7500, Loss- 53.26243591308594\n",
            "Epoch- 0, Step-7600, Loss- 96.98764038085938\n",
            "Epoch- 0, Step-7700, Loss- 70.45223236083984\n",
            "Epoch- 0, Step-7800, Loss- 62.514678955078125\n",
            "Epoch- 0, Step-7900, Loss- 101.61569213867188\n",
            "Epoch- 0, Step-8000, Loss- 79.79459381103516\n",
            "Epoch- 0, Step-8100, Loss- 110.56343841552734\n",
            "Epoch- 0, Step-8200, Loss- 122.30695343017578\n",
            "Epoch- 0, Step-8300, Loss- 111.981201171875\n",
            "Epoch- 0, Step-8400, Loss- 88.91943359375\n",
            "Epoch- 0, Step-8500, Loss- 102.29864501953125\n",
            "Epoch- 0, Step-8600, Loss- 145.726806640625\n",
            "Epoch- 0, Step-8700, Loss- 124.53501892089844\n",
            "Epoch- 0, Step-8800, Loss- 90.59449768066406\n",
            "Epoch- 0, Step-8900, Loss- 100.71354675292969\n",
            "Epoch- 0, Step-9000, Loss- 70.34033203125\n",
            "Epoch- 0, Step-9100, Loss- 78.30342864990234\n",
            "Epoch- 0, Step-9200, Loss- 63.45201873779297\n",
            "Epoch- 0, Step-9300, Loss- 86.65396118164062\n",
            "Epoch- 0, Step-9400, Loss- 78.71792602539062\n",
            "Epoch- 0, Step-9500, Loss- 88.99791717529297\n",
            "Epoch- 0, Step-9600, Loss- 96.96581268310547\n",
            "Epoch- 0, Step-9700, Loss- 129.28753662109375\n",
            "Epoch- 0, Step-9800, Loss- 93.61433410644531\n",
            "Epoch- 0, Step-9900, Loss- 137.89590454101562\n",
            "Epoch- 0, Step-10000, Loss- 127.44303894042969\n",
            "Epoch- 0, Step-10100, Loss- 110.22032165527344\n",
            "Epoch- 0, Step-10200, Loss- 124.35621643066406\n",
            "Epoch- 0, Step-10300, Loss- 128.97425842285156\n",
            "Epoch- 0, Step-10400, Loss- 105.53975677490234\n",
            "Epoch- 0, Step-10500, Loss- 136.55711364746094\n",
            "Epoch- 0, Step-10600, Loss- 95.51578521728516\n",
            "Epoch- 0, Step-10700, Loss- 72.78224182128906\n",
            "Epoch- 0, Step-10800, Loss- 87.49925994873047\n",
            "Epoch- 0, Step-10900, Loss- 109.68791198730469\n",
            "Epoch- 0, Step-11000, Loss- 127.57268524169922\n",
            "Epoch- 0, Step-11100, Loss- 104.49906158447266\n",
            "Epoch- 0, Step-11200, Loss- 139.04364013671875\n",
            "Epoch- 0, Step-11300, Loss- 226.75213623046875\n",
            "Epoch- 0, Step-11400, Loss- 126.93228912353516\n",
            "Epoch- 0, Step-11500, Loss- 131.3970184326172\n",
            "Epoch- 0, Step-11600, Loss- 135.74420166015625\n",
            "Epoch- 0, Step-11700, Loss- 163.06765747070312\n",
            "Epoch- 0, Step-11800, Loss- 77.93582916259766\n",
            "Epoch- 0, Step-11900, Loss- 153.63027954101562\n",
            "Epoch- 0, Step-12000, Loss- 112.53854370117188\n",
            "Epoch- 0, Step-12100, Loss- 84.98460388183594\n",
            "Epoch- 0, Step-12200, Loss- 171.37930297851562\n",
            "Epoch- 0, Step-12300, Loss- 115.53543853759766\n",
            "Epoch- 0, Step-12400, Loss- 137.3247833251953\n",
            "Epoch- 0, Step-12500, Loss- 175.2138214111328\n",
            "Epoch- 0, Step-12600, Loss- 127.12306213378906\n",
            "Epoch- 0, Step-12700, Loss- 128.50711059570312\n",
            "Epoch- 0, Step-12800, Loss- 127.45865631103516\n",
            "Epoch- 0, Step-12900, Loss- 141.50807189941406\n",
            "Epoch- 0, Step-13000, Loss- 152.46823120117188\n",
            "Epoch- 0, Step-13100, Loss- 115.23419189453125\n",
            "Epoch- 0, Step-13200, Loss- 121.17008209228516\n",
            "Epoch- 0, Step-13300, Loss- 135.0268096923828\n",
            "Epoch- 0, Step-13400, Loss- 140.26841735839844\n",
            "Epoch- 0, Step-13500, Loss- 121.13709259033203\n",
            "Epoch- 0, Step-13600, Loss- 85.76486206054688\n",
            "Epoch- 0, Step-13700, Loss- 119.4460220336914\n",
            "Epoch- 0, Step-13800, Loss- 150.1592254638672\n",
            "Epoch- 0, Step-13900, Loss- 114.75859832763672\n",
            "Epoch- 0, Step-14000, Loss- 110.2086410522461\n",
            "Epoch- 0, Step-14100, Loss- 116.46803283691406\n",
            "Epoch- 0, Step-14200, Loss- 181.2737579345703\n",
            "Epoch- 0, Step-14300, Loss- 140.4315185546875\n",
            "Epoch- 0, Step-14400, Loss- 112.97267150878906\n",
            "Epoch- 0, Step-14500, Loss- 161.7477569580078\n",
            "Epoch- 0, Step-14600, Loss- 143.09613037109375\n",
            "Epoch- 0, Step-14700, Loss- 132.55657958984375\n",
            "Epoch- 0, Step-14800, Loss- 125.5666732788086\n",
            "Epoch- 0, Step-14900, Loss- 177.40756225585938\n",
            "Epoch- 0, Step-15000, Loss- 137.3152618408203\n",
            "Epoch- 0, Step-15100, Loss- 129.86111450195312\n",
            "Epoch- 0, Step-15200, Loss- 136.04649353027344\n",
            "Epoch- 0, Step-15300, Loss- 167.84478759765625\n",
            "Epoch- 0, Step-15400, Loss- 136.87672424316406\n",
            "Epoch- 0, Step-15500, Loss- 147.1180877685547\n",
            "Epoch- 0, Step-15600, Loss- 152.13436889648438\n",
            "Epoch- 0, Step-15700, Loss- 206.3203582763672\n",
            "Epoch- 0, Step-15800, Loss- 197.2681427001953\n",
            "Epoch- 0, Step-15900, Loss- 201.82763671875\n",
            "Epoch- 0, Step-16000, Loss- 187.99453735351562\n",
            "Epoch- 0, Step-16100, Loss- 158.2584228515625\n",
            "Epoch- 0, Step-16200, Loss- 182.95652770996094\n",
            "Epoch- 0, Step-16300, Loss- 111.49468994140625\n",
            "Epoch- 0, Step-16400, Loss- 184.88739013671875\n",
            "Epoch- 0, Step-16500, Loss- 183.70440673828125\n",
            "Epoch- 0, Step-16600, Loss- 176.52723693847656\n",
            "Epoch- 0, Step-16700, Loss- 172.21788024902344\n",
            "Epoch- 0, Step-16800, Loss- 137.57318115234375\n",
            "Epoch- 0, Step-16900, Loss- 107.56734466552734\n",
            "Epoch- 0, Step-17000, Loss- 78.6110610961914\n",
            "Epoch- 0, Step-17100, Loss- 167.29510498046875\n",
            "Epoch- 0, Step-17200, Loss- 149.25340270996094\n",
            "Epoch- 0, Step-17300, Loss- 130.87734985351562\n",
            "Epoch- 0, Step-17400, Loss- 179.74081420898438\n",
            "Epoch- 0, Step-17500, Loss- 151.40493774414062\n",
            "Epoch- 0, Step-17600, Loss- 194.89295959472656\n",
            "Epoch- 0, Step-17700, Loss- 245.495849609375\n",
            "Epoch- 0, Step-17800, Loss- 144.55059814453125\n",
            "Epoch- 0, Step-17900, Loss- 194.78118896484375\n",
            "Epoch- 0, Step-18000, Loss- 183.66998291015625\n",
            "Epoch- 0, Step-18100, Loss- 141.67457580566406\n",
            "Epoch- 0, Step-18200, Loss- 246.64926147460938\n",
            "Epoch- 0, Step-18300, Loss- 146.5587158203125\n",
            "Epoch- 0, Step-18400, Loss- 122.72954559326172\n",
            "Epoch- 0, Step-18500, Loss- 133.12814331054688\n",
            "Epoch- 0, Step-18600, Loss- 128.62088012695312\n",
            "Epoch- 0, Step-18700, Loss- 184.43765258789062\n",
            "Epoch- 0, Step-18800, Loss- 185.83560180664062\n",
            "Epoch- 0, Step-18900, Loss- 233.641845703125\n",
            "Epoch- 0, Step-19000, Loss- 202.11790466308594\n",
            "Epoch- 0, Step-19100, Loss- 166.42698669433594\n",
            "Epoch- 0, Step-19200, Loss- 172.02662658691406\n",
            "Epoch- 0, Step-19300, Loss- 239.4875946044922\n",
            "Epoch- 0, Step-19400, Loss- 188.68585205078125\n",
            "Epoch- 0, Step-19500, Loss- 256.8127746582031\n",
            "Epoch- 0, Step-19600, Loss- 181.81134033203125\n",
            "Epoch- 0, Step-19700, Loss- 127.44293212890625\n",
            "Epoch- 0, Step-19800, Loss- 222.9152374267578\n",
            "Epoch- 0, Step-19900, Loss- 162.0293426513672\n",
            "Epoch- 0, Step-20000, Loss- 208.15081787109375\n",
            "Epoch- 0, Step-20100, Loss- 207.00843811035156\n",
            "Epoch- 0, Step-20200, Loss- 158.46585083007812\n",
            "Epoch- 0, Step-20300, Loss- 196.84213256835938\n",
            "Epoch- 0, Step-20400, Loss- 158.22830200195312\n",
            "Epoch- 0, Step-20500, Loss- 182.94944763183594\n",
            "Epoch- 0, Step-20600, Loss- 179.49969482421875\n",
            "Epoch- 0, Step-20700, Loss- 184.54298400878906\n",
            "Epoch- 0, Step-20800, Loss- 163.5457000732422\n",
            "Epoch- 0, Step-20900, Loss- 216.5072479248047\n",
            "Epoch- 0, Step-21000, Loss- 176.3388214111328\n",
            "Epoch- 0, Step-21100, Loss- 174.4967041015625\n",
            "Epoch- 0, Step-21200, Loss- 213.13052368164062\n",
            "Epoch- 0, Step-21300, Loss- 155.16836547851562\n",
            "Epoch- 0, Step-21400, Loss- 212.5068817138672\n",
            "Epoch- 0, Step-21500, Loss- 171.41969299316406\n",
            "Epoch- 0, Step-21600, Loss- 135.7365264892578\n",
            "Epoch- 0, Step-21700, Loss- 147.51414489746094\n",
            "Epoch- 0, Step-21800, Loss- 160.5281982421875\n",
            "Epoch- 0, Step-21900, Loss- 217.5037384033203\n",
            "Epoch- 0, Step-22000, Loss- 151.6548309326172\n",
            "Epoch- 0, Step-22100, Loss- 191.64974975585938\n",
            "Epoch- 0, Step-22200, Loss- 221.888427734375\n",
            "Epoch- 0, Step-22300, Loss- 289.64569091796875\n",
            "Epoch- 0, Step-22400, Loss- 225.5259552001953\n",
            "Epoch- 0, Step-22500, Loss- 223.3583526611328\n",
            "Epoch- 0, Step-22600, Loss- 205.525634765625\n",
            "Epoch- 0, Step-22700, Loss- 259.911376953125\n",
            "Epoch- 0, Step-22800, Loss- 168.4698028564453\n",
            "Epoch- 0, Step-22900, Loss- 308.995849609375\n",
            "Epoch- 0, Step-23000, Loss- 156.5546875\n",
            "Epoch- 0, Step-23100, Loss- 226.43991088867188\n",
            "Epoch- 0, Step-23200, Loss- 230.88287353515625\n",
            "Epoch- 0, Step-23300, Loss- 183.02552795410156\n",
            "Epoch- 0, Step-23400, Loss- 163.57725524902344\n",
            "Epoch- 0, Step-23500, Loss- 163.54029846191406\n",
            "Epoch- 0, Step-23600, Loss- 169.89749145507812\n",
            "Epoch- 0, Step-23700, Loss- 190.6905517578125\n",
            "Epoch- 0, Step-23800, Loss- 242.3866729736328\n",
            "Epoch- 0, Step-23900, Loss- 251.2299041748047\n",
            "Epoch- 0, Step-24000, Loss- 209.36468505859375\n",
            "Epoch- 0, Step-24100, Loss- 384.42138671875\n",
            "Epoch- 0, Step-24200, Loss- 253.14248657226562\n",
            "Epoch- 0, Step-24300, Loss- 310.99163818359375\n",
            "Epoch- 0, Step-24400, Loss- 168.06336975097656\n",
            "Epoch- 0, Step-24500, Loss- 265.1180114746094\n",
            "Epoch- 0, Step-24600, Loss- 288.9540100097656\n",
            "Epoch- 0, Step-24700, Loss- 265.7396545410156\n",
            "Epoch- 0, Step-24800, Loss- 257.427978515625\n",
            "Epoch- 0, Step-24900, Loss- 315.1787109375\n",
            "Epoch- 0, Step-25000, Loss- 242.40179443359375\n",
            "Epoch- 0, Step-25100, Loss- 160.2101593017578\n",
            "Epoch- 0, Step-25200, Loss- 276.4592590332031\n",
            "Epoch- 0, Step-25300, Loss- 233.01751708984375\n",
            "Epoch- 0, Step-25400, Loss- 266.8296203613281\n",
            "Epoch- 0, Step-25500, Loss- 315.58380126953125\n",
            "Epoch- 0, Step-25600, Loss- 217.43409729003906\n",
            "Epoch- 0, Step-25700, Loss- 248.7255859375\n",
            "Epoch- 0, Step-25800, Loss- 147.42689514160156\n",
            "Epoch- 0, Step-25900, Loss- 211.93919372558594\n",
            "Epoch- 0, Step-26000, Loss- 179.6016387939453\n",
            "Epoch- 0, Step-26100, Loss- 188.34246826171875\n",
            "Epoch- 0, Step-26200, Loss- 221.95071411132812\n",
            "Epoch- 0, Step-26300, Loss- 288.9081115722656\n",
            "Epoch- 0, Step-26400, Loss- 199.940185546875\n",
            "Epoch- 0, Step-26500, Loss- 322.1600646972656\n",
            "Epoch- 0, Step-26600, Loss- 272.4187316894531\n",
            "Epoch- 0, Step-26700, Loss- 382.41094970703125\n",
            "Epoch- 0, Step-26800, Loss- 226.6450653076172\n",
            "Epoch- 0, Step-26900, Loss- 320.91644287109375\n",
            "Epoch- 0, Step-27000, Loss- 157.26480102539062\n",
            "Epoch- 0, Step-27100, Loss- 307.2894592285156\n",
            "Epoch- 0, Step-27200, Loss- 261.2794189453125\n",
            "Epoch- 0, Step-27300, Loss- 361.7904968261719\n",
            "Epoch- 0, Step-27400, Loss- 325.2191467285156\n",
            "Epoch- 0, Step-27500, Loss- 247.71044921875\n",
            "Epoch- 0, Step-27600, Loss- 207.45252990722656\n",
            "Epoch- 0, Step-27700, Loss- 288.98773193359375\n",
            "Epoch- 0, Step-27800, Loss- 196.90672302246094\n",
            "Epoch- 0, Step-27900, Loss- 216.6087646484375\n",
            "Epoch- 0, Step-28000, Loss- 210.71458435058594\n",
            "Epoch- 0, Step-28100, Loss- 280.9681701660156\n",
            "Epoch- 0, Step-28200, Loss- 232.90980529785156\n",
            "Epoch- 0, Step-28300, Loss- 274.1520690917969\n",
            "Epoch- 0, Step-28400, Loss- 203.01292419433594\n",
            "Epoch- 0, Step-28500, Loss- 263.3844299316406\n",
            "Epoch- 0, Step-28600, Loss- 229.0971221923828\n",
            "Epoch- 0, Step-28700, Loss- 246.4804229736328\n",
            "Epoch- 0, Step-28800, Loss- 242.16014099121094\n",
            "Epoch- 0, Step-28900, Loss- 194.9832763671875\n",
            "Epoch- 0, Step-29000, Loss- 263.6329650878906\n",
            "Epoch- 0, Step-29100, Loss- 256.15667724609375\n",
            "Epoch- 0, Step-29200, Loss- 270.1837158203125\n",
            "Epoch- 0, Step-29300, Loss- 174.0714874267578\n",
            "Epoch- 0, Step-29400, Loss- 248.87696838378906\n",
            "Epoch- 0, Step-29500, Loss- 374.5234680175781\n",
            "Epoch- 0, Step-29600, Loss- 234.54823303222656\n",
            "Epoch- 0, Step-29700, Loss- 248.251220703125\n",
            "Epoch- 0, Step-29800, Loss- 237.39706420898438\n",
            "Epoch- 0, Step-29900, Loss- 352.5758972167969\n",
            "Epoch- 0, Step-30000, Loss- 318.8412780761719\n",
            "Epoch- 0, Step-30100, Loss- 256.7138366699219\n",
            "Epoch- 0, Step-30200, Loss- 376.833251953125\n",
            "Epoch- 0, Step-30300, Loss- 226.10354614257812\n",
            "Epoch- 0, Step-30400, Loss- 237.4246063232422\n",
            "Epoch- 0, Step-30500, Loss- 283.1307678222656\n",
            "Epoch- 0, Step-30600, Loss- 387.40509033203125\n",
            "Epoch- 0, Step-30700, Loss- 440.5402526855469\n",
            "Epoch- 0, Step-30800, Loss- 366.5438232421875\n",
            "Epoch- 0, Step-30900, Loss- 290.8533020019531\n",
            "Epoch- 0, Step-31000, Loss- 262.4055480957031\n",
            "Epoch- 0, Step-31100, Loss- 207.3213653564453\n",
            "Epoch- 0, Step-31200, Loss- 346.65777587890625\n",
            "Epoch- 0, Step-31300, Loss- 329.6451416015625\n",
            "Epoch- 0, Step-31400, Loss- 320.11297607421875\n",
            "Epoch- 0, Step-31500, Loss- 242.26959228515625\n",
            "Epoch- 0, Step-31600, Loss- 261.0999450683594\n",
            "Epoch- 0, Step-31700, Loss- 282.66192626953125\n",
            "Epoch- 0, Step-31800, Loss- 266.53729248046875\n",
            "Epoch- 0, Step-31900, Loss- 407.1767883300781\n",
            "Epoch- 0, Step-32000, Loss- 417.4389343261719\n",
            "Epoch- 0, Step-32100, Loss- 277.5013732910156\n",
            "Epoch- 0, Step-32200, Loss- 213.90231323242188\n",
            "Epoch- 0, Step-32300, Loss- 378.68658447265625\n",
            "Epoch- 0, Step-32400, Loss- 412.7197570800781\n",
            "Epoch- 0, Step-32500, Loss- 376.642578125\n",
            "Epoch- 0, Step-32600, Loss- 375.8279724121094\n",
            "Epoch- 0, Step-32700, Loss- 548.92724609375\n",
            "Epoch- 0, Step-32800, Loss- 514.3153686523438\n",
            "Epoch- 0, Step-32900, Loss- 329.3734130859375\n",
            "Epoch- 0, Step-33000, Loss- 450.524169921875\n",
            "Epoch- 0, Step-33100, Loss- 331.96624755859375\n",
            "Epoch- 0, Step-33200, Loss- 336.6084899902344\n",
            "Epoch- 0, Step-33300, Loss- 350.6098327636719\n",
            "Epoch- 0, Step-33400, Loss- 450.1379699707031\n",
            "Epoch- 0, Step-33500, Loss- 316.51373291015625\n",
            "Epoch- 0, Step-33600, Loss- 296.9074401855469\n",
            "Epoch- 0, Step-33700, Loss- 331.8681945800781\n",
            "Epoch- 0, Step-33800, Loss- 332.08111572265625\n",
            "Epoch- 0, Step-33900, Loss- 304.1771545410156\n",
            "Epoch- 0, Step-34000, Loss- 336.2417297363281\n",
            "Epoch- 0, Step-34100, Loss- 338.15362548828125\n",
            "Epoch- 0, Step-34200, Loss- 413.80084228515625\n",
            "Epoch- 0, Step-34300, Loss- 334.4831237792969\n",
            "Epoch- 0, Step-34400, Loss- 334.2199401855469\n",
            "Epoch- 0, Step-34500, Loss- 288.255126953125\n",
            "Epoch- 0, Step-34600, Loss- 258.8921203613281\n",
            "Epoch- 0, Step-34700, Loss- 319.2388000488281\n",
            "Epoch- 0, Step-34800, Loss- 310.6756896972656\n",
            "Epoch- 0, Step-34900, Loss- 213.97654724121094\n",
            "Epoch- 0, Step-35000, Loss- 389.5729064941406\n",
            "Epoch- 0, Step-35100, Loss- 266.9718322753906\n",
            "Epoch- 0, Step-35200, Loss- 243.8453369140625\n",
            "Epoch- 0, Step-35300, Loss- 253.7794189453125\n",
            "Epoch- 0, Step-35400, Loss- 414.87469482421875\n",
            "Epoch- 0, Step-35500, Loss- 351.0852355957031\n",
            "Epoch- 0, Step-35600, Loss- 281.3423767089844\n",
            "Epoch- 0, Step-35700, Loss- 308.7514953613281\n",
            "Epoch- 0, Step-35800, Loss- 412.9651184082031\n",
            "Epoch- 0, Step-35900, Loss- 272.3782043457031\n",
            "Epoch- 0, Step-36000, Loss- 213.02137756347656\n",
            "Epoch- 0, Step-36100, Loss- 321.93243408203125\n",
            "Epoch- 0, Step-36200, Loss- 283.7889709472656\n",
            "Epoch- 0, Step-36300, Loss- 345.5782775878906\n",
            "Epoch- 0, Step-36400, Loss- 391.0488586425781\n",
            "Epoch- 0, Step-36500, Loss- 306.16436767578125\n",
            "Epoch- 0, Step-36600, Loss- 233.06854248046875\n",
            "Epoch- 0, Step-36700, Loss- 310.9529724121094\n",
            "Epoch- 0, Step-36800, Loss- 244.007080078125\n",
            "Epoch- 0, Step-36900, Loss- 333.7727966308594\n",
            "Epoch- 0, Step-37000, Loss- 306.2322082519531\n",
            "Epoch- 0, Step-37100, Loss- 186.09805297851562\n",
            "Epoch- 0, Step-37200, Loss- 309.3601379394531\n",
            "Epoch- 0, Step-37300, Loss- 326.6429443359375\n",
            "Epoch- 0, Step-37400, Loss- 491.9722900390625\n",
            "Epoch- 0, Step-37500, Loss- 300.7254943847656\n",
            "Epoch- 0, Step-37600, Loss- 289.4966125488281\n",
            "Epoch- 0, Step-37700, Loss- 263.6705017089844\n",
            "Epoch- 0, Step-37800, Loss- 388.3146057128906\n",
            "Epoch- 0, Step-37900, Loss- 395.3917541503906\n",
            "Epoch- 0, Step-38000, Loss- 276.8861083984375\n",
            "Epoch- 0, Step-38100, Loss- 376.4638977050781\n",
            "Epoch- 0, Step-38200, Loss- 279.9276123046875\n",
            "Epoch- 0, Step-38300, Loss- 341.9997253417969\n",
            "Epoch- 0, Step-38400, Loss- 230.26690673828125\n",
            "Epoch- 0, Step-38500, Loss- 241.59446716308594\n",
            "Epoch- 0, Step-38600, Loss- 405.5361022949219\n",
            "Epoch- 0, Step-38700, Loss- 413.67034912109375\n",
            "Epoch- 0, Step-38800, Loss- 643.185546875\n",
            "Epoch- 0, Step-38900, Loss- 387.91021728515625\n",
            "Epoch- 0, Step-39000, Loss- 466.6158752441406\n",
            "Epoch- 0, Step-39100, Loss- 442.9768981933594\n",
            "Epoch- 0, Step-39200, Loss- 446.7614440917969\n",
            "Epoch- 0, Step-39300, Loss- 465.701171875\n",
            "Epoch- 0, Step-39400, Loss- 419.7442626953125\n",
            "Epoch- 0, Step-39500, Loss- 376.8134460449219\n",
            "Epoch- 0, Step-39600, Loss- 339.50665283203125\n",
            "Epoch- 0, Step-39700, Loss- 370.5501403808594\n",
            "Epoch- 0, Step-39800, Loss- 393.1890563964844\n",
            "Epoch- 0, Step-39900, Loss- 367.4049987792969\n",
            "Epoch- 0, Step-40000, Loss- 202.25045776367188\n",
            "Epoch- 0, Step-40100, Loss- 342.0838623046875\n",
            "Epoch- 0, Step-40200, Loss- 348.0398254394531\n",
            "Epoch- 0, Step-40300, Loss- 430.0615234375\n",
            "Epoch- 0, Step-40400, Loss- 409.4375305175781\n",
            "Epoch- 0, Step-40500, Loss- 321.1548767089844\n",
            "Epoch- 0, Step-40600, Loss- 338.39202880859375\n",
            "Epoch- 0, Step-40700, Loss- 448.3774719238281\n",
            "Epoch- 0, Step-40800, Loss- 375.63037109375\n",
            "Epoch- 0, Step-40900, Loss- 421.36328125\n",
            "Epoch- 0, Step-41000, Loss- 510.9666442871094\n",
            "Epoch- 0, Step-41100, Loss- 360.3579406738281\n",
            "Epoch- 0, Step-41200, Loss- 419.26123046875\n",
            "Epoch- 0, Step-41300, Loss- 395.0234680175781\n",
            "Epoch- 0, Step-41400, Loss- 315.3147277832031\n",
            "Epoch- 0, Step-41500, Loss- 436.9921875\n",
            "Epoch- 0, Step-41600, Loss- 386.48577880859375\n",
            "Epoch- 0, Step-41700, Loss- 351.83447265625\n",
            "Epoch- 0, Step-41800, Loss- 408.9039001464844\n",
            "Epoch- 0, Step-41900, Loss- 409.3322448730469\n",
            "Epoch- 0, Step-42000, Loss- 351.1840515136719\n",
            "Epoch- 0, Step-42100, Loss- 275.4912414550781\n",
            "Epoch- 0, Step-42200, Loss- 377.2749938964844\n",
            "Epoch- 0, Step-42300, Loss- 293.5345153808594\n",
            "Epoch- 0, Step-42400, Loss- 282.8992614746094\n",
            "Epoch- 0, Step-42500, Loss- 454.83984375\n",
            "Epoch- 0, Step-42600, Loss- 521.4618530273438\n",
            "Epoch- 0, Step-42700, Loss- 408.8014221191406\n",
            "Epoch- 0, Step-42800, Loss- 306.6187744140625\n",
            "Epoch- 0, Step-42900, Loss- 371.1357116699219\n",
            "Epoch- 0, Step-43000, Loss- 219.76438903808594\n",
            "Epoch- 0, Step-43100, Loss- 350.1892395019531\n",
            "Epoch- 0, Step-43200, Loss- 344.9504699707031\n",
            "Epoch- 0, Step-43300, Loss- 369.2028503417969\n",
            "Epoch- 0, Step-43400, Loss- 289.38031005859375\n",
            "Epoch- 0, Step-43500, Loss- 495.7978820800781\n",
            "Epoch- 0, Step-43600, Loss- 412.1553039550781\n",
            "Epoch- 0, Step-43700, Loss- 429.8406677246094\n",
            "Epoch- 0, Step-43800, Loss- 355.2607727050781\n",
            "Epoch- 0, Step-43900, Loss- 375.09613037109375\n",
            "Epoch- 0, Step-44000, Loss- 376.15386962890625\n",
            "Epoch- 0, Step-44100, Loss- 262.20758056640625\n",
            "Epoch- 0, Step-44200, Loss- 433.09326171875\n",
            "Epoch- 0, Step-44300, Loss- 377.2801208496094\n",
            "Epoch- 0, Step-44400, Loss- 457.832763671875\n",
            "Epoch- 0, Step-44500, Loss- 417.6815185546875\n",
            "Epoch- 0, Step-44600, Loss- 352.7891845703125\n",
            "Epoch- 0, Step-44700, Loss- 301.3898010253906\n",
            "Epoch- 0, Step-44800, Loss- 374.2609558105469\n",
            "Epoch- 0, Step-44900, Loss- 331.07562255859375\n",
            "Epoch- 0, Step-45000, Loss- 277.2245178222656\n",
            "Epoch- 0, Step-45100, Loss- 440.4318542480469\n",
            "Epoch- 0, Step-45200, Loss- 411.7868347167969\n",
            "Epoch- 0, Step-45300, Loss- 409.457763671875\n",
            "Epoch- 0, Step-45400, Loss- 361.6410827636719\n",
            "Epoch- 0, Step-45500, Loss- 367.8399658203125\n",
            "Epoch- 0, Step-45600, Loss- 424.16241455078125\n",
            "Epoch- 0, Step-45700, Loss- 316.85205078125\n",
            "Epoch- 0, Step-45800, Loss- 418.65106201171875\n",
            "Epoch- 0, Step-45900, Loss- 359.9869384765625\n",
            "Epoch- 0, Step-46000, Loss- 359.8855895996094\n",
            "Epoch- 0, Step-46100, Loss- 329.4064025878906\n",
            "Epoch- 0, Step-46200, Loss- 342.7353515625\n",
            "Epoch- 0, Step-46300, Loss- 338.382568359375\n",
            "Epoch- 0, Step-46400, Loss- 364.05731201171875\n",
            "Epoch- 0, Step-46500, Loss- 438.353515625\n",
            "Epoch- 0, Step-46600, Loss- 510.5882568359375\n",
            "Epoch- 0, Step-46700, Loss- 530.9151000976562\n",
            "Epoch- 0, Step-46800, Loss- 535.4400024414062\n",
            "Epoch- 0, Step-46900, Loss- 482.4680480957031\n",
            "Epoch- 0, Step-47000, Loss- 606.3135986328125\n",
            "Epoch- 0, Step-47100, Loss- 607.1829833984375\n",
            "Epoch- 0, Step-47200, Loss- 437.86285400390625\n",
            "Epoch- 0, Step-47300, Loss- 402.7393493652344\n",
            "Epoch- 0, Step-47400, Loss- 277.5438232421875\n",
            "Epoch- 0, Step-47500, Loss- 436.1765441894531\n",
            "Epoch- 0, Step-47600, Loss- 546.281494140625\n",
            "Epoch- 0, Step-47700, Loss- 469.68902587890625\n",
            "Epoch- 0, Step-47800, Loss- 538.6253051757812\n",
            "Epoch- 0, Step-47900, Loss- 472.0611267089844\n",
            "Epoch- 0, Step-48000, Loss- 364.7822265625\n",
            "Epoch- 0, Step-48100, Loss- 527.9000244140625\n",
            "Epoch- 0, Step-48200, Loss- 387.7995300292969\n",
            "Epoch- 0, Step-48300, Loss- 526.1948852539062\n",
            "Epoch- 0, Step-48400, Loss- 589.6320190429688\n",
            "Epoch- 0, Step-48500, Loss- 326.6542663574219\n",
            "Epoch- 0, Step-48600, Loss- 454.32989501953125\n",
            "Epoch- 0, Step-48700, Loss- 263.1635437011719\n",
            "Epoch- 0, Step-48800, Loss- 343.3876953125\n",
            "Epoch- 0, Step-48900, Loss- 674.0755004882812\n",
            "Epoch- 0, Step-49000, Loss- 506.8359069824219\n",
            "Epoch- 0, Step-49100, Loss- 470.4102478027344\n",
            "Epoch- 0, Step-49200, Loss- 625.9217529296875\n",
            "Epoch- 0, Step-49300, Loss- 477.6176452636719\n",
            "Epoch- 0, Step-49400, Loss- 679.5845336914062\n",
            "Epoch- 0, Step-49500, Loss- 371.05841064453125\n",
            "Epoch- 0, Step-49600, Loss- 601.4095458984375\n",
            "Epoch- 0, Step-49700, Loss- 334.00244140625\n",
            "Epoch- 0, Step-49800, Loss- 598.8955688476562\n",
            "Epoch- 0, Step-49900, Loss- 557.8716430664062\n",
            "Epoch- 0, Step-50000, Loss- 522.942626953125\n",
            "Epoch- 0, Step-50100, Loss- 438.7817687988281\n",
            "Epoch- 0, Step-50200, Loss- 470.24273681640625\n",
            "Epoch- 0, Step-50300, Loss- 308.1584167480469\n",
            "Epoch- 0, Step-50400, Loss- 266.0747375488281\n",
            "Epoch- 0, Step-50500, Loss- 515.970947265625\n",
            "Epoch- 0, Step-50600, Loss- 596.78857421875\n",
            "Epoch- 0, Step-50700, Loss- 444.32232666015625\n",
            "Epoch- 0, Step-50800, Loss- 617.0277709960938\n",
            "Epoch- 0, Step-50900, Loss- 441.05133056640625\n",
            "Epoch- 0, Step-51000, Loss- 447.7269287109375\n",
            "Epoch- 0, Step-51100, Loss- 777.1171264648438\n",
            "Epoch- 0, Step-51200, Loss- 460.0281066894531\n",
            "Epoch- 0, Step-51300, Loss- 577.1792602539062\n",
            "Epoch- 0, Step-51400, Loss- 636.4906616210938\n",
            "Epoch- 0, Step-51500, Loss- 432.6156311035156\n",
            "Epoch- 0, Step-51600, Loss- 475.0600280761719\n",
            "Epoch- 0, Step-51700, Loss- 402.2884826660156\n",
            "Epoch- 0, Step-51800, Loss- 577.0021362304688\n",
            "Epoch- 0, Step-51900, Loss- 520.1755981445312\n",
            "Epoch- 0, Step-52000, Loss- 422.2393798828125\n",
            "Epoch- 0, Step-52100, Loss- 421.8054504394531\n",
            "Epoch- 0, Step-52200, Loss- 549.0816650390625\n",
            "Epoch- 0, Step-52300, Loss- 457.4034423828125\n",
            "Epoch- 0, Step-52400, Loss- 408.166015625\n",
            "Epoch- 0, Step-52500, Loss- 363.50592041015625\n",
            "Epoch- 0, Step-52600, Loss- 426.2634582519531\n",
            "Epoch- 0, Step-52700, Loss- 373.6758117675781\n",
            "Epoch- 0, Step-52800, Loss- 308.28857421875\n",
            "Epoch- 0, Step-52900, Loss- 480.2636413574219\n",
            "Epoch- 0, Step-53000, Loss- 450.1689758300781\n",
            "Epoch- 0, Step-53100, Loss- 337.4267883300781\n",
            "Epoch- 0, Step-53200, Loss- 402.70770263671875\n",
            "Epoch- 0, Step-53300, Loss- 450.2626647949219\n",
            "Epoch- 0, Step-53400, Loss- 406.2324523925781\n",
            "Epoch- 0, Step-53500, Loss- 532.11279296875\n",
            "Epoch- 0, Step-53600, Loss- 655.6727905273438\n",
            "Epoch- 0, Step-53700, Loss- 404.93414306640625\n",
            "Epoch- 0, Step-53800, Loss- 465.1404724121094\n",
            "Epoch- 0, Step-53900, Loss- 591.0005493164062\n",
            "Epoch- 0, Step-54000, Loss- 262.48486328125\n",
            "Epoch- 0, Step-54100, Loss- 512.6675415039062\n",
            "Epoch- 0, Step-54200, Loss- 357.9385070800781\n",
            "Epoch- 0, Step-54300, Loss- 484.8857727050781\n",
            "Epoch- 0, Step-54400, Loss- 619.030029296875\n",
            "Epoch- 0, Step-54500, Loss- 606.659912109375\n",
            "Epoch- 0, Step-54600, Loss- 576.41259765625\n",
            "Epoch- 0, Step-54700, Loss- 613.6533203125\n",
            "Epoch- 0, Step-54800, Loss- 550.5953979492188\n",
            "Epoch- 0, Step-54900, Loss- 443.8325500488281\n",
            "Epoch- 0, Step-55000, Loss- 561.0462646484375\n",
            "Epoch- 0, Step-55100, Loss- 464.75189208984375\n",
            "Epoch- 0, Step-55200, Loss- 566.47412109375\n",
            "Epoch- 0, Step-55300, Loss- 479.1336364746094\n",
            "Epoch- 0, Step-55400, Loss- 446.8827819824219\n",
            "Epoch- 0, Step-55500, Loss- 533.0265502929688\n",
            "Epoch- 0, Step-55600, Loss- 560.7579956054688\n",
            "Epoch- 0, Step-55700, Loss- 412.1591491699219\n",
            "Epoch- 0, Step-55800, Loss- 525.3738403320312\n",
            "Epoch- 0, Step-55900, Loss- 644.5571899414062\n",
            "Epoch- 0, Step-56000, Loss- 554.859375\n",
            "Epoch- 0, Step-56100, Loss- 467.8377990722656\n",
            "Epoch- 0, Step-56200, Loss- 721.8472900390625\n",
            "Epoch- 0, Step-56300, Loss- 469.0894470214844\n",
            "Epoch- 0, Step-56400, Loss- 549.6957397460938\n",
            "Epoch- 0, Step-56500, Loss- 531.29736328125\n",
            "Epoch- 0, Step-56600, Loss- 594.85595703125\n",
            "Epoch- 0, Step-56700, Loss- 622.2059936523438\n",
            "Epoch- 0, Step-56800, Loss- 683.5115966796875\n",
            "Epoch- 0, Step-56900, Loss- 661.4150390625\n",
            "Epoch- 0, Step-57000, Loss- 458.93658447265625\n",
            "Epoch- 0, Step-57100, Loss- 421.7580871582031\n",
            "Epoch- 0, Step-57200, Loss- 582.3419799804688\n",
            "Epoch- 0, Step-57300, Loss- 675.3092041015625\n",
            "Epoch- 0, Step-57400, Loss- 613.8440551757812\n",
            "Epoch- 0, Step-57500, Loss- 386.9560546875\n",
            "Epoch- 0, Step-57600, Loss- 489.73779296875\n",
            "Epoch- 0, Step-57700, Loss- 728.6810913085938\n",
            "Epoch- 0, Step-57800, Loss- 561.4004516601562\n",
            "Epoch- 0, Step-57900, Loss- 627.9090576171875\n",
            "Epoch- 0, Step-58000, Loss- 519.7623291015625\n",
            "Epoch- 0, Step-58100, Loss- 459.59356689453125\n",
            "Epoch- 0, Step-58200, Loss- 508.5267333984375\n",
            "Epoch- 0, Step-58300, Loss- 352.1691589355469\n",
            "Epoch- 0, Step-58400, Loss- 676.1779174804688\n",
            "Epoch- 0, Step-58500, Loss- 494.66015625\n",
            "Epoch- 0, Step-58600, Loss- 517.5526123046875\n",
            "Epoch- 0, Step-58700, Loss- 512.7931518554688\n",
            "Epoch- 0, Step-58800, Loss- 702.5615844726562\n",
            "Epoch- 0, Step-58900, Loss- 437.85260009765625\n",
            "Epoch- 0, Step-59000, Loss- 595.6024780273438\n",
            "Epoch- 0, Step-59100, Loss- 726.4299926757812\n",
            "Epoch- 0, Step-59200, Loss- 617.2168579101562\n",
            "Epoch- 0, Step-59300, Loss- 620.32568359375\n",
            "Epoch- 0, Step-59400, Loss- 556.117431640625\n",
            "Epoch- 0, Step-59500, Loss- 606.2486572265625\n",
            "Epoch- 0, Step-59600, Loss- 510.0983581542969\n",
            "Epoch- 0, Step-59700, Loss- 461.9200439453125\n",
            "Epoch- 0, Step-59800, Loss- 547.0731201171875\n",
            "Epoch- 0, Step-59900, Loss- 663.755126953125\n",
            "Epoch- 0, Step-60000, Loss- 516.2012939453125\n",
            "Epoch- 0, Step-60100, Loss- 653.0570678710938\n",
            "Epoch- 0, Step-60200, Loss- 721.1224365234375\n",
            "Epoch- 0, Step-60300, Loss- 538.7877197265625\n",
            "Epoch- 0, Step-60400, Loss- 605.1351928710938\n",
            "Epoch- 0, Step-60500, Loss- 737.8180541992188\n",
            "Epoch- 0, Step-60600, Loss- 506.1639709472656\n",
            "Epoch- 0, Step-60700, Loss- 533.2461547851562\n",
            "Epoch- 0, Step-60800, Loss- 457.4664306640625\n",
            "Epoch- 0, Step-60900, Loss- 706.4326171875\n",
            "Epoch- 0, Step-61000, Loss- 495.9022216796875\n",
            "Epoch- 0, Step-61100, Loss- 654.669677734375\n",
            "Epoch- 0, Step-61200, Loss- 547.6782836914062\n",
            "Epoch- 0, Step-61300, Loss- 706.1735229492188\n",
            "Epoch- 0, Step-61400, Loss- 600.9246826171875\n",
            "Epoch- 0, Step-61500, Loss- 666.7706909179688\n",
            "Epoch- 0, Step-61600, Loss- 478.20294189453125\n",
            "Epoch- 0, Step-61700, Loss- 608.4945068359375\n",
            "Epoch- 0, Step-61800, Loss- 769.8509521484375\n",
            "Epoch- 0, Step-61900, Loss- 727.4322509765625\n",
            "Epoch- 0, Step-62000, Loss- 739.488525390625\n",
            "Epoch- 0, Step-62100, Loss- 501.7936096191406\n",
            "Epoch- 0, Step-62200, Loss- 437.7646179199219\n",
            "Epoch- 0, Step-62300, Loss- 502.4462585449219\n",
            "Epoch- 0, Step-62400, Loss- 495.21331787109375\n",
            "Epoch- 0, Step-62500, Loss- 503.7092590332031\n",
            "Epoch- 0, Step-62600, Loss- 538.5992431640625\n",
            "Epoch- 0, Step-62700, Loss- 590.0454711914062\n",
            "Epoch- 0, Step-62800, Loss- 443.31317138671875\n",
            "Epoch- 0, Step-62900, Loss- 434.7540588378906\n",
            "Epoch- 0, Step-63000, Loss- 400.5400695800781\n",
            "Epoch- 0, Step-63100, Loss- 466.00921630859375\n",
            "Epoch- 0, Step-63200, Loss- 897.569580078125\n",
            "Epoch- 0, Step-63300, Loss- 830.0752563476562\n",
            "Epoch- 0, Step-63400, Loss- 475.3145446777344\n",
            "Epoch- 0, Step-63500, Loss- 693.7964477539062\n",
            "Epoch- 0, Step-63600, Loss- 354.364013671875\n",
            "Epoch- 0, Step-63700, Loss- 695.9429931640625\n",
            "Epoch- 0, Step-63800, Loss- 560.3297119140625\n",
            "Epoch- 0, Step-63900, Loss- 593.395751953125\n",
            "Epoch- 0, Step-64000, Loss- 436.4837341308594\n",
            "Epoch- 0, Step-64100, Loss- 559.7675170898438\n",
            "Epoch- 0, Step-64200, Loss- 656.6851806640625\n",
            "Epoch- 0, Step-64300, Loss- 525.3723754882812\n",
            "Epoch- 0, Step-64400, Loss- 525.84619140625\n",
            "Epoch- 0, Step-64500, Loss- 553.77099609375\n",
            "Epoch- 0, Step-64600, Loss- 562.090576171875\n",
            "Epoch- 0, Step-64700, Loss- 714.98828125\n",
            "Epoch- 0, Step-64800, Loss- 1028.3209228515625\n",
            "Epoch- 0, Step-64900, Loss- 643.4683837890625\n",
            "Epoch- 0, Step-65000, Loss- 820.16796875\n",
            "Epoch- 0, Step-65100, Loss- 607.677001953125\n",
            "Epoch- 0, Step-65200, Loss- 677.9141235351562\n",
            "Epoch- 0, Step-65300, Loss- 653.2610473632812\n",
            "Epoch- 0, Step-65400, Loss- 654.4067993164062\n",
            "Epoch- 0, Step-65500, Loss- 563.1784057617188\n",
            "Epoch- 0, Step-65600, Loss- 461.1803283691406\n",
            "Epoch- 0, Step-65700, Loss- 617.9464721679688\n",
            "Epoch- 0, Step-65800, Loss- 572.3820190429688\n",
            "Epoch- 0, Step-65900, Loss- 395.9822998046875\n",
            "Epoch- 0, Step-66000, Loss- 504.2513427734375\n",
            "Epoch- 0, Step-66100, Loss- 565.1622314453125\n",
            "Epoch- 0, Step-66200, Loss- 643.1171264648438\n",
            "Epoch- 0, Step-66300, Loss- 627.86376953125\n",
            "Epoch- 0, Step-66400, Loss- 593.8973999023438\n",
            "Epoch- 0, Step-66500, Loss- 546.6014404296875\n",
            "Epoch- 0, Step-66600, Loss- 874.6605834960938\n",
            "Epoch- 0, Step-66700, Loss- 822.1015625\n",
            "Epoch- 0, Step-66800, Loss- 576.3351440429688\n",
            "Epoch- 0, Step-66900, Loss- 686.604248046875\n",
            "Epoch- 0, Step-67000, Loss- 735.2177734375\n",
            "Epoch- 0, Step-67100, Loss- 479.8233642578125\n",
            "Epoch- 0, Step-67200, Loss- 563.8759765625\n",
            "Epoch- 0, Step-67300, Loss- 747.659912109375\n",
            "Epoch- 0, Step-67400, Loss- 598.8663330078125\n",
            "Epoch- 0, Step-67500, Loss- 382.3767395019531\n",
            "Epoch- 0, Step-67600, Loss- 505.97357177734375\n",
            "Epoch- 0, Step-67700, Loss- 550.2011108398438\n",
            "Epoch- 0, Step-67800, Loss- 730.6573486328125\n",
            "Epoch- 0, Step-67900, Loss- 460.4515686035156\n",
            "Epoch- 0, Step-68000, Loss- 491.7144775390625\n",
            "Epoch- 0, Step-68100, Loss- 576.9186401367188\n",
            "Epoch- 0, Step-68200, Loss- 569.7382202148438\n",
            "Epoch- 0, Step-68300, Loss- 622.9694213867188\n",
            "Epoch- 0, Step-68400, Loss- 476.4051513671875\n",
            "Epoch- 0, Step-68500, Loss- 732.7235107421875\n",
            "Epoch- 0, Step-68600, Loss- 559.0307006835938\n",
            "Epoch- 0, Step-68700, Loss- 790.3923950195312\n",
            "Epoch- 0, Step-68800, Loss- 719.5057983398438\n",
            "Epoch- 0, Step-68900, Loss- 893.8416748046875\n",
            "Epoch- 0, Step-69000, Loss- 684.8443603515625\n",
            "Epoch- 0, Step-69100, Loss- 643.9789428710938\n",
            "Epoch- 0, Step-69200, Loss- 600.4971313476562\n",
            "Epoch- 0, Step-69300, Loss- 705.595458984375\n",
            "Epoch- 0, Step-69400, Loss- 518.3387451171875\n",
            "Epoch- 0, Step-69500, Loss- 549.1795654296875\n",
            "Epoch- 0, Step-69600, Loss- 727.9927978515625\n",
            "Epoch- 0, Step-69700, Loss- 737.1465454101562\n",
            "Epoch- 0, Step-69800, Loss- 857.9411010742188\n",
            "Epoch- 0, Step-69900, Loss- 658.9537353515625\n",
            "Epoch- 0, Step-70000, Loss- 712.0189208984375\n",
            "Epoch- 0, Step-70100, Loss- 653.0162963867188\n",
            "Epoch- 0, Step-70200, Loss- 729.2928466796875\n",
            "Epoch- 0, Step-70300, Loss- 756.7242431640625\n",
            "Epoch- 0, Step-70400, Loss- 576.943603515625\n",
            "Epoch- 0, Step-70500, Loss- 831.2412109375\n",
            "Epoch- 0, Step-70600, Loss- 834.6404418945312\n",
            "Epoch- 0, Step-70700, Loss- 783.2274169921875\n",
            "Epoch- 0, Step-70800, Loss- 718.5928955078125\n",
            "Epoch- 0, Step-70900, Loss- 818.911376953125\n",
            "Epoch- 0, Step-71000, Loss- 474.2037658691406\n",
            "Epoch- 0, Step-71100, Loss- 627.33837890625\n",
            "Epoch- 0, Step-71200, Loss- 667.127685546875\n",
            "Epoch- 0, Step-71300, Loss- 798.7369995117188\n",
            "Epoch- 0, Step-71400, Loss- 667.4273681640625\n",
            "Epoch- 0, Step-71500, Loss- 680.8722534179688\n",
            "Epoch- 0, Step-71600, Loss- 570.1650390625\n",
            "Epoch- 0, Step-71700, Loss- 620.4168701171875\n",
            "Epoch- 0, Step-71800, Loss- 724.160400390625\n",
            "Epoch- 0, Step-71900, Loss- 722.8930053710938\n",
            "Epoch- 0, Step-72000, Loss- 570.7814331054688\n",
            "Epoch- 0, Step-72100, Loss- 645.3084106445312\n",
            "Epoch- 0, Step-72200, Loss- 935.9869995117188\n",
            "Epoch- 0, Step-72300, Loss- 810.515869140625\n",
            "Epoch- 0, Step-72400, Loss- 811.0702514648438\n",
            "Epoch- 0, Step-72500, Loss- 612.9928588867188\n",
            "Epoch- 0, Step-72600, Loss- 826.7357177734375\n",
            "Epoch- 0, Step-72700, Loss- 737.794677734375\n",
            "Epoch- 0, Step-72800, Loss- 763.3341064453125\n",
            "Epoch- 0, Step-72900, Loss- 774.790283203125\n",
            "Epoch- 0, Step-73000, Loss- 781.0376586914062\n",
            "Epoch- 0, Step-73100, Loss- 657.5103759765625\n",
            "Epoch- 0, Step-73200, Loss- 629.7320556640625\n",
            "Epoch- 0, Step-73300, Loss- 820.3717651367188\n",
            "Epoch- 0, Step-73400, Loss- 1169.0694580078125\n",
            "Epoch- 0, Step-73500, Loss- 515.640380859375\n",
            "Epoch- 0, Step-73600, Loss- 812.9366455078125\n",
            "Epoch- 0, Step-73700, Loss- 876.5382690429688\n",
            "Epoch- 0, Step-73800, Loss- 759.5885620117188\n",
            "Epoch- 0, Step-73900, Loss- 466.39385986328125\n",
            "Epoch- 0, Step-74000, Loss- 841.3383178710938\n",
            "Epoch- 0, Step-74100, Loss- 615.2946166992188\n",
            "Epoch- 0, Step-74200, Loss- 481.6927490234375\n",
            "Epoch- 0, Step-74300, Loss- 582.3150634765625\n",
            "Epoch- 0, Step-74400, Loss- 824.3602905273438\n",
            "Epoch- 0, Step-74500, Loss- 433.54278564453125\n",
            "Epoch- 0, Step-74600, Loss- 814.9689331054688\n",
            "Epoch- 0, Step-74700, Loss- 449.21356201171875\n",
            "Epoch- 0, Step-74800, Loss- 726.9049682617188\n",
            "Epoch- 0, Step-74900, Loss- 674.1805419921875\n",
            "Epoch- 0, Step-75000, Loss- 848.2435913085938\n",
            "Epoch- 0, Step-75100, Loss- 809.7492065429688\n",
            "Epoch- 0, Step-75200, Loss- 870.8482055664062\n",
            "Epoch- 0, Step-75300, Loss- 841.0855102539062\n",
            "Epoch- 0, Step-75400, Loss- 786.817138671875\n",
            "Epoch- 0, Step-75500, Loss- 751.4041748046875\n",
            "Epoch- 0, Step-75600, Loss- 453.54840087890625\n",
            "Epoch- 0, Step-75700, Loss- 843.971435546875\n",
            "Epoch- 0, Step-75800, Loss- 694.9273681640625\n",
            "Epoch- 0, Step-75900, Loss- 543.1450805664062\n",
            "Epoch- 0, Step-76000, Loss- 477.9562683105469\n",
            "Epoch- 0, Step-76100, Loss- 822.2689208984375\n",
            "Epoch- 0, Step-76200, Loss- 557.250244140625\n",
            "Epoch- 0, Step-76300, Loss- 632.97314453125\n",
            "Epoch- 0, Step-76400, Loss- 656.948974609375\n",
            "Epoch- 0, Step-76500, Loss- 934.3552856445312\n",
            "Epoch- 0, Step-76600, Loss- 511.5516052246094\n",
            "Epoch- 0, Step-76700, Loss- 407.51763916015625\n",
            "Epoch- 0, Step-76800, Loss- 683.3882446289062\n",
            "Epoch- 0, Step-76900, Loss- 783.3944091796875\n",
            "Epoch- 0, Step-77000, Loss- 640.5340576171875\n",
            "Epoch- 0, Step-77100, Loss- 519.7599487304688\n",
            "Epoch- 0, Step-77200, Loss- 687.4532470703125\n",
            "Epoch- 0, Step-77300, Loss- 711.630615234375\n",
            "Epoch- 0, Step-77400, Loss- 645.8202514648438\n",
            "Epoch- 0, Step-77500, Loss- 763.2831420898438\n",
            "Epoch- 0, Step-77600, Loss- 538.150634765625\n",
            "Epoch- 0, Step-77700, Loss- 964.04443359375\n",
            "Epoch- 0, Step-77800, Loss- 920.3983764648438\n",
            "Epoch- 0, Step-77900, Loss- 1031.6478271484375\n",
            "Epoch- 0, Step-78000, Loss- 906.6828002929688\n",
            "Epoch- 0, Step-78100, Loss- 802.71728515625\n",
            "Epoch- 0, Step-78200, Loss- 697.6201782226562\n",
            "Epoch- 0, Step-78300, Loss- 751.610595703125\n",
            "Epoch- 0, Step-78400, Loss- 884.3309326171875\n",
            "Epoch- 0, Step-78500, Loss- 949.6096801757812\n",
            "Epoch- 0, Step-78600, Loss- 642.7626342773438\n",
            "Epoch- 0, Step-78700, Loss- 725.8839111328125\n",
            "Epoch- 0, Step-78800, Loss- 740.234130859375\n",
            "Epoch- 0, Step-78900, Loss- 967.7353515625\n",
            "Epoch- 0, Step-79000, Loss- 833.2721557617188\n",
            "Epoch- 0, Step-79100, Loss- 778.658935546875\n",
            "Epoch- 0, Step-79200, Loss- 761.3963623046875\n",
            "Epoch- 0, Step-79300, Loss- 1078.197509765625\n",
            "Epoch- 0, Step-79400, Loss- 816.394287109375\n",
            "Epoch- 0, Step-79500, Loss- 893.224853515625\n",
            "Epoch- 0, Step-79600, Loss- 1066.2952880859375\n",
            "Epoch- 0, Step-79700, Loss- 572.3756103515625\n",
            "Epoch- 0, Step-79800, Loss- 785.1141357421875\n",
            "Epoch- 0, Step-79900, Loss- 753.7909545898438\n",
            "Epoch- 0, Step-80000, Loss- 774.000732421875\n",
            "Epoch- 0, Step-80100, Loss- 793.79638671875\n",
            "Epoch- 0, Step-80200, Loss- 886.6337280273438\n",
            "Epoch- 0, Step-80300, Loss- 419.7310485839844\n",
            "Epoch- 0, Step-80400, Loss- 807.4984130859375\n",
            "Epoch- 0, Step-80500, Loss- 1095.1217041015625\n",
            "Epoch- 0, Step-80600, Loss- 1199.1033935546875\n",
            "Epoch- 0, Step-80700, Loss- 921.7659301757812\n",
            "Epoch- 0, Step-80800, Loss- 638.7672119140625\n",
            "Epoch- 0, Step-80900, Loss- 960.816162109375\n",
            "Epoch- 0, Step-81000, Loss- 1074.3502197265625\n",
            "Epoch- 0, Step-81100, Loss- 554.2374267578125\n",
            "Epoch- 0, Step-81200, Loss- 806.1698608398438\n",
            "Epoch- 0, Step-81300, Loss- 929.6937866210938\n",
            "Epoch- 0, Step-81400, Loss- 860.5902099609375\n",
            "Epoch- 0, Step-81500, Loss- 748.4358520507812\n",
            "Epoch- 0, Step-81600, Loss- 825.7618408203125\n",
            "Epoch- 0, Step-81700, Loss- 870.1078491210938\n",
            "Epoch- 0, Step-81800, Loss- 843.4434814453125\n",
            "Epoch- 0, Step-81900, Loss- 761.2967529296875\n",
            "Epoch- 0, Step-82000, Loss- 946.723388671875\n",
            "Epoch- 0, Step-82100, Loss- 764.634521484375\n",
            "Epoch- 0, Step-82200, Loss- 1025.1087646484375\n",
            "Epoch- 0, Step-82300, Loss- 771.7994384765625\n",
            "Epoch- 0, Step-82400, Loss- 1201.5694580078125\n",
            "Epoch- 0, Step-82500, Loss- 802.142578125\n",
            "Epoch- 0, Step-82600, Loss- 788.2965698242188\n",
            "Epoch- 0, Step-82700, Loss- 802.6812133789062\n",
            "Epoch- 0, Step-82800, Loss- 1070.484130859375\n",
            "Epoch- 0, Step-82900, Loss- 665.9186401367188\n",
            "Epoch- 0, Step-83000, Loss- 1017.734619140625\n",
            "Epoch- 0, Step-83100, Loss- 839.0554809570312\n",
            "Epoch- 0, Step-83200, Loss- 966.8182373046875\n",
            "Epoch- 0, Step-83300, Loss- 849.5994262695312\n",
            "Epoch- 0, Step-83400, Loss- 957.5140991210938\n",
            "Epoch- 0, Step-83500, Loss- 916.5233154296875\n",
            "Epoch- 0, Step-83600, Loss- 906.2598876953125\n",
            "Epoch- 0, Step-83700, Loss- 712.6099243164062\n",
            "Epoch- 0, Step-83800, Loss- 937.2692260742188\n",
            "Epoch- 0, Step-83900, Loss- 706.6513061523438\n",
            "Epoch- 0, Step-84000, Loss- 1096.942138671875\n",
            "Epoch- 0, Step-84100, Loss- 1136.344482421875\n",
            "Epoch- 0, Step-84200, Loss- 571.5468139648438\n",
            "Epoch- 0, Step-84300, Loss- 737.3093872070312\n",
            "Epoch- 0, Step-84400, Loss- 937.5081176757812\n",
            "Epoch- 0, Step-84500, Loss- 1162.5958251953125\n",
            "Epoch- 0, Step-84600, Loss- 611.8770141601562\n",
            "Epoch- 0, Step-84700, Loss- 715.8709716796875\n",
            "Epoch- 0, Step-84800, Loss- 459.76080322265625\n",
            "Epoch- 0, Step-84900, Loss- 750.82568359375\n",
            "Epoch- 0, Step-85000, Loss- 828.975341796875\n",
            "Epoch- 0, Step-85100, Loss- 894.9093017578125\n",
            "Epoch- 0, Step-85200, Loss- 585.4822387695312\n",
            "Epoch- 0, Step-85300, Loss- 1255.2794189453125\n",
            "Epoch- 0, Step-85400, Loss- 1063.0989990234375\n",
            "Epoch- 0, Step-85500, Loss- 781.3994750976562\n",
            "Epoch- 0, Step-85600, Loss- 922.1075439453125\n",
            "Epoch- 0, Step-85700, Loss- 663.4301147460938\n",
            "Epoch- 0, Step-85800, Loss- 838.4766235351562\n",
            "Epoch- 0, Step-85900, Loss- 524.2023315429688\n",
            "Epoch- 0, Step-86000, Loss- 904.7905883789062\n",
            "Epoch- 0, Step-86100, Loss- 892.208251953125\n",
            "Epoch- 0, Step-86200, Loss- 835.3132934570312\n",
            "Epoch- 0, Step-86300, Loss- 543.4501342773438\n",
            "Epoch- 0, Step-86400, Loss- 756.2984619140625\n",
            "Epoch- 0, Step-86500, Loss- 802.0972900390625\n",
            "Epoch- 0, Step-86600, Loss- 765.0390625\n",
            "Epoch- 0, Step-86700, Loss- 709.7005615234375\n",
            "Epoch- 0, Step-86800, Loss- 901.3507080078125\n",
            "Epoch- 0, Step-86900, Loss- 724.6721801757812\n",
            "Epoch- 0, Step-87000, Loss- 863.2000122070312\n",
            "Epoch- 0, Step-87100, Loss- 911.6027221679688\n",
            "Epoch- 0, Step-87200, Loss- 695.8659057617188\n",
            "Epoch- 0, Step-87300, Loss- 566.6539306640625\n",
            "Epoch- 0, Step-87400, Loss- 669.6686401367188\n",
            "Epoch- 0, Step-87500, Loss- 604.6028442382812\n",
            "Epoch- 0, Step-87600, Loss- 792.3442993164062\n",
            "Epoch- 0, Step-87700, Loss- 576.782470703125\n",
            "Epoch- 0, Step-87800, Loss- 1025.41845703125\n",
            "Epoch- 0, Step-87900, Loss- 700.5509033203125\n",
            "Epoch- 0, Step-88000, Loss- 841.43798828125\n",
            "Epoch- 0, Step-88100, Loss- 798.5252685546875\n",
            "Epoch- 0, Step-88200, Loss- 841.0904541015625\n",
            "Epoch- 0, Step-88300, Loss- 1003.449951171875\n",
            "Epoch- 0, Step-88400, Loss- 1132.0382080078125\n",
            "Epoch- 0, Step-88500, Loss- 965.434326171875\n",
            "Epoch- 0, Step-88600, Loss- 610.81494140625\n",
            "Epoch- 0, Step-88700, Loss- 926.2191772460938\n",
            "Epoch- 0, Step-88800, Loss- 934.4119873046875\n",
            "Epoch- 0, Step-88900, Loss- 832.17333984375\n",
            "Epoch- 0, Step-89000, Loss- 974.2838134765625\n",
            "Epoch- 0, Step-89100, Loss- 604.8636474609375\n",
            "Epoch- 0, Step-89200, Loss- 954.3938598632812\n",
            "Epoch- 0, Step-89300, Loss- 1012.4468383789062\n",
            "Epoch- 0, Step-89400, Loss- 806.7028198242188\n",
            "Epoch- 0, Step-89500, Loss- 1104.724609375\n",
            "Epoch- 0, Step-89600, Loss- 743.0922241210938\n",
            "Epoch- 0, Step-89700, Loss- 1213.1978759765625\n",
            "Epoch- 0, Step-89800, Loss- 1198.5899658203125\n",
            "Epoch- 0, Step-89900, Loss- 791.74755859375\n",
            "Epoch- 0, Step-90000, Loss- 714.95947265625\n",
            "Epoch- 0, Step-90100, Loss- 771.8536376953125\n",
            "Epoch- 0, Step-90200, Loss- 652.4732666015625\n",
            "Epoch- 0, Step-90300, Loss- 859.753173828125\n",
            "Epoch- 0, Step-90400, Loss- 1001.1926879882812\n",
            "Epoch- 0, Step-90500, Loss- 995.2992553710938\n",
            "Epoch- 0, Step-90600, Loss- 997.6572265625\n",
            "Epoch- 0, Step-90700, Loss- 780.721923828125\n",
            "Epoch- 0, Step-90800, Loss- 878.08056640625\n",
            "Epoch- 0, Step-90900, Loss- 1246.722412109375\n",
            "Epoch- 0, Step-91000, Loss- 1087.549072265625\n",
            "Epoch- 0, Step-91100, Loss- 1016.7303466796875\n",
            "Epoch- 0, Step-91200, Loss- 896.3668212890625\n",
            "Epoch- 0, Step-91300, Loss- 845.8002319335938\n",
            "Epoch- 0, Step-91400, Loss- 950.2872314453125\n",
            "Epoch- 0, Step-91500, Loss- 838.8346557617188\n",
            "Epoch- 0, Step-91600, Loss- 927.432861328125\n",
            "Epoch- 0, Step-91700, Loss- 1008.3590087890625\n",
            "Epoch- 0, Step-91800, Loss- 703.1011962890625\n",
            "Epoch- 0, Step-91900, Loss- 961.2657470703125\n",
            "Epoch- 0, Step-92000, Loss- 849.12060546875\n",
            "Epoch- 0, Step-92100, Loss- 831.0535888671875\n",
            "Epoch- 0, Step-92200, Loss- 1069.72412109375\n",
            "Epoch- 0, Step-92300, Loss- 854.5798950195312\n",
            "Epoch- 0, Step-92400, Loss- 865.88623046875\n",
            "Epoch- 0, Step-92500, Loss- 806.4447021484375\n",
            "Epoch- 0, Step-92600, Loss- 1019.9332275390625\n",
            "Epoch- 0, Step-92700, Loss- 1057.1259765625\n",
            "Epoch- 0, Step-92800, Loss- 1056.468505859375\n",
            "Epoch- 0, Step-92900, Loss- 1155.1224365234375\n",
            "Epoch- 0, Step-93000, Loss- 840.81640625\n",
            "Epoch- 0, Step-93100, Loss- 757.1893920898438\n",
            "Epoch- 0, Step-93200, Loss- 1089.0562744140625\n",
            "Epoch- 0, Step-93300, Loss- 1108.3455810546875\n",
            "Epoch- 0, Step-93400, Loss- 1180.23583984375\n",
            "Epoch- 0, Step-93500, Loss- 849.0062255859375\n",
            "Epoch- 0, Step-93600, Loss- 806.3060913085938\n",
            "Epoch- 0, Step-93700, Loss- 922.6885375976562\n",
            "Epoch- 0, Step-93800, Loss- 836.3228759765625\n",
            "Epoch- 0, Step-93900, Loss- 613.499267578125\n",
            "Epoch- 0, Step-94000, Loss- 892.7562866210938\n",
            "Epoch- 0, Step-94100, Loss- 894.1595458984375\n",
            "Epoch- 0, Step-94200, Loss- 1008.5875854492188\n",
            "Epoch- 0, Step-94300, Loss- 769.0276489257812\n",
            "Epoch- 0, Step-94400, Loss- 631.863037109375\n",
            "Epoch- 0, Step-94500, Loss- 741.611083984375\n",
            "Epoch- 0, Step-94600, Loss- 827.4346923828125\n",
            "Epoch- 0, Step-94700, Loss- 1336.579833984375\n",
            "Epoch- 0, Step-94800, Loss- 1168.3828125\n",
            "Epoch- 0, Step-94900, Loss- 742.39794921875\n",
            "Epoch- 0, Step-95000, Loss- 764.8074951171875\n",
            "Epoch- 0, Step-95100, Loss- 846.2056274414062\n",
            "Epoch- 0, Step-95200, Loss- 751.5105590820312\n",
            "Epoch- 0, Step-95300, Loss- 949.0039672851562\n",
            "Epoch- 0, Step-95400, Loss- 942.2100219726562\n",
            "Epoch- 0, Step-95500, Loss- 973.178466796875\n",
            "Epoch- 0, Step-95600, Loss- 1014.0762939453125\n",
            "Epoch- 0, Step-95700, Loss- 916.5762939453125\n",
            "Epoch- 0, Step-95800, Loss- 723.229736328125\n",
            "Epoch- 0, Step-95900, Loss- 613.8939819335938\n",
            "Epoch- 0, Step-96000, Loss- 732.2997436523438\n",
            "Epoch- 0, Step-96100, Loss- 879.46044921875\n",
            "Epoch- 0, Step-96200, Loss- 1057.63720703125\n",
            "Epoch- 0, Step-96300, Loss- 928.8997802734375\n",
            "Epoch- 0, Step-96400, Loss- 1006.6631469726562\n",
            "Epoch- 0, Step-96500, Loss- 1007.3336181640625\n",
            "Epoch- 0, Step-96600, Loss- 981.86376953125\n",
            "Epoch- 0, Step-96700, Loss- 977.8401489257812\n",
            "Epoch- 0, Step-96800, Loss- 1139.325927734375\n",
            "Epoch- 0, Step-96900, Loss- 1068.611572265625\n",
            "Epoch- 0, Step-97000, Loss- 1183.28564453125\n",
            "Epoch- 0, Step-97100, Loss- 643.66650390625\n",
            "Epoch- 0, Step-97200, Loss- 1120.146484375\n",
            "Epoch- 0, Step-97300, Loss- 950.21630859375\n",
            "Epoch- 0, Step-97400, Loss- 1157.2880859375\n",
            "Epoch- 0, Step-97500, Loss- 1174.822998046875\n",
            "Epoch- 0, Step-97600, Loss- 1221.13623046875\n",
            "Epoch- 0, Step-97700, Loss- 981.6925048828125\n",
            "Epoch- 0, Step-97800, Loss- 896.344482421875\n",
            "Epoch- 0, Step-97900, Loss- 826.1851196289062\n",
            "Epoch- 0, Step-98000, Loss- 1133.580322265625\n",
            "Epoch- 0, Step-98100, Loss- 935.161376953125\n",
            "Epoch- 0, Step-98200, Loss- 1124.8310546875\n",
            "Epoch- 0, Step-98300, Loss- 1273.202392578125\n",
            "Epoch- 0, Step-98400, Loss- 693.2328491210938\n",
            "Epoch- 0, Step-98500, Loss- 999.362548828125\n",
            "Epoch- 0, Step-98600, Loss- 765.924072265625\n",
            "Epoch- 0, Step-98700, Loss- 1126.67138671875\n",
            "Epoch- 0, Step-98800, Loss- 1095.82666015625\n",
            "Epoch- 0, Step-98900, Loss- 1031.515625\n",
            "Epoch- 0, Step-99000, Loss- 1121.8953857421875\n",
            "Epoch- 0, Step-99100, Loss- 1092.63623046875\n",
            "Epoch- 0, Step-99200, Loss- 1038.3682861328125\n",
            "Epoch- 0, Step-99300, Loss- 1032.15283203125\n",
            "Epoch- 0, Step-99400, Loss- 986.5762939453125\n",
            "Epoch- 0, Step-99500, Loss- 711.343994140625\n",
            "Epoch- 0, Step-99600, Loss- 1043.368896484375\n",
            "Epoch- 0, Step-99700, Loss- 955.2595825195312\n",
            "Epoch- 0, Step-99800, Loss- 1084.018798828125\n",
            "Epoch- 0, Step-99900, Loss- 1139.0877685546875\n",
            "Epoch- 0, Step-100000, Loss- 1234.3734130859375\n",
            "Epoch- 0, Step-100100, Loss- 1107.2783203125\n",
            "Epoch- 0, Step-100200, Loss- 780.3665161132812\n",
            "Epoch- 0, Step-100300, Loss- 1274.6412353515625\n",
            "Epoch- 0, Step-100400, Loss- 1122.9488525390625\n",
            "Epoch- 0, Step-100500, Loss- 1043.7532958984375\n",
            "Epoch- 0, Step-100600, Loss- 847.7750244140625\n",
            "Epoch- 0, Step-100700, Loss- 995.41259765625\n",
            "Epoch- 0, Step-100800, Loss- 846.3291625976562\n",
            "Epoch- 0, Step-100900, Loss- 1280.40185546875\n",
            "Epoch- 0, Step-101000, Loss- 874.94873046875\n",
            "Epoch- 0, Step-101100, Loss- 1076.975830078125\n",
            "Epoch- 0, Step-101200, Loss- 1045.843505859375\n",
            "Epoch- 0, Step-101300, Loss- 1037.416015625\n",
            "Epoch- 0, Step-101400, Loss- 1378.8162841796875\n",
            "Epoch- 0, Step-101500, Loss- 829.7662963867188\n",
            "Epoch- 0, Step-101600, Loss- 856.3406982421875\n",
            "Epoch- 0, Step-101700, Loss- 1127.9493408203125\n",
            "Epoch- 0, Step-101800, Loss- 1045.2960205078125\n",
            "Epoch- 0, Step-101900, Loss- 1066.99462890625\n",
            "Epoch- 0, Step-102000, Loss- 1047.0238037109375\n",
            "Epoch- 0, Step-102100, Loss- 963.1583862304688\n",
            "Epoch- 0, Step-102200, Loss- 1015.8479614257812\n",
            "Epoch- 0, Step-102300, Loss- 991.8287353515625\n",
            "Epoch- 0, Step-102400, Loss- 806.8936767578125\n",
            "Epoch- 0, Step-102500, Loss- 1015.3203125\n",
            "Epoch- 0, Step-102600, Loss- 973.7470092773438\n",
            "Epoch- 0, Step-102700, Loss- 987.5975952148438\n",
            "Epoch- 0, Step-102800, Loss- 1365.7750244140625\n",
            "Epoch- 0, Step-102900, Loss- 1099.5986328125\n",
            "Epoch- 0, Step-103000, Loss- 1360.751220703125\n",
            "Epoch- 0, Step-103100, Loss- 811.788818359375\n",
            "Epoch- 0, Step-103200, Loss- 789.8665771484375\n",
            "Epoch- 0, Step-103300, Loss- 1283.3189697265625\n",
            "Epoch- 0, Step-103400, Loss- 1059.400390625\n",
            "Epoch- 0, Step-103500, Loss- 976.232666015625\n",
            "Epoch- 0, Step-103600, Loss- 882.476318359375\n",
            "Epoch- 0, Step-103700, Loss- 823.7048950195312\n",
            "Epoch- 0, Step-103800, Loss- 995.2415161132812\n",
            "Epoch- 0, Step-103900, Loss- 1009.1982421875\n",
            "Epoch- 0, Step-104000, Loss- 1065.9942626953125\n",
            "Epoch- 0, Step-104100, Loss- 936.0016479492188\n",
            "Epoch- 0, Step-104200, Loss- 902.8920288085938\n",
            "Epoch- 0, Step-104300, Loss- 751.7562866210938\n",
            "Epoch- 0, Step-104400, Loss- 812.9734497070312\n",
            "Epoch- 0, Step-104500, Loss- 1134.5924072265625\n",
            "Epoch- 0, Step-104600, Loss- 1145.224853515625\n",
            "Epoch- 0, Step-104700, Loss- 903.4624633789062\n",
            "Epoch- 0, Step-104800, Loss- 1031.2574462890625\n",
            "Epoch- 0, Step-104900, Loss- 795.0380859375\n",
            "Epoch- 0, Step-105000, Loss- 801.226318359375\n",
            "Epoch- 0, Step-105100, Loss- 1091.8677978515625\n",
            "Epoch- 0, Step-105200, Loss- 1136.7191162109375\n",
            "Epoch- 0, Step-105300, Loss- 1059.907470703125\n",
            "Epoch- 0, Step-105400, Loss- 1186.7880859375\n",
            "Epoch- 0, Step-105500, Loss- 1124.24609375\n",
            "Epoch- 0, Step-105600, Loss- 999.3446655273438\n",
            "Epoch- 0, Step-105700, Loss- 1221.40283203125\n",
            "Epoch- 0, Step-105800, Loss- 988.24609375\n",
            "Epoch- 0, Step-105900, Loss- 844.580078125\n",
            "Epoch- 0, Step-106000, Loss- 1043.8095703125\n",
            "Epoch- 0, Step-106100, Loss- 748.206787109375\n",
            "Epoch- 0, Step-106200, Loss- 1114.5189208984375\n",
            "Epoch- 0, Step-106300, Loss- 1189.2381591796875\n",
            "Epoch- 0, Step-106400, Loss- 1220.072021484375\n",
            "Epoch- 0, Step-106500, Loss- 935.4765625\n",
            "Epoch- 0, Step-106600, Loss- 1151.9713134765625\n",
            "Epoch- 0, Step-106700, Loss- 771.4677124023438\n",
            "Epoch- 0, Step-106800, Loss- 917.36328125\n",
            "Epoch- 0, Step-106900, Loss- 872.7979736328125\n",
            "Epoch- 0, Step-107000, Loss- 1187.1455078125\n",
            "Epoch- 0, Step-107100, Loss- 1110.91650390625\n",
            "Epoch- 0, Step-107200, Loss- 957.83740234375\n",
            "Epoch- 0, Step-107300, Loss- 996.93408203125\n",
            "Epoch- 0, Step-107400, Loss- 1100.261962890625\n",
            "Epoch- 0, Step-107500, Loss- 1037.883056640625\n",
            "Epoch- 0, Step-107600, Loss- 936.7988891601562\n",
            "Epoch- 0, Step-107700, Loss- 1093.79443359375\n",
            "Epoch- 0, Step-107800, Loss- 1038.5673828125\n",
            "Epoch- 0, Step-107900, Loss- 1043.7354736328125\n",
            "Epoch- 0, Step-108000, Loss- 1050.939697265625\n",
            "Epoch- 0, Step-108100, Loss- 1311.202392578125\n",
            "Epoch- 0, Step-108200, Loss- 967.390625\n",
            "Epoch- 0, Step-108300, Loss- 1118.2987060546875\n",
            "Epoch- 0, Step-108400, Loss- 881.2598876953125\n",
            "Epoch- 0, Step-108500, Loss- 1237.2518310546875\n",
            "Epoch- 0, Step-108600, Loss- 810.4935302734375\n",
            "Epoch- 0, Step-108700, Loss- 671.2108154296875\n",
            "Epoch- 0, Step-108800, Loss- 968.9366455078125\n",
            "Epoch- 0, Step-108900, Loss- 744.3260498046875\n",
            "Epoch- 0, Step-109000, Loss- 1324.114013671875\n",
            "Epoch- 0, Step-109100, Loss- 1192.6331787109375\n",
            "Epoch- 0, Step-109200, Loss- 816.7658081054688\n",
            "Epoch- 0, Step-109300, Loss- 1092.53076171875\n",
            "Epoch- 0, Step-109400, Loss- 1067.525146484375\n",
            "Epoch- 0, Step-109500, Loss- 984.6312255859375\n",
            "Epoch- 0, Step-109600, Loss- 843.8284301757812\n",
            "Epoch- 0, Step-109700, Loss- 1295.3521728515625\n",
            "Epoch- 0, Step-109800, Loss- 1271.492919921875\n",
            "Epoch- 0, Step-109900, Loss- 1058.81298828125\n",
            "Epoch- 0, Step-110000, Loss- 1050.8546142578125\n",
            "Epoch- 0, Step-110100, Loss- 899.513427734375\n",
            "Epoch- 0, Step-110200, Loss- 1107.1414794921875\n",
            "Epoch- 0, Step-110300, Loss- 841.693115234375\n",
            "Epoch- 0, Step-110400, Loss- 1026.7685546875\n",
            "Epoch- 0, Step-110500, Loss- 1270.1298828125\n",
            "Epoch- 0, Step-110600, Loss- 1144.2099609375\n",
            "Epoch- 0, Step-110700, Loss- 1118.2137451171875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-33b64948f445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moutput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-6f17d9cbb0fd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgerman_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, sequence, feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (sequence, batch, feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[1;32m    148\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    544\u001b[0m     def _sa_block(self, x: Tensor,\n\u001b[1;32m    545\u001b[0m                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n\u001b[0;32m--> 546\u001b[0;31m         x = self.self_attn(x, x, x,\n\u001b[0m\u001b[1;32m    547\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights)\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1168\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5044\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5045\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5046\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5047\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5048\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4736\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4739\u001b[0m             \u001b[0;31m# encoder-decoder attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Stage."
      ],
      "metadata": {
        "id": "Vs8b2urA2nup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "def to_sentence(tokens,PAD_IDX=0):\n",
        "    \"\"\" Convert list of word-index to a sentence \"\"\"\n",
        "    return ' '.join([ger_vocab.itos[x] for x in tokens.squeeze() if x != PAD_IDX])"
      ],
      "metadata": {
        "id": "Ts4Z2ZqExAWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [['Follow me.','Folge mir.'],['I got it.','Ich habe es verstanden.']]\n",
        "test_df = pd.DataFrame(data,columns=['English','German'])"
      ],
      "metadata": {
        "id": "vc7hdAXh7sMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TranslationDataset(test_df,eng_vocab,ger_vocab)\n",
        "batch_size=1\n",
        "pad_idx = eng_vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "test_loader = DataLoader(test_dataset,batch_size, num_workers=1, shuffle=False,pin_memory=True, collate_fn=Collater(pad_idx))"
      ],
      "metadata": {
        "id": "bui9RPhW8sIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    losses = 0.\n",
        "    scores = 0.\n",
        "    model.eval()\n",
        "    for i, (batch) in enumerate(test_loader):\n",
        "        input_seq = batch['input'].to(device)\n",
        "        output_seq = batch['output'].to(device)\n",
        "        target = output_seq[:1]\n",
        "        while len(target) < 50 and target[-1] != eng_vocab['<END>']:\n",
        "            pred = model(input_seq, target)\n",
        "            my_targets = torch.cat((\n",
        "                my_targets, \n",
        "                pred[-1,].argmax().unsqueeze(dim=0).unsqueeze(dim=0).to('cpu')\n",
        "            ))\n",
        "\n",
        "        target_sentence = to_sentence(output_seq[1:-1])\n",
        "        pred_sentence = to_sentence(output_seq[1:-1])\n",
        "        score = bleu_score([pred_sentence.split()], [[target_sentence.split()]])\n",
        "        scores += score\n",
        "        print('Bleu score: {}'.format(score))\n",
        "        print('Original-{},Predicted-{}'.format(target_sentence,pred_sentence))\n",
        "    "
      ],
      "metadata": {
        "id": "3dfIXY5w24no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model()"
      ],
      "metadata": {
        "id": "_0bRMsLF7rXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bk7BCGr9Qxre"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}