{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### About\n","> NER Linking\n","\n","https://www.kaggle.com/datasets/namanj27/ner-dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:25:02.389289Z","iopub.status.busy":"2023-05-08T05:25:02.388923Z","iopub.status.idle":"2023-05-08T05:25:04.099908Z","shell.execute_reply":"2023-05-08T05:25:04.098795Z","shell.execute_reply.started":"2023-05-08T05:25:02.389259Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:25:04.102732Z","iopub.status.busy":"2023-05-08T05:25:04.102005Z","iopub.status.idle":"2023-05-08T05:25:04.115575Z","shell.execute_reply":"2023-05-08T05:25:04.114605Z","shell.execute_reply.started":"2023-05-08T05:25:04.102688Z"},"trusted":true},"outputs":[],"source":["# Define a custom dataset class for the NER data\n","class NERDataset(Dataset):\n","    def __init__(self, filepath):\n","        self.data = pd.read_csv(filepath, encoding='ISO-8859-1').fillna(method='ffill')\n","        \n","        # Create a list of unique words and tags\n","        self.words = list(set(self.data[\"Word\"].values))\n","        self.tags = list(set(self.data[\"Tag\"].values))\n","        \n","        # Create dictionaries for mapping words and tags to integers\n","        self.word2idx = {w: i+1 for i, w in enumerate(self.words)}\n","        self.tag2idx = {t: i for i, t in enumerate(self.tags)}\n","        \n","        self.sentences = self._get_sentences()\n","    \n","    def __len__(self):\n","        return len(self.sentences)\n","    \n","    def __getitem__(self, index):\n","        sentence = self.sentences[index]\n","        words = [word[0] for word in sentence]\n","        tags = [word[1] for word in sentence]\n","        \n","        # Convert words and tags to numerical values using the dictionaries\n","        x = [self.word2idx[w] for w in words]\n","        y = [self.tag2idx[t] for t in tags]\n","        \n","        return torch.tensor(x), torch.tensor(y)\n","    \n","    def _get_sentences(self):\n","        # Group the data by sentence number\n","        grouped = self.data.groupby(\"Sentence #\")\n","        sentences = []\n","        for _, group in grouped:\n","            words = group[\"Word\"].values.tolist()\n","            tags = group[\"Tag\"].values.tolist()\n","            sentence = list(zip(words, tags))\n","            sentences.append(sentence)\n","        return sentences"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:25:04.117670Z","iopub.status.busy":"2023-05-08T05:25:04.116935Z","iopub.status.idle":"2023-05-08T05:25:04.128372Z","shell.execute_reply":"2023-05-08T05:25:04.127028Z","shell.execute_reply.started":"2023-05-08T05:25:04.117612Z"},"trusted":true},"outputs":[],"source":["# Define a simple LSTM-based NER model\n","class NERModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","    \n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, _ = self.lstm(embedded)\n","        output = self.fc(output)\n","        return output"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:25:04.131461Z","iopub.status.busy":"2023-05-08T05:25:04.131102Z","iopub.status.idle":"2023-05-08T05:25:09.664242Z","shell.execute_reply":"2023-05-08T05:25:09.663094Z","shell.execute_reply.started":"2023-05-08T05:25:04.131429Z"},"trusted":true},"outputs":[],"source":["# Define hyperparameters and train the model\n","BATCH_SIZE = 32\n","EMBEDDING_DIM = 32\n","HIDDEN_DIM = 64\n","LEARNING_RATE = 0.1\n","EPOCHS = 10\n","\n","dataset = NERDataset(\"/kaggle/input/ner-dataset/ner_datasetreference.csv\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:25:09.666139Z","iopub.status.busy":"2023-05-08T05:25:09.665669Z","iopub.status.idle":"2023-05-08T05:25:09.677502Z","shell.execute_reply":"2023-05-08T05:25:09.676520Z","shell.execute_reply.started":"2023-05-08T05:25:09.666093Z"},"trusted":true},"outputs":[],"source":["# Split the dataset into training and validation sets\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:25:09.679027Z","iopub.status.busy":"2023-05-08T05:25:09.678703Z","iopub.status.idle":"2023-05-08T05:25:09.687634Z","shell.execute_reply":"2023-05-08T05:25:09.686425Z","shell.execute_reply.started":"2023-05-08T05:25:09.678985Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    x = [item[0] for item in batch]\n","    y = [item[1] for item in batch]\n","    x_lengths = [len(seq) for seq in x]\n","    y_lengths = [len(seq) for seq in y]\n","    \n","    # Pad the sequences to the same length\n","    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n","    y = nn.utils.rnn.pad_sequence(y, batch_first=True)\n","    \n","    # Create a mask to ignore padding values in the loss calculation\n","    x_mask = torch.arange(x.size(1))[None, :] < torch.tensor(x_lengths)[:, None]\n","    y_mask = torch.arange(y.size(1))[None, :] < torch.tensor(y_lengths)[:, None]\n","    \n","    return x, y, x_mask, y_mask\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:25:09.689358Z","iopub.status.busy":"2023-05-08T05:25:09.688967Z","iopub.status.idle":"2023-05-08T05:25:09.697729Z","shell.execute_reply":"2023-05-08T05:25:09.696807Z","shell.execute_reply.started":"2023-05-08T05:25:09.689328Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn = collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:26:24.872077Z","iopub.status.busy":"2023-05-08T05:26:24.871696Z","iopub.status.idle":"2023-05-08T05:26:24.888067Z","shell.execute_reply":"2023-05-08T05:26:24.887147Z","shell.execute_reply.started":"2023-05-08T05:26:24.872049Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 2703, 28529, 16032,  ...,     0,     0,     0],\n","        [ 2223, 19550, 28676,  ...,     0,     0,     0],\n","        [32490,   352, 21606,  ...,     0,     0,     0],\n","        ...,\n","        [24857, 32455, 24002,  ..., 22563,  8646, 11935],\n","        [28898, 18716,   925,  ...,     0,     0,     0],\n","        [27536, 12384,  2268,  ...,     0,     0,     0]]) tensor([[13, 15, 12,  ...,  0,  0,  0],\n","        [12, 12, 12,  ...,  0,  0,  0],\n","        [12, 12, 12,  ...,  0,  0,  0],\n","        ...,\n","        [13, 15, 15,  ...,  2, 12, 12],\n","        [12, 12,  2,  ...,  0,  0,  0],\n","        [12, 10, 12,  ...,  0,  0,  0]])\n"]}],"source":["for batch in train_loader:\n","    print(batch[0],batch[1])\n","    break\n","    "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:25:09.712073Z","iopub.status.busy":"2023-05-08T05:25:09.711253Z","iopub.status.idle":"2023-05-08T05:25:09.732099Z","shell.execute_reply":"2023-05-08T05:25:09.730886Z","shell.execute_reply.started":"2023-05-08T05:25:09.712019Z"},"trusted":true},"outputs":[],"source":["model = NERModel(len(dataset.words) + 1, EMBEDDING_DIM, HIDDEN_DIM, len(dataset.tags))\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:28:38.557515Z","iopub.status.busy":"2023-05-08T05:28:38.557099Z","iopub.status.idle":"2023-05-08T05:35:08.940303Z","shell.execute_reply":"2023-05-08T05:35:08.939221Z","shell.execute_reply.started":"2023-05-08T05:28:38.557483Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Train Loss: 0.1399, Val Loss: 0.1309\n","Epoch 2, Train Loss: 0.1209, Val Loss: 0.1288\n","Epoch 3, Train Loss: 0.1193, Val Loss: 0.1296\n","Epoch 4, Train Loss: 0.1187, Val Loss: 0.1312\n","Epoch 5, Train Loss: 0.1196, Val Loss: 0.1307\n","Epoch 6, Train Loss: 0.1192, Val Loss: 0.1336\n","Epoch 7, Train Loss: 0.1170, Val Loss: 0.1320\n","Epoch 8, Train Loss: 0.1186, Val Loss: 0.1305\n","Epoch 9, Train Loss: 0.1166, Val Loss: 0.1310\n","Epoch 10, Train Loss: 0.1162, Val Loss: 0.1276\n"]}],"source":["for epoch in range(EPOCHS):\n","    train_loss = 0.0\n","    val_loss = 0.0\n","    model.train()\n","    for batch in train_loader:\n","        x, y, x_mask, y_mask = batch\n","        optimizer.zero_grad()\n","        output = model(x)\n","        loss = criterion(output.view(-1, len(dataset.tags)), y.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * x.size(0)\n","        \n","    train_loss /= len(train_dataset)\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            x, y, x_mask, y_mask = batch\n","            output = model(x)\n","            loss = criterion(output.view(-1, len(dataset.tags)), y.view(-1))\n","            val_loss += loss.item() * x.size(0)\n","        val_loss /= len(val_dataset)\n","\n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Evaluating the trained model."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T05:41:52.751160Z","iopub.status.busy":"2023-05-08T05:41:52.750760Z","iopub.status.idle":"2023-05-08T05:41:52.760994Z","shell.execute_reply":"2023-05-08T05:41:52.759695Z","shell.execute_reply.started":"2023-05-08T05:41:52.751127Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['O', 'O', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'I-tim', 'O', 'O', 'O', 'I-tim', 'O']\n"]}],"source":["text = \"This is Suraj and I am doing a demo of the trained NER model\"\n","\n","# Split the text into words\n","words = text.split()\n","\n","# Convert words to numerical values using the word2idx dictionary\n","x = [dataset.word2idx.get(word, 0) for word in words]\n","\n","# Convert the numerical values to a tensor and add a batch dimension\n","x = torch.tensor(x).unsqueeze(0)\n","\n","# Pass the tensor through the model to get the predicted tags\n","model.eval()\n","with torch.no_grad():\n","    output = model(x)\n","    _, predicted_tags = torch.max(output, dim=2)\n","\n","# Convert the predicted tags back to their corresponding tag labels using the idx2tag dictionary\n","predicted_tags = predicted_tags.squeeze().tolist()\n","predicted_labels = [dataset.tags[idx] for idx in predicted_tags]\n","print(predicted_labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
