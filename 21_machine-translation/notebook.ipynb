{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "Machine Translation in PyTorch.\n",
    "\n",
    "English to Hinglish.\n",
    "\n",
    "Dataset - https://www.kaggle.com/datasets/mrutyunjaybiswal/hinge-english-to-hinglish-machine-translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mandatory imports\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/suraj/ClickUp/Jan-Feb/data/synthetic-dataset/train.csv')\n",
    "valid_df = pd.read_csv('/home/suraj/ClickUp/Jan-Feb/data/synthetic-dataset/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Hinglish</th>\n",
       "      <th>Average rating</th>\n",
       "      <th>Disagreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Program module is a file that contains instruc...</td>\n",
       "      <td>माड्यूल, एक संचिका होती है, जिसमें या तो स्रोत...</td>\n",
       "      <td>module , ek program hoti hai , jismen ya to so...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And to Thamud We sent their brother Sali 'h. H...</td>\n",
       "      <td>और (हमने) क़ौमे समूद के पास उनके भाई सालेह को ...</td>\n",
       "      <td>aur hamne aume samood ke pas unke bhaee saleh ...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and, when reminded, do not remember\\n</td>\n",
       "      <td>और जब उन्हें याद दिलाया जाता है, तो वे याद नही...</td>\n",
       "      <td>aur jab unhen yad dilaya jata hai , to ve yad ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you won the TED Prize 2011.\\n</td>\n",
       "      <td>तुम्हें २०११ का टेड प्राइज़ मिल गया है.\\n</td>\n",
       "      <td>tumhen २०११ ka ted prize mil gaya hai\\n</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He gone to Kerodemal College of Delhi Universi...</td>\n",
       "      <td>उन्होंने बाद अध्ययन करने के लिए ये दिल्ली विश्...</td>\n",
       "      <td>unhonne bad science karne ke lie ye delhi univ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  Program module is a file that contains instruc...   \n",
       "1  And to Thamud We sent their brother Sali 'h. H...   \n",
       "2              and, when reminded, do not remember\\n   \n",
       "3                      you won the TED Prize 2011.\\n   \n",
       "4  He gone to Kerodemal College of Delhi Universi...   \n",
       "\n",
       "                                               Hindi  \\\n",
       "0  माड्यूल, एक संचिका होती है, जिसमें या तो स्रोत...   \n",
       "1  और (हमने) क़ौमे समूद के पास उनके भाई सालेह को ...   \n",
       "2  और जब उन्हें याद दिलाया जाता है, तो वे याद नही...   \n",
       "3          तुम्हें २०११ का टेड प्राइज़ मिल गया है.\\n   \n",
       "4  उन्होंने बाद अध्ययन करने के लिए ये दिल्ली विश्...   \n",
       "\n",
       "                                            Hinglish  Average rating  \\\n",
       "0  module , ek program hoti hai , jismen ya to so...               7   \n",
       "1  aur hamne aume samood ke pas unke bhaee saleh ...               6   \n",
       "2  aur jab unhen yad dilaya jata hai , to ve yad ...              10   \n",
       "3            tumhen २०११ ka ted prize mil gaya hai\\n               9   \n",
       "4  unhonne bad science karne ke lie ye delhi univ...               7   \n",
       "\n",
       "   Disagreement  \n",
       "0             6  \n",
       "1             4  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Hinglish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you sure you want to permanently delete th...</td>\n",
       "      <td>क्या आप इन फ़ाइलों को स्थायी रूप से हटाना चाहत...</td>\n",
       "      <td>kya aap in files ko sthayi roop se permanently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Three public meetings were held in Bombay City...</td>\n",
       "      <td>उस अवसर पर बंबई में तीन सभाएं की गयीं।\\n</td>\n",
       "      <td>us avasar par bombay meetings were held ki gay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nominee of the insurance has to be a near rela...</td>\n",
       "      <td>बीमा का नामित व्यक्ति अभिदाता का निकट संबंधी ह...</td>\n",
       "      <td>insurance ka namit vyakti abhidata ka nikat sn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thus there was an inadequate appreciation of t...</td>\n",
       "      <td>इस प्रकार इस महत्वपूर्ण क्षेत्र में तेजी से का...</td>\n",
       "      <td>is prkar is vital sector inadequate appreciati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Arabs laughed at him , and the alchemist l...</td>\n",
       "      <td>अरब सैनिक उसकी बात सुनकर हंस पड़े । &lt;s&gt; उनके स...</td>\n",
       "      <td>arab sainik uski bat sunakar hns pare arabs  s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  Are you sure you want to permanently delete th...   \n",
       "1  Three public meetings were held in Bombay City...   \n",
       "2  Nominee of the insurance has to be a near rela...   \n",
       "3  Thus there was an inadequate appreciation of t...   \n",
       "4  The Arabs laughed at him , and the alchemist l...   \n",
       "\n",
       "                                               Hindi  \\\n",
       "0  क्या आप इन फ़ाइलों को स्थायी रूप से हटाना चाहत...   \n",
       "1           उस अवसर पर बंबई में तीन सभाएं की गयीं।\\n   \n",
       "2  बीमा का नामित व्यक्ति अभिदाता का निकट संबंधी ह...   \n",
       "3  इस प्रकार इस महत्वपूर्ण क्षेत्र में तेजी से का...   \n",
       "4  अरब सैनिक उसकी बात सुनकर हंस पड़े । <s> उनके स...   \n",
       "\n",
       "                                            Hinglish  \n",
       "0  kya aap in files ko sthayi roop se permanently...  \n",
       "1  us avasar par bombay meetings were held ki gay...  \n",
       "2  insurance ka namit vyakti abhidata ka nikat sn...  \n",
       "3  is prkar is vital sector inadequate appreciati...  \n",
       "4  arab sainik uski bat sunakar hns pare arabs  s...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English', 'Hinglish'], dtype='object')\n",
      "Index(['English', 'Hinglish'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#dropping all columns except English and Hinglish in the dataset\n",
    "train_df = train_df.drop(['Hindi','Average rating','Disagreement'],axis=1)\n",
    "valid_df = valid_df.drop(['Hindi'],axis=1)\n",
    "print(train_df.columns)\n",
    "print(valid_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hinglish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Program module is a file that contains instruc...</td>\n",
       "      <td>module , ek program hoti hai , jismen ya to so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And to Thamud We sent their brother Sali 'h. H...</td>\n",
       "      <td>aur hamne aume samood ke pas unke bhaee saleh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and, when reminded, do not remember\\n</td>\n",
       "      <td>aur jab unhen yad dilaya jata hai , to ve yad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you won the TED Prize 2011.\\n</td>\n",
       "      <td>tumhen २०११ ka ted prize mil gaya hai\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He gone to Kerodemal College of Delhi Universi...</td>\n",
       "      <td>unhonne bad science karne ke lie ye delhi univ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>Polar ice caps may melt further and increase t...</td>\n",
       "      <td>large size men polar ki barph pighalne se ocea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>It ' s what turns lead into gold , and makes t...</td>\n",
       "      <td>yahi chakr lead into gold bana deta hai , aur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>The President said the North Eastern Hill Univ...</td>\n",
       "      <td>president ne kaha ki north parvtiy university ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>The violin bow might well have grown out of th...</td>\n",
       "      <td>bahut snbhav hai ki vaylin ka gaj bhi ek chhar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>In fact, clans or families which succeeded in ...</td>\n",
       "      <td>vastav men sardar ka pad jitnevale vnshon ya p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2766 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                English  \\\n",
       "0     Program module is a file that contains instruc...   \n",
       "1     And to Thamud We sent their brother Sali 'h. H...   \n",
       "2                 and, when reminded, do not remember\\n   \n",
       "3                         you won the TED Prize 2011.\\n   \n",
       "4     He gone to Kerodemal College of Delhi Universi...   \n",
       "...                                                 ...   \n",
       "2761  Polar ice caps may melt further and increase t...   \n",
       "2762  It ' s what turns lead into gold , and makes t...   \n",
       "2763  The President said the North Eastern Hill Univ...   \n",
       "2764  The violin bow might well have grown out of th...   \n",
       "2765  In fact, clans or families which succeeded in ...   \n",
       "\n",
       "                                               Hinglish  \n",
       "0     module , ek program hoti hai , jismen ya to so...  \n",
       "1     aur hamne aume samood ke pas unke bhaee saleh ...  \n",
       "2     aur jab unhen yad dilaya jata hai , to ve yad ...  \n",
       "3               tumhen २०११ ka ted prize mil gaya hai\\n  \n",
       "4     unhonne bad science karne ke lie ye delhi univ...  \n",
       "...                                                 ...  \n",
       "2761  large size men polar ki barph pighalne se ocea...  \n",
       "2762  yahi chakr lead into gold bana deta hai , aur ...  \n",
       "2763  president ne kaha ki north parvtiy university ...  \n",
       "2764  bahut snbhav hai ki vaylin ka gaj bhi ek chhar...  \n",
       "2765  vastav men sardar ka pad jitnevale vnshon ya p...  \n",
       "\n",
       "[2766 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the dataframe\n",
    "train_df.dropna(inplace=True)\n",
    "valid_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the text\n",
    "#turning unicode string to plain ASCII\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "#clean text by converting to lower case, removing non -letter characters\n",
    "def clean_text(text):\n",
    "    text = unicodeToAscii(text.lower().strip())\n",
    "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    text = re.sub(\"[.!?]\", '', text)\n",
    "    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the clean_text method to df\n",
    "train_df[\"English\"] = train_df[\"English\"].apply(clean_text)\n",
    "train_df[\"Hinglish\"] = train_df[\"Hinglish\"].apply(clean_text)\n",
    "\n",
    "valid_df[\"English\"] = valid_df[\"English\"].apply(clean_text)\n",
    "valid_df[\"Hinglish\"] = valid_df[\"Hinglish\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hinglish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>program module is a file that contains instruc...</td>\n",
       "      <td>module ek program hoti hai jismen ya to source...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and to thamud we sent their brother sali h he ...</td>\n",
       "      <td>aur hamne aume samood ke pas unke bhaee saleh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and when reminded do not remember</td>\n",
       "      <td>aur jab unhen yad dilaya jata hai to ve yad na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you won the ted prize</td>\n",
       "      <td>tumhen ka ted prize mil gaya hai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he gone to kerodemal college of delhi universi...</td>\n",
       "      <td>unhonne bad science karne ke lie ye delhi univ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  program module is a file that contains instruc...   \n",
       "1  and to thamud we sent their brother sali h he ...   \n",
       "2                  and when reminded do not remember   \n",
       "3                             you won the ted prize    \n",
       "4  he gone to kerodemal college of delhi universi...   \n",
       "\n",
       "                                            Hinglish  \n",
       "0  module ek program hoti hai jismen ya to source...  \n",
       "1  aur hamne aume samood ke pas unke bhaee saleh ...  \n",
       "2  aur jab unhen yad dilaya jata hai to ve yad na...  \n",
       "3                   tumhen ka ted prize mil gaya hai  \n",
       "4  unhonne bad science karne ke lie ye delhi univ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hinglish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>are you sure you want to permanently delete th...</td>\n",
       "      <td>kya aap in files ko sthayi roop se permanently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>three public meetings were held in bombay city...</td>\n",
       "      <td>us avasar par bombay meetings were held ki gayin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nominee of the insurance has to be a near rela...</td>\n",
       "      <td>insurance ka namit vyakti abhidata ka nikat sn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thus there was an inadequate appreciation of t...</td>\n",
       "      <td>is prkar is vital sector inadequate appreciati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the arabs laughed at him and the alchemist lau...</td>\n",
       "      <td>arab sainik uski bat sunakar hns pare arabs s ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  are you sure you want to permanently delete th...   \n",
       "1  three public meetings were held in bombay city...   \n",
       "2  nominee of the insurance has to be a near rela...   \n",
       "3  thus there was an inadequate appreciation of t...   \n",
       "4  the arabs laughed at him and the alchemist lau...   \n",
       "\n",
       "                                            Hinglish  \n",
       "0  kya aap in files ko sthayi roop se permanently...  \n",
       "1  us avasar par bombay meetings were held ki gayin   \n",
       "2  insurance ka namit vyakti abhidata ka nikat sn...  \n",
       "3  is prkar is vital sector inadequate appreciati...  \n",
       "4  arab sainik uski bat sunakar hns pare arabs s ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As per https://www.kaggle.com/code/zeyadkhalid/machine-translation-transformers\n",
    "# We have to define word indexing\n",
    "# string_to_index  - string to its associated index i.e stoi\n",
    "# index_to_string - index to its string i.e itos\n",
    "\n",
    "\n",
    "start_token = 1\n",
    "end_token =2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "        self.stoi = {}#string2index\n",
    "        self.stoc = {} # string2count\n",
    "        self.itos = {0:\"<PAD>\",start_token:\"<START>\",end_token:\"<END>\"}#index2sting\n",
    "        for k,v in self.itos.items():\n",
    "            self.stoi[v]=k\n",
    "        self.num_words =3\n",
    "    \n",
    "    def add_word(self,word):\n",
    "        if word not in self.stoi:\n",
    "            self.stoi[word]=self.num_words\n",
    "            self.stoc[word]=1\n",
    "            self.itos[self.num_words]=word\n",
    "            self.num_words+=1\n",
    "        else:\n",
    "            self.stoc[word]+=1\n",
    "\n",
    "    def process_sentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = Vocabulary('English')\n",
    "hinglish_vocab = Vocabulary('Hinglish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "# while training, we shall need pair of input and output tensors whose ending is marked by end_token\n",
    "start_token=1\n",
    "end_token=2\n",
    "\n",
    "#Step 1\n",
    "#Helper function for creating indexes from sentence\n",
    "def index_from_sentence(language,sentence):\n",
    "    return [language.stoi[word] for word in sentence.split(' ')]\n",
    "# [Hi There]  - [H, I, T, H, E, R, E] - [4,5,8,3,6,4]\n",
    "\n",
    "def tensor_from_sentence(language,sentence):\n",
    "    indices = index_from_sentence(language,sentence)\n",
    "    indices.append(end_token)\n",
    "    return torch.tensor(indices, dtype=torch.long).view(1,-1)\n",
    "\n",
    "# step2- create tensor dataset by padding\n",
    "def tensor_from_dataset(pair, input_language, output_language, max_input_len):\n",
    "    input_tensor = tensor_from_sentence(input_language,pair[0])\n",
    "    output_tensor = tensor_from_sentence(output_language, pair[1])\n",
    "    with torch.no_grad():\n",
    "        #padding\n",
    "        pad_input = nn.ConstantPad1d((0,max_input_len-input_tensor.shape[1]),0)\n",
    "        pad_output = nn.ConstantPad1d((0,max_input_len-output_tensor.shape[1]),0)\n",
    "\n",
    "        #applying padding\n",
    "        input_tensor_padded = pad_input(input_tensor)\n",
    "        output_tensor_padded = pad_output(output_tensor)\n",
    "    pair_tensor = pad_sequence([input_tensor_padded,output_tensor_padded],batch_first=False, padding_value=0)\n",
    "\n",
    "    return pair_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_hinglish_tensor = tensor_from_sentence(hinglish_vocab,\"kya aap in files ko sthayi\")\n",
    "# print(sample_hinglish_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(self,dataframe,english_vocab,hinglish_vocab,transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.english = self.dataframe['English'].values.tolist()\n",
    "        self.hinglish = self.dataframe['Hinglish'].values.tolist()\n",
    "        self.input_lang = english_vocab\n",
    "        self.output_lang = hinglish_vocab\n",
    "        #building vocabulary\n",
    "        for english_sent in self.english:\n",
    "            self.input_lang.process_sentence(english_sent)\n",
    "        for hinglish_sent in self.hinglish:\n",
    "            self.output_lang.process_sentence(hinglish_sent)\n",
    "        # creating tensors\n",
    "        self.hinglish_tensors =[tensor_from_sentence(hinglish_vocab,sentence) for sentence in self.hinglish]\n",
    "        self.english_tensors = [tensor_from_sentence(english_vocab,sentence) for sentence in self.english]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        hinglish_sample = self.hinglish_tensors[index][0]\n",
    "        english_sample = self.english_tensors[index][0]\n",
    "        sample = {'input':english_sample,'output':hinglish_sample}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_df,english_vocab,hinglish_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
       "         42, 43, 44, 45, 33, 46, 39, 47, 48, 49, 17, 15, 50, 24, 51, 52, 39, 53,\n",
       "         14, 54, 55, 56, 45, 25, 57, 39, 24, 58, 59, 60, 45, 14, 61, 62, 35, 63,\n",
       "          5, 64, 65, 25, 66, 23,  2]),\n",
       " 'output': tensor([23, 24, 25, 26, 15, 27, 28, 29, 30, 31, 32, 33, 34, 10, 35, 36, 37, 38,\n",
       "         39, 40, 41, 37, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "         56, 44, 57, 38, 58, 59, 23, 55, 60, 61, 10, 62, 63, 44, 64, 65, 66, 67,\n",
       "         68, 17, 69, 46, 70, 71, 72, 73, 74, 15, 75, 23, 76, 77, 23, 64, 78, 79,\n",
       "          7,  2])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TranslationDataset(valid_df,english_vocab,hinglish_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84, 27, 85, 86, 25, 87, 11, 17, 15, 88, 89, 90, 91, 92, 15, 93, 17, 29,\n",
      "        94, 23,  2]) torch.Size([21]) tensor([ 66,  24,  81,  34, 110, 111, 112,  44, 113, 114, 115,  38, 116,  82,\n",
      "        117,   7,  44, 118, 119, 120,  85, 121,   2]) torch.Size([23])\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset.__getitem__(5)\n",
    "print(item['input'], item['input'].shape, item['output'],item['output'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6684 10166\n"
     ]
    }
   ],
   "source": [
    "print(len(english_vocab.stoi),len(hinglish_vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate_function\n",
    "class Collater(object):\n",
    "    def __init__(self, pad_index):\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        input = [item['input'] for item in batch]\n",
    "        output = [item['output'] for item in batch]\n",
    "        input = pad_sequence(input, batch_first=False, padding_value=self.pad_index)\n",
    "        output = pad_sequence(output, batch_first=False, padding_value=self.pad_index)\n",
    "\n",
    "        item = {'input':input, 'output':output}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = english_vocab.stoi[\"<PAD>\"]\n",
    "train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True, num_workers=4, pin_memory=True,collate_fn=Collater(pad_idx))#, collate_fn=Collater(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 4]) torch.Size([53, 4])\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(train_loader):\n",
    "    print(item['input'].shape, item['output'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture  #partially referenced from Aladdin pearson seq2seq model\n",
    "# 1. Encoder \n",
    "# It generates a single output vector that summarises the input sequence meaning\n",
    "# steps\n",
    "# a. a word is fed to a network that generates an output and hidden state,\n",
    "# b. The hidden state is fed to thenext word and process continues for updating weights.\n",
    "# c. Last output also known as context vector is the representative of input sequence\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size,embedding_size,hidden_size,num_layers,prob):\n",
    "        super().__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(prob)\n",
    "        #generating embeddings\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,num_layers,prob)\n",
    "\n",
    "    def forward(self,inputs):#input - longtensor of indexes , shape - seq_len,N\n",
    "        embedding = self.dropout(self.embedding(inputs))\n",
    "        #embedding shape, seq_len,N,embedding_size - each N mapped to an embedding size \n",
    "        outputs,(hidden_state,cell_state) = self.lstm(embedding)\n",
    "        #context vector lies in hidden state and cell state\n",
    "        return hidden_state,cell_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 . Decoder \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The context representative vector shall be used as initial hidden state with following steps\n",
    "    1. At each step,an input token and hidden state is fed to the network. start_token = initiallyy\n",
    "    The firt hidden is context vector og encoder\n",
    "    2. The first output should be first sentence of the output \n",
    "    3. Output ends with end_token or at max_len termination\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size,embedding_size, hidden_size,output_size,num_layers,prob):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(prob)\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=prob)\n",
    "        self.linear = nn.Linear(hidden_size,output_size) # out_size - lenght of language vocab\n",
    "\n",
    "    def forward(self,inputs,hidden_state, cell_state):\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(input))\n",
    "        outputs, (hidden_state,cell_state) = self.lstm(embedding,(hidden_state,cell_state))\n",
    "        output = self.linear(outputs) #1,N, length_ of vovcab\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        return output,hidden_state,cell_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Linear Class which takes sequence and outputs sequence\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self,source, target,teacher_force_ratio=0.4): #source - english , target -correct hinglish, \n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        batch_size = self.source.shape[1]\n",
    "        target_len = self.target.shape[0]\n",
    "        \n",
    "        \n",
    "        target_vocab_size = len(hinglish_vocab.stoi)\n",
    "        outputs = torch.zeros(target_len,batch_size,target_vocab_size).to(device)\n",
    "        hidden_state,cell_state = self.encoder(self.source)\n",
    "\n",
    "        #grabbing start token\n",
    "        x = self.target[0]\n",
    "        for t in range(1,target_len):\n",
    "            output,hidden_state,cell_state = self.decoder(x,hidden_state,cell_state)\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random()< teacher_force_ratio else best_guess #feeding the guess or the original target\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "num_epochs=10\n",
    "lr=1e-2\n",
    "batch_size=32\n",
    "\n",
    "load_model = False\n",
    "input_size_encoder =  len(english_vocab.stoi)\n",
    "input_size_decoder = len(hinglish_vocab.stoi)\n",
    "output_size = len(hinglish_vocab.stoi)\n",
    "encoder_embedding_size=512\n",
    "decoder_embedding_size=512\n",
    "hidden_size=2048\n",
    "num_layers=4\n",
    "\n",
    "encoder_dropout=0.5\n",
    "decoder_dropout=0.5\n",
    "\n",
    "#tensorboard \n",
    "writer = SummaryWriter(f'runs/loss_plot')\n",
    "step=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising the encoder and decoder nets\n",
    "encoder = Encoder(input_size_encoder,encoder_embedding_size,hidden_size,num_layers,encoder_dropout).to(device)\n",
    "decoder = Decoder(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_layers, decoder_dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(encoder,decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(6684, 512)\n",
       "    (rnn): LSTM(512, 2048, num_layers=4, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(10166, 512)\n",
       "    (rnn): LSTM(512, 2048, num_layers=4, dropout=0.5)\n",
       "    (fc): Linear(in_features=2048, out_features=10166, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n",
      "torch.Size([36, 4])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (140) to match target batch_size (36).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35759/4246177936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (140) to match target batch_size (36)."
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
    "    torch.save(checkpoint,\"checkpoint.pth.tar\")\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        #input = torch.LongTensor(input)\n",
    "        #output = torch.LongTensor(output)\n",
    "        inputs = batch['input'].to(device)\n",
    "        outputs = batch['output'].to(device)\n",
    "        #input = input.permute(0,2,1)\n",
    "        #output = output.permute(0,2,1)\n",
    "        print(inputs.shape)\n",
    "        print(outputs.shape)\n",
    "        \n",
    "        out = model(inputs,outputs)\n",
    "        out = out[1:].reshape(-1,out.shape[2])\n",
    "        output = output[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out,outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"Train loss\",loss, global_step=step)\n",
    "        step+=1\n",
    "    print(\"Epoch- {}, Loss - {}\".format(epoch,loss.item()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "To Do\n",
    "1. Write Bleu score methods.\n",
    "2. Predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc43966dd8a35b9bb4dacfb26d54ec70461d2f8773a70bf315d67d5e8c2bf14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
