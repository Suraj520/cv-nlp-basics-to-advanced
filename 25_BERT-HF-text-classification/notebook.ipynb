{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXs2N2FZun7Y"
      },
      "source": [
        "#### Notebook\n",
        "Text Classification using BERT(HuggingFace Transformers) in PyTorch. In this, We shall leverage a pre-trained BERT model from HuggingFace\n",
        "\n",
        "Dataset Link - https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sllGFyp-un7c"
      },
      "outputs": [],
      "source": [
        "#neccessary imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob \n",
        "import io\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install transformers --quiet\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gVAGZ-rvAh7",
        "outputId": "1a25b854-44d2-4549-bf69-5ad8f291b180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUn_1YQMupFj"
      },
      "outputs": [],
      "source": [
        "# os.chdir('/content/drive/MyDrive/Datasets')\n",
        "# !unzip archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jQLHMGcWun7f"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Datasets/bbc-fulltext (document classification)/bbc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTtnac8uun7f",
        "outputId": "7e775a4e-76d6-4562-dd0f-ee40c488367d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/Datasets/bbc-fulltext (document classification)/bbc', '/content/drive/MyDrive/Datasets/bbc-fulltext (document classification)/bbc/business', '/content/drive/MyDrive/Datasets/bbc-fulltext (document classification)/bbc/entertainment', '/content/drive/MyDrive/Datasets/bbc-fulltext (document classification)/bbc/politics', '/content/drive/MyDrive/Datasets/bbc-fulltext (document classification)/bbc/sport', '/content/drive/MyDrive/Datasets/bbc-fulltext (document classification)/bbc/tech']\n"
          ]
        }
      ],
      "source": [
        "dir_path = []\n",
        "for dirname,_, filenames in os.walk(dataset_path):\n",
        "    dir_path.append(dirname)\n",
        "\n",
        "print(dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y8ataw2Mun7h"
      },
      "outputs": [],
      "source": [
        "#converting text to dataframe\n",
        "def text_to_df(dataset_path):\n",
        "    #creating a dataframe\n",
        "    df = pd.DataFrame(columns=['NEWS','CATEGORY'])\n",
        "    text,label = [],[]\n",
        "    for dir_path in dataset_path:\n",
        "        text_files_path = sorted(glob.glob(os.path.join(dir_path,\"*.txt\")))\n",
        "        for text_path in text_files_path:\n",
        "            with io.open(text_path,'r',encoding='utf-8', errors='ignore') as txt_file:\n",
        "                text.append(txt_file.read())\n",
        "                label.append(dir_path.split('/')[-1])\n",
        "\n",
        "    df['NEWS']=text\n",
        "    df['CATEGORY']=label\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jZaUEM84un7i"
      },
      "outputs": [],
      "source": [
        "df = text_to_df(dir_path[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "l5hUKJXIun7i",
        "outputId": "b7d85848-eb7c-40ae-ae41-ef644ac5aecb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-33607295-523a-454a-9629-e1d2c307a318\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NEWS</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>Be careful how you code\\n\\nA new European dire...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>Losing yourself in online gaming\\n\\nOnline rol...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33607295-523a-454a-9629-e1d2c307a318')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33607295-523a-454a-9629-e1d2c307a318 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33607295-523a-454a-9629-e1d2c307a318');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   NEWS  CATEGORY\n",
              "0     Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
              "1     Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
              "2     Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
              "3     High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
              "4     Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
              "...                                                 ...       ...\n",
              "2220  BT program to beat dialler scams\\n\\nBT is intr...      tech\n",
              "2221  Spam e-mails tempt net shoppers\\n\\nComputer us...      tech\n",
              "2222  Be careful how you code\\n\\nA new European dire...      tech\n",
              "2223  US cyber security chief resigns\\n\\nThe man mak...      tech\n",
              "2224  Losing yourself in online gaming\\n\\nOnline rol...      tech\n",
              "\n",
              "[2225 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zRrAVmr-un7j"
      },
      "outputs": [],
      "source": [
        "news = df['NEWS'].values.tolist()\n",
        "category = df['CATEGORY'].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzFJQTaNun7k"
      },
      "source": [
        "#### About BERT\n",
        "1. BERT represents Bidirectional Encoder Representations from Transformers.\n",
        "2. BERT architecture consists of several Transformer encoders stacked together where each transformer encoder is comprised of two sub layers - self attention and a feed forward layer.\n",
        "3. There are two flavours of BERT models - BERT base and BERT large.\n",
        "4. BERT base consists of 12 transformer encoders, 12 attention head and 768 hidden size.\n",
        "5. BERT large conssits fo 24 transformer encoders, 16 attention heads and 1024 hidden size.\n",
        "6. BERT's powerful language model since it's trained on unlabeled data extracted from BooksCorpus and from Wikipedia.\n",
        "7. BERT learns from sequence of words from Left to right and right to left.\n",
        "8. BERT model's input is a sequence of tokens where each token has\n",
        " > [CLS] token which is the beginning of each token. It mean Classification token.\n",
        " > [SEP] token is for next sentence prediction or Q-A answering task.\n",
        " > Maximum size of tokens that can be fed into BERT is 512, If the tokens in a sequence are less than 512, We use [PAD] tokens.\n",
        "9. BERT model's output is an embedding vector of size 768 also know as pooled output. It's mainly used for fine tuning tasks like Text classification, NSP, NER or Q&A. The embedding vector of size 768 from [CLS] token is fed to any classifier for the aforementioned task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_UfxD58un7l",
        "outputId": "6e94b5e5-6c2c-4c69-ae9a-6c9f0751ae9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in 241th sample is 604 \n",
            "Number of tokens in 251th sample is 599 \n",
            "Number of tokens in 261th sample is 539 \n",
            "Number of tokens in 331th sample is 537 \n",
            "Number of tokens in 531th sample is 592 \n",
            "Number of tokens in 631th sample is 736 \n",
            "Number of tokens in 651th sample is 554 \n",
            "Number of tokens in 771th sample is 829 \n",
            "Number of tokens in 881th sample is 627 \n",
            "Number of tokens in 961th sample is 526 \n",
            "Number of tokens in 1141th sample is 564 \n",
            "Number of tokens in 1151th sample is 577 \n",
            "Number of tokens in 1231th sample is 805 \n",
            "Number of tokens in 1291th sample is 687 \n",
            "Number of tokens in 1301th sample is 976 \n",
            "Number of tokens in 1371th sample is 675 \n",
            "Number of tokens in 1381th sample is 793 \n",
            "Number of tokens in 1491th sample is 644 \n",
            "Number of tokens in 1931th sample is 606 \n",
            "Number of tokens in 1961th sample is 619 \n",
            "Number of tokens in 1991th sample is 571 \n",
            "Number of tokens in 2011th sample is 1324 \n",
            "Number of tokens in 2021th sample is 915 \n",
            "Number of tokens in 2031th sample is 612 \n",
            "Number of tokens in 2041th sample is 517 \n",
            "Number of tokens in 2051th sample is 653 \n",
            "Number of tokens in 2091th sample is 547 \n",
            "Number of tokens in 2111th sample is 792 \n",
            "Number of tokens in 2161th sample is 653 \n",
            "Number of tokens in 2181th sample is 996 \n",
            "Number of tokens in 2201th sample is 579 \n",
            "Number of tokens in 2211th sample is 563 \n",
            "Percentage of documents that are suitable for BERT input are 8.584269662921349 %\n"
          ]
        }
      ],
      "source": [
        "#analysing length of token\n",
        "suitable_doc = 0\n",
        "for i in range(1,len(news),10):\n",
        "    #print(\"Number of tokens in {}th sample is {} \".format(i,len(news[i].split(' '))))  #max size for BERT input is 512\n",
        "    if len(news[i].split(' '))<512:\n",
        "        suitable_doc+=1\n",
        "    else:\n",
        "        print(\"Number of tokens in {}th sample is {} \".format(i,len(news[i].split(' ')))) \n",
        "\n",
        "print(\"Percentage of documents that are suitable for BERT input are {} %\".format(suitable_doc/len(news)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b5tLtWSun7m"
      },
      "source": [
        "In all the samples where input_size is greater than 512, We shall truncate the input and pass further. However, It is not recommended.\n",
        "\n",
        "List of various tokenizers can be found from https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O5BZa3Auun7m"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "e10Kqvb1un7n"
      },
      "outputs": [],
      "source": [
        "tokenised_text = tokenizer(news[2091],padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktDRYpaSun7n"
      },
      "source": [
        "The tokenised text is a dictionary with following keys which are elaborated.\n",
        "1. input_ids - It is the id representation of each token.\n",
        "2. token_type_ids - It is a binary mask that identifies in which sequence a token belongs. It's an optional input for various tasks.\n",
        "3. attention_mask - It is a binary mask which identifies whether a token is a real word or padding. If the token contains [CLS],[SEP] or real word then mask is 1 else for [PAD] its 9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsOB6NMxun7o",
        "outputId": "04e9ddde-e48b-4ad2-c0bd-824d6e6878ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenised_text.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KArOoIc7un7o",
        "outputId": "c9d6a830-74ce-44f8-d0b3-e306307a3a67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101,  9980,  2489,  2015,  3156,  4007, 13979,  3274,  5016,  9980,\n",
              "          2758,  3156,  1997,  2049,  4007, 13979,  2097,  2022,  2207,  2046,\n",
              "          1996,  2330,  2458,  2451,  1012,  1996,  2693,  2965,  9797,  2097,\n",
              "          2022,  2583,  2000,  2224,  1996,  6786,  2302,  7079,  2005,  1037,\n",
              "         11172,  2013,  1996,  2194,  1012,  9980,  2649,  1996,  3357,  2004,\n",
              "          1037,  1000,  2047,  3690,  1000,  1999,  2129,  2009,  9411,  2007,\n",
              "          7789,  3200,  1998,  5763,  2582, 13979,  2052,  2022,  2081, 10350,\n",
              "          2800,  1012,  1996, 13979,  2421,  4007,  2005,  1037,  2846,  1997,\n",
              "          6078,  1010,  2164,  3793,  5038,  1998,  7809,  2968,  1012,  3151,\n",
              "          2974,  2449,  3343,  2003,  2000, 25933,  4757, 13979,  1998,  2750,\n",
              "          9980,  1005,  1055,  8874,  1996,  2194,  4247,  2000,  3582,  2023,\n",
              "          2799,  1012,  9980,  2001,  4379,  1017,  1010, 24568, 13979,  1999,\n",
              "          2432,  1010,  2062,  2084,  2151,  2060,  3813,  1999,  1996,  2149,\n",
              "          1010,  1996,  2047,  2259,  2335,  4311,  1012,  2005,  2169,  1997,\n",
              "          1996,  2627,  2260,  2086,  9980,  2038,  2042,  4379,  2062,  2149,\n",
              "         13979,  2084,  2151,  2060,  2194,  1012,  9980,  2038,  2363,  2423,\n",
              "          1010,  6255,  2475,  2149, 13979,  1999,  2008,  2558,  1998,  7283,\n",
              "          2038,  2062,  2084,  2871,  1010,  2199,  2783, 13979,  1012,  1999,\n",
              "          1037,  4861,  1010,  2852,  2198,  1041,  1012,  5163,  1010,  9980,\n",
              "          3026,  3580,  2343,  1010,  2974,  1998,  7789,  3200,  1010,  2056,\n",
              "          1024,  1000,  2995,  8144,  4105,  2003,  2055,  2062,  2084,  2074,\n",
              "          1996,  3616,  1997, 13979,  4379,  1012,  2009,  1005,  1055,  2055,\n",
              "          7601,  7103,  3436,  2000,  5770,  6304,  1010,  5826,  1998,  2554,\n",
              "          1012,  1000,  2256, 16393,  2651,  2003,  1996,  2927,  1997,  1037,\n",
              "          2047,  3690,  1999,  2129,  9980,  2097,  6133,  7789,  3200,  1012,\n",
              "          1000,  1999,  1996,  2627,  1010,  9980,  2038,  3569,  1996,  2512,\n",
              "          1011,  3293,  4082,  2291, 11603,  2348,  4401,  2031,  2056,  2023,\n",
              "          2001,  2589,  2069,  2004,  2019,  3535,  2000, 25174,  7513,  1012,\n",
              "          1996,  2194,  2056,  2009,  2359,  2000,  8627,  2060,  9786,  2000,\n",
              "          2713, 13979,  2046,  2054,  2009,  2170,  1037,  1000,  7353,  7674,\n",
              "          1000,  1012,  4205,  8183, 19036,  2015,  1010,  9980,  1005,  1055,\n",
              "          2088,  1011,  2898, 11603,  5656,  3208,  1010,  2056,  1996,  2693,\n",
              "          2001,  1037, 10218,  3535,  2000,  8627,  8144,  1012,  1000,  2057,\n",
              "          2903,  2008,  8287,  2122, 13979,  2097,  2765,  1999,  8144,  3048,\n",
              "          2062,  2855,  1012,  1000,  2023,  2003,  2055, 11434,  5792,  1998,\n",
              "          2206,  1037,  2944,  2172,  2066, 16926,  1012,  1000,  2720,  8183,\n",
              "         19036,  2015, 28834,  1996,  2933,  2005,  1037,  7353,  7674,  2000,\n",
              "          1996,  2126,  1996,  4274,  2001,  2764,  1998,  2056,  3071,  2071,\n",
              "          2202,  5056,  1997,  1996,  2765,  1997,  5792,  1012,  1000,  1996,\n",
              "          4274,  1005,  1055,  4254,  2038,  2042,  2006,  3071,  1012,  1996,\n",
              "          6666,  2024,  2045,  2005,  3071,  2000,  2202,  5056,  1997,  1012,\n",
              "          1000,  6990,  9946,  1010,  2708,  3237,  1997,  2149,  3813,  2330,\n",
              "          3120,  2458, 13625,  1010,  2056,  1996,  2693,  2071,  2812,  1037,\n",
              "          2689,  1999,  1996,  2126,  3316,  3066,  2007, 13979,  1012,  1000,\n",
              "          1045,  2228,  2060,  3316,  2097,  3582,  4848,  1010,  1000,  2002,\n",
              "          2056,  1012,  2021,  2025,  3071,  2001,  2004, 16408,  1012, 29517,\n",
              "         26774,  1010,  3049,  3208,  1997,  1037,  2177, 19670,  2327,  2890,\n",
              "         15338,  4007, 13979,  3352,  3423,  1999,  1996,  2647,  2586,  1010,\n",
              "          7219,  9980,  1005,  1055,  2693,  2004, 16021, 12083, 12693, 20925,\n",
              "          1012,  1000,  2009,  1005,  1055,  2074, 20150,  5649,  9887,  1010,\n",
              "          1000,  2626,  2720, 26774,  1010,  2040,  5260, 15460, 15794,  8059,\n",
              "         17585,   102]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenised_text['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1lp7B_6un7o",
        "outputId": "c8ba016d-8c27-4803-c90d-c390698b4ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] ibm frees 500 software patents computer giant ibm says 500 of its software patents will be released into the open development community. the move means developers will be able to use the technologies without paying for a licence from the company. ibm described the step as a \" new era \" in how it dealt with intellectual property and promised further patents would be made freely available. the patents include software for a range of practices, including text recognition and database management. traditional technology business policy is to amass patents and despite ibm's announcement the company continues to follow this route. ibm was granted 3, 248 patents in 2004, more than any other firm in the us, the new york times reports. for each of the past 12 years ibm has been granted more us patents than any other company. ibm has received 25, 772 us patents in that period and reportedly has more than 40, 000 current patents. in a statement, dr john e. kelly, ibm senior vice president, technology and intellectual property, said : \" true innovation leadership is about more than just the numbers of patents granted. it's about innovating to benefit customers, partners and society. \" our pledge today is the beginning of a new era in how ibm will manage intellectual property. \" in the past, ibm has supported the non - commercial operating system linux although critics have said this was done only as an attempt to undermine microsoft. the company said it wanted to encourage other firms to release patents into what it called a \" patent commons \". adam jollans, ibm's world - wide linux strategy manager, said the move was a genuine attempt to encourage innovation. \" we believe that releasing these patents will result in innovation moving more quickly. \" this is about encouraging collaboration and following a model much like academia. \" mr jollans likened the plan for a patent commons to the way the internet was developed and said everyone could take advantage of the result of collaboration. \" the internet's impact has been on everyone. the benefits are there for everyone to take advantage of. \" stuart cohen, chief executive of us firm open source development labs, said the move could mean a change in the way companies deal with patents. \" i think other companies will follow suit, \" he said. but not everyone was as supportive. florian mueller, campaign manager of a group lobbying toprevent software patents becoming legal in the european union, dismissed ibm's move as insubstantial. \" it's just diversionary tactics, \" wrote mr mueller, who leadsnosoftwarepate [SEP]\n"
          ]
        }
      ],
      "source": [
        "#getting back text from input_ids\n",
        "example_text = tokenizer.decode(tokenised_text.input_ids[0])\n",
        "print(example_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYAyXtsJun7p",
        "outputId": "cf36b624-4012-4339-8560-c600b0e85059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'business', 1: 'entertainment', 2: 'sport', 3: 'tech', 4: 'politics'}\n"
          ]
        }
      ],
      "source": [
        "#building the dataset\n",
        "label_mapper = {'business':0, 'entertainment':1, 'sport':2, 'tech':3, 'politics':4}\n",
        "inverse_label_mapper = {} #for prediction\n",
        "for k,v in label_mapper.items():\n",
        "    inverse_label_mapper[v] = k\n",
        "\n",
        "print(inverse_label_mapper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KyxQ_8lyun7p"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self,news,labels):\n",
        "        self.tokenizer= BertTokenizer.from_pretrained('bert-base-cased')\n",
        "        self.tokenized_news = []\n",
        "        for doc_text in news:\n",
        "            tokenised_doc = self.tokenizer(doc_text,padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "            self.tokenized_news.append(tokenised_doc)\n",
        "        \n",
        "        self.encoded_labels = []\n",
        "        for label in labels:\n",
        "            self.encoded_labels.append(label_mapper[label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_news)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokenised_news = self.tokenized_news[idx]\n",
        "        encoded_label = self.encoded_labels[idx]\n",
        "        \n",
        "        item = {'input_tokens':tokenised_news,'output_labels':np.array(encoded_label)}\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DRPjpwNhun7r"
      },
      "outputs": [],
      "source": [
        "#splitting into train ,val\n",
        "X_train, X_test, y_train, y_test = train_test_split(news, category, test_size=0.10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OWImLGb7un7s"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(X_train,y_train)\n",
        "val_dataset = TextClassificationDataset(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyJ_513jun7t",
        "outputId": "1e3d47b9-bc9e-427f-cb5d-cdd09f22378e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_tokens': {'input_ids': tensor([[  101, 22163,  2705,  2274, 10217,  4829,  1109,  2705,  3275,  1104,\n",
              "           1103, 22163, 27482,  1144,  1678,  1285,   118,  1106,   118,  1285,\n",
              "           1654,  1104,  1157,  7851,  1610,  1671,  1107,  1126,  3098,  1106,\n",
              "           1885,  1122,  1213,   119, 15720,  1345,  1988,  1673,  1144,  2125,\n",
              "           6717,  3177, 10212,  1112,  2705,  3275,  1104, 22163, 12983,   117,\n",
              "           1114,  1828,  3177, 10212,  2128,  1103,  1419,   119,  1828,  1345,\n",
              "           1988,  1673,  3316,  1103,  2223,  1246,  1104,  1103,  1671,   118,\n",
              "           1134,  1110,  2637,  1106,  1294,   170,  4645,  1306, 27772,   113,\n",
              "            109,   122,  1830,  1179,   114,  2445,  1107,  1516,   118,  1107,\n",
              "           1112,  1242,  1201,   119, 22163,  1223,  3365, 17747,  1103,  2319,\n",
              "           1107,  1980,  1314,  1214,   117,  3195,  3596,  3813,   119,  1109,\n",
              "           1610,  1671,  1144,  1189,  1126,  3389,  2445,  1107,  1421,  1104,\n",
              "           1103,  1314,  1565,  1201,  1105,  1108,  2257,  1106,  4684,  1171,\n",
              "           1157,  2549,   118,  1256,  4010,  1121,  1478,  1106,  1386,   119,\n",
              "           1109,  2635,  2607,  1132,  1226,  1104,   170,  6815,  5854,   118,\n",
              "           1146,  1104,  1103,  1671,  1378, 22163,   112,   188,  6021,  1104,\n",
              "           1157,  7287,  1114,  1615, 13931,   119,  1249,  1226,  1104,   170,\n",
              "           1558, 20841,   117, 22163,  1110,  1106, 18831,  1103,  7085,  6906,\n",
              "          11745,  1610,  1419,   118,  1971,  2205,  1118, 14013,   118,  1439,\n",
              "           1157,  1319,  2500,   119, 14013,   117,  1107,  1134, 22163,  8300,\n",
              "            170,  2656,  8219,   117,  1180,  1129, 10380, 14146,  1113,  1103,\n",
              "           4482,  2319,  1107,  1719,  1386,  1137,  1384,   119,  1828,  1345,\n",
              "           1988,  1673,   117,  1150,  1178,  1688,  1103,  1419,  1314,  1214,\n",
              "            117,  1163, 22163, 12983,  1108,  1208,  1103,   107,  3981,  2817,\n",
              "            107,  1104,  1117,  2209,   119,   107,   146,  1138,  1189,  1103,\n",
              "           2383,  1106,  1321,  1113,  1103,  2112,  1104,  2705,  3275,  1104,\n",
              "           1103, 12365,  2587,  1106,  2420,  1146,  1103,  1419,   112,   188,\n",
              "           7593,   117,   107,  1119,  1163,   119,   107,   138, 14511,  3057,\n",
              "           9047,  1110, 14910,  1378,   170,  2635,  1231, 28037,  1115,  1144,\n",
              "           4653,   170,  1167,   170,  5389,  1513,  1105,  7856,  2401,   117,\n",
              "            107,  1119,  1896,   119,  1966,  1828,  1345,  1988,  1673,  1674,\n",
              "           1136,  1138,   170,  3582,  1107,  1103,  1610,  2380,   117,  1119,\n",
              "           1144,  1151,  1773,  1126,  4138,  1648,  1107,  1103,  1372,   112,\n",
              "            188,  2619,   119,  4254,  1214,   117,  1119,  1163,  1115,   170,\n",
              "           1326,  1104,  1207,  3584,   117,  2536,  1112,  1226,  1104,  1103,\n",
              "           1372,   112,   188,  7593,  2197,   117,  1125,  1136, 14112,  1174,\n",
              "          13081,  1112,  1277,  1112,  4320,   119,  1109,  1610,  1671,   117,\n",
              "           1436,  1227,  1111,  1157, 27099, 12992, 12477, 23121,   117,  1110,\n",
              "           2637,  1106,  1294,   170,  2445,  1104,  1164,  4645,  1306, 25921,\n",
              "           1107,  1516,   119, 15689,  1132,  2637,  1106,  2303,  1107,  1478,\n",
              "            117, 22163,  1163,  1142,  1989,   117,  1112,  1122, 18387,  8362,\n",
              "           1643,  2180, 14067,  1895,  1877,  1216,  1112,  1103, 13519,  1610,\n",
              "           2319,   119,  1828,  3177, 10212,   117,   170,  1610,  2380,  8230,\n",
              "            117,  1261,  1103, 22778,  1107,  1379,  1581,  1170,  1217,  8443,\n",
              "           1118,  1393, 22163,  2705,  3275, 12752, 12556, 10340,  2660,   119,\n",
              "           1828, 12556, 10340,  2660,  1189,   170,  6875,  1314,  1214,  1106,\n",
              "           1561,  3931,  1170,  1103,  1473,  1104,  2084, 12189, 26831,   138,\n",
              "           8376,  6473,   119,  1438,   117,  1142,  1108,  5164,  1118,  1103,\n",
              "           4263,   138,  8376,  6473,  1266,  1105,  1828, 12556, 10340,  2660,\n",
              "           2886,  4603,   119, 15993,  1142,  1989,   117, 22163,  1680,  1126,\n",
              "           3311,  1114, 14748,  1106, 24907,  1126,  7214,  1134,  1180,  1138,\n",
              "          15450,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]])}, 'output_labels': array(0)}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.__getitem__(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XDdqdbLfun7t"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset,shuffle=True,pin_memory=True,batch_size=64)\n",
        "val_loader = DataLoader(val_dataset,shuffle=True,pin_memory=True,batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dkvyolrun7t",
        "outputId": "ddbfda3f-c129-4241-c613-d25ca254c552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_tokens': {'input_ids': tensor([[[  101, 12646,  8755,  ...,  2846,  1112,   102]],\n",
            "\n",
            "        [[  101, 11644,  6439,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[  101,  7268,  2642,  ...,  2616,  2182,   102]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[  101,   148,  5114,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[  101,  1993, 20335,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[  101,  6940,  3606,  ...,     0,     0,     0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 1, 1, 1]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]]])}, 'output_labels': tensor([3, 0, 0, 2, 1, 0, 3, 2, 2, 3, 1, 3, 0, 1, 4, 4, 2, 1, 2, 2, 0, 2, 2, 1,\n",
            "        1, 1, 2, 3, 0, 4, 2, 3, 2, 2, 3, 0, 2, 2, 3, 0, 1, 0, 4, 3, 0, 4, 0, 1,\n",
            "        0, 4, 1, 1, 2, 2, 2, 4, 2, 0, 1, 2, 2, 0, 4, 0])}\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Q0KGcvRXun7u"
      },
      "outputs": [],
      "source": [
        "#creating model\n",
        "class BertFineTuner(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertFineTuner,self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        #freezing model\n",
        "        for p in self.bert.parameters():\n",
        "                p.requires_grad = False\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(768,5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,input_id,attention_mask):\n",
        "        # 768 dim pooled output\n",
        "        _,pooled_output = self.bert(input_ids = input_id,attention_mask=attention_mask,return_dict=False)\n",
        "        out = self.dropout(pooled_output)\n",
        "        out = self.linear(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NDRDFV5un7u",
        "outputId": "d78e3308-4cd2-4a4c-eb95-39dbf661586b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch - 0, Train Loss - 1.6241945028305054, Train Accuracy -3, Val Loss - 1.6002681255340576, Val Accuracy - 5\n",
            "Epoch - 1, Train Loss - 1.3529332876205444, Train Accuracy -6, Val Loss - 1.8286601305007935, Val Accuracy - 2\n",
            "Epoch - 2, Train Loss - 1.536146879196167, Train Accuracy -4, Val Loss - 1.4122750759124756, Val Accuracy - 9\n",
            "Epoch - 3, Train Loss - 1.682178258895874, Train Accuracy -4, Val Loss - 1.4606709480285645, Val Accuracy - 9\n",
            "Epoch - 4, Train Loss - 1.2396433353424072, Train Accuracy -9, Val Loss - 1.3118032217025757, Val Accuracy - 11\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Training model\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertFineTuner().to(device)\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "num_epochs =20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i,batch in enumerate(train_loader):\n",
        "        input_ids= batch['input_tokens']['input_ids'].squeeze(1).to(device)\n",
        "        attention_masks = batch['input_tokens']['attention_mask'].to(device)\n",
        "        output_label = batch['output_labels'].to(device)\n",
        "        output = model(input_ids,attention_masks)\n",
        "        train_loss = criterion(output,output_label.long())\n",
        "        train_acc = (output.argmax(dim=1) == output_label).sum().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "    #validation\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i,batch in enumerate(val_loader):\n",
        "            input_ids= batch['input_tokens']['input_ids'].squeeze(1).to(device)\n",
        "            attention_masks = batch['input_tokens']['attention_mask'].to(device)\n",
        "            output_label = batch['output_labels'].to(device)\n",
        "            output = model(input_ids,attention_masks)\n",
        "            val_loss = criterion(output,output_label.long())\n",
        "            val_acc = (output.argmax(dim=1) == output_label).sum().item()\n",
        "    print(\"Epoch - {}, Train Loss - {}, Train Accuracy -{}, Val Loss - {}, Val Accuracy - {}\".format(epoch,train_loss.item(),train_acc,val_loss.item(),val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "H5Wz4pduun7v"
      },
      "outputs": [],
      "source": [
        "def predict(text):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "  # tokenize text and generate input_ids and attn_mask\n",
        "  token = tokenizer(text,padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "  token['input_ids'] = token['input_ids'].squeeze(1).to(device)\n",
        "  token['attention_mask'] = token['attention_mask'].to(device)\n",
        "  with torch.no_grad():\n",
        "    logits = model(token['input_ids'],token['attention_mask'])\n",
        "  #applying softmax\n",
        "  probs = torch.nn.functional.softmax(logits,dim=1).cpu().numpy()\n",
        "  category = np.argmax(probs,axis=1)[0]\n",
        "  return inverse_label_mapper[category]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "lErh1fx2xskY",
        "outputId": "c5e03e44-22a7-4435-a8b7-5da073a84f67"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sport'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(\"The plain green Norway spruce is displayed in the gallery's foyer. Its light bulb adornments are dimmed, ordinary domestic ones joined together with string. The plates decorating the branches will be auctioned off for the children's charity ArtWorks. Wentworth worked as an assistant to sculptor Henry Moore in the late 1960s. His reputation as a sculptor grew in the 1980s, while he has been one of the most influential teachers during the last two decades. Wentworth is also known for his photography of mundane, everyday subjects such as a cigarette packet jammed under the wonky leg of a table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8zIT4rdEcOD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "torch_dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3fc43966dd8a35b9bb4dacfb26d54ec70461d2f8773a70bf315d67d5e8c2bf14"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
