{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "Sentence Segmentation is a vital task in Natural language processing that is meant to split a text into sentences seperated by in-built or custom delimiter.\n",
    "\n",
    "Spacy has in-built functionalities to facilitate this task which can be tweaked as per custom use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/envs/dl/lib/python3.7/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2023-01-17 00:25:21.829262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-17 00:26:05.333830: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-17 00:26:05.336047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-17 00:26:05.341226: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-01-17 00:26:05.341286: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: suraj\n",
      "2023-01-17 00:26:05.341299: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: suraj\n",
      "2023-01-17 00:26:05.341514: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.226.0\n",
      "2023-01-17 00:26:05.341541: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.226.0\n",
      "2023-01-17 00:26:05.341549: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.226.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Suraj.\n",
      "I am here to illustrate the first demo of sentence segmentation.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is Suraj. I am here to illustrate the first demo of sentence segmentation.\"\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This segmented the sentence by its default delimiter ie fullstop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentences.append(sent)\n",
    "\n",
    "print(type(sent[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 16\n"
     ]
    }
   ],
   "source": [
    "# it can also be used as span and define the start and end.\n",
    "print(sentences[1].start, sentences[1].end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, We can segment texts based on custom delimiters too. Let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How we do things when no one is looking at it, Often matters; It is because that's what defines our character.\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at a different example.\n",
    "text2 = \"How we do things when no one is looking at it, Often matters; It is because that's what defines our character.\"\n",
    "doc2 = nlp(text2)\n",
    "sentences2 = []\n",
    "for sent in doc2.sents:\n",
    "    sentences2.append(sent)\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, The inbuilt sentence segmentation just segmented our second text into just one sentence ignoring the delimiters ; and ,\n",
    "Let's build custom rules for it. Refer <a href=\"https://spacy.io/usage/processing-pipelines#custom-components\"> Link </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "#always add the following decorator \n",
    "@Language.component(\"set_rules\")\n",
    "def set_rules(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \";\" or token.text == ',':\n",
    "            doc[token.i+1].is_sent_start=True\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_rules(doc)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add this to existing nlp pipe\n",
    "nlp.add_pipe(\"set_rules\", before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'set_rules',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How we do things when no one is looking at it,\n",
      "Often matters;\n",
      "It is because that's what defines our character.\n"
     ]
    }
   ],
   "source": [
    "# re-run the doc object creation\n",
    "doc2 = nlp(text2)\n",
    "sentences2_modified = []\n",
    "for sent in doc2.sents:\n",
    "    sentences2_modified.append(sent)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences2_modified)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - We can see that it segments our text based on our custom rules\n",
    "\n",
    "Let's see one more complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"Hi, This is Suraj. \\n We are back with yet another illustration on the topic. \\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next sentence\n",
      "Hi, This is Suraj. \n",
      " We are back with yet another illustration on the topic. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', exclude=[\"parser\"])\n",
    "    \n",
    "config = {\"punct_chars\": ['\\n']}\n",
    "nlp.add_pipe(\"sentencizer\", config=config)\n",
    "\n",
    "for sent in nlp(text3).sents:\n",
    "    print(\"next sentence\")\n",
    "    print(sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details in multiple language, Use <a href=\"https://github.com/nipunsadvilkar/pySBD\"> pySBD </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50052c996937e9a0e161d422489677fdaadc23d756ac209b7397e80e5ea8cea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
