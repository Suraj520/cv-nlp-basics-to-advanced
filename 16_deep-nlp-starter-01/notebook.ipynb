{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "DeepNLP Starter 01- RNN and its application for name classification.\n",
    "\n",
    "- RNN(Recurrent NN) is very special as it takes x as input and produces and output, ensuring the hidden state is being passed to the next state as per the following diagram.\n",
    "\n",
    "![RNN](RNN.png \"RNN\")\n",
    "\n",
    "The series of inputs x1,x2,x3 of each consecutive cell from L to R generates y1,y2,y3 thereby passing hidden states from L to R.\n",
    "\n",
    "This is useful in handling time series data, language modelling, translation etc task where data is a correlated series of data.\n",
    "\n",
    "RNNs can be of following types\n",
    "\n",
    "1. one to many for Image captioning tasks (image as input and caption as sequential input)\n",
    "2. many to one for sentiment analysis taks (text as input and output - sentiment )\n",
    "3. many to many for language translation tasks where one language sequential data is fed as input and it generates other language as output.\n",
    "\n",
    "![RNN2](RNN2.jpg \"RNN2\")\n",
    "\n",
    "The above diagram shows the working of RNN where previous state's output(h_t-1) is being concatenated with current state's input x_t and passed to an activation function tanh thereby generating h_t which is being passed to next state.\n",
    "\n",
    "To use RNN or its complex derivatives in PyTorch, We need to use the following syntax,\n",
    "\n",
    "```\n",
    "#output is usually quoted as hidden_size\n",
    "RNN_cell= nn.RNN(input_size=7, hidden_size=2, batch_first=True)\n",
    "GRU_cell = nn.GRU(input_size=7, hidden_size=2, batch_first=True)\n",
    "LSTM_cell = nn.LSTM(input_size=7, hidden_size=2, batch_first=True)\n",
    "\n",
    "```\n",
    "- GRU and LSTM have various gates to boost its performance with respect to RNN.\n",
    "\n",
    "\n",
    "- To implement a general RNN, we should keep the following dimensions of input handy\n",
    "\n",
    "```\n",
    "inputs_dim = batch_size, sequence_length, input_size\n",
    "hidden = cell(inputs, hidden_prev_layer) #dimension - num_layers,batch_size,hidden_size\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Let's teach RNN to say a word - Hello World\n",
    "\n",
    "1. One hot encode the unique letters in the letters\n",
    "```\n",
    "H - [1, 0, 0, 0, 0, 0, 0]\n",
    "E - [0, 1, 0, 0, 0, 0, 0]\n",
    "L - [0, 0, 1, 0, 0, 0, 0]\n",
    "O - [0, 0, 0, 1, 0, 0, 0]\n",
    "W - [0, 0, 0, 0, 1, 0, 0]\n",
    "R - [0, 0, 0, 0, 0, 1, 0]\n",
    "D - [0, 0, 0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "2. Feeding each letter to RNN node \n",
    "\n",
    "> input_shape to each RNN_cell =  1,1,7\n",
    "input to first RNN - [[[1, 0, 0, 0, 0, 0, 0]]] #batch_size,seq_len, input_len\n",
    "\n",
    "> since, we define output_size as 2 i.e hidden_dim\n",
    "then dimension of h_t to this cell shall be [[[x,x]]] #1,1,2 - num_layers,batchsize, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 7])\n",
      "output of first RNN cell is tensor([[[-0.3513, -0.5473]]], grad_fn=<TransposeBackward1>)\n",
      "hidden state- tensor([[[-0.3513, -0.5473]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "RNN_Cell = nn.RNN(input_size=7, hidden_size=2, batch_first=True)\n",
    "\n",
    "inputs = torch.Tensor([[[1, 0, 0, 0, 0, 0, 0]]])\n",
    "print(inputs.shape)\n",
    "\n",
    "# initialising an initial hidden state to be passed to first RNN cell\n",
    "init_hidden_state = torch.randn(1,1,2) # num_layers,batch,hidden_size\n",
    "\n",
    "out, hidden = RNN_Cell(inputs,init_hidden_state)\n",
    "print(\"output of first RNN cell is {}\".format(out))\n",
    "print(\"hidden state- {}\".format(hidden))\n",
    "\n",
    "#clearly both are same and this shall be passed to next RNN cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's create a many to many RNN stacked where each hidden_state is passed to next cell\n",
    "#### one for each h,e,l,l,o, w, o,r,l,d\n",
    "\n",
    "shape = (1,10,7) # batch,seq_len,input_len\n",
    "\n",
    "#### Also, by increasing batches, We mean we want to forward multiple permutations\n",
    "\n",
    "e,l,l,o,w,o,r,l,d,h etc\n",
    "the shape thus becomes\n",
    "\n",
    "2,10,7\n",
    "\n",
    "- However, This shall have its imprint on computation cost as it will be slow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc43966dd8a35b9bb4dacfb26d54ec70461d2f8773a70bf315d67d5e8c2bf14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
